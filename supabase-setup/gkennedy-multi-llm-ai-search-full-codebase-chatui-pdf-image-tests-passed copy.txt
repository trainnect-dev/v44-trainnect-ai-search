Directory structure:
└── mindful-ai-dude-v99-gkennedy-ai-search/
    ├── README.md //https://github.com/mindful-ai-dude
    ├── babel.config.test.js
    ├── eslint.config.mjs
    ├── jest.config.js
    ├── jest.setup.js
    ├── middleware.ts
    ├── next.config.ts
    ├── package.json
    ├── postcss.config.mjs
    ├── react.d.ts
    ├── tailwind.config.ts
    ├── test-openai.sh
    ├── test.txt
    ├── tsconfig.json
    ├── types.d.ts
    ├── .env.example
    ├── .windsurfrules
    ├── __mocks__/
    │   ├── fileMock.js
    │   ├── framer-motion.js
    │   ├── react-markdown.js
    │   ├── @ai-sdk/
    │   │   └── react.js
    │   └── lib/
    │       ├── models.js
    │       └── posthog.js
    ├── ai_agents_output/
    │   ├── ai-agents-2025-03-17T16:47:18.372Z-claude-3.7-sonnet-o3-mini.json
    │   ├── ai-agents-2025-03-17T16:53:03.324Z-gemini-2.0-flash-claude-3.5-sonnet.json
    │   ├── ai-agents-2025-03-17T17:00:36.069Z-gemini-2.0-flash-claude-3.5-sonnet.json
    │   ├── ai-agents-2025-03-18T05:56:46.385Z-qwen-qwq-32b-gemini-2.0-flash.json
    │   ├── ai-agents-2025-03-18T05:57:27.709Z-claude-3.5-sonnet-o3-mini.json
    │   └── ai-agents-log.jsonl
    ├── app/
    │   ├── globals.css
    │   ├── layout.tsx
    │   ├── page.tsx
    │   ├── providers.tsx
    │   ├── ai-agents/
    │   │   └── page.tsx
    │   ├── api/
    │   │   ├── ai-agents/
    │   │   │   └── route.ts
    │   │   ├── chat/
    │   │   │   └── route.ts
    │   │   ├── tavily-chat/
    │   │   │   └── route.ts
    │   │   └── tavily-search/
    │   │       └── route.ts
    │   └── tavily-ai-search/
    │       └── page.tsx
    ├── components/
    │   ├── chat.tsx
    │   ├── deploy-button.tsx
    │   ├── footnote.tsx
    │   ├── icons.tsx
    │   ├── input.tsx
    │   ├── markdown-components.tsx
    │   ├── messages.tsx
    │   ├── model-selector.tsx
    │   ├── navigation.tsx
    │   ├── new-chat-button.tsx
    │   ├── sidebar.tsx
    │   ├── star-button.tsx
    │   ├── tavily-chat.tsx
    │   ├── ai-agents/
    │   │   └── agent-chat.tsx
    │   └── chat-history/
    │       └── chat-history-dropdown.tsx
    ├── lib/
    │   ├── models.ts
    │   ├── utils.ts
    │   ├── __tests__/
    │   │   ├── api.test.ts
    │   │   ├── models.test.ts
    │   │   ├── tavily-chat.test.ts
    │   │   └── tavily-search.test.ts
    │   ├── ai-agents/
    │   │   ├── types.ts
    │   │   └── utils.ts
    │   ├── supabase/
    │   │   └── chat-history.ts
    │   └── types/
    │       └── chat-history.ts
    ├── public/
    ├── scripts/
    │   ├── run-all-tests.sh
    │   └── test-models.js
    ├── supabase/
    │   ├── README.md
    │   └── schema.sql
    ├── tests/
    │   ├── multimodal.test.tsx
    │   └── tavily-chat.test.tsx
    ├── tools/
    │   ├── README.md
    │   └── tavily-search.ts
    └── utils/
        ├── ai-agents-logger.ts
        ├── tavily-logger.ts
        └── supabase/
            ├── client.ts
            ├── middleware.ts
            ├── server-client.ts
            └── server.ts

================================================
File: README.md
================================================
<a href="https://github.com/mindful-ai-dude">
<img alt="Agentic Multi-LLM AI Assistant Built With Next.js 15 App Router, React 19, Tavily Search, and Vercel's AI SDK.  Built with love by Gregory Kennedy." src="app/opengraph-image.png">
  <h1 align="center">Multi-LLM AI Assistant</h1>
</a>

<p align="center">
  An Open-Source AI Assistant Built With Next.js 15 App Router, React 19, Multiple LLM Providers, and Vercel's AI SDK.
</p>

<p align="center">
  <a href="#features"><strong>Features</strong></a> ·
  <a href="#architecture"><strong>Architecture</strong></a> ·
  <a href="#deploy-your-own"><strong>Deploy Your Own</strong></a> ·
  <a href="#running-locally"><strong>Running locally</strong></a>
</p>
<br/>

## Features

- [Next.js](https://nextjs.org) App Router
  - Advanced routing for seamless navigation and performance
  - React Server Components (RSCs) and Server Actions for server-side rendering and increased performance
- [AI SDK](https://sdk.vercel.ai/docs)
  - Unified API for generating text, structured objects, and tool calls with Multi-LLMs
  - Hooks for building dynamic chat and generative user interfaces
  - Supports multiple LLM providers with a unified interface
- Supported Models
  - [Anthropic Claude](https://www.anthropic.com/claude)
    - Claude 3.7 Sonnet - Anthropic's most intelligent model with extended thinking capabilities
    - Claude 3.5 Sonnet - Balances intelligence and speed for enterprise workloads
  - [OpenAI](https://openai.com/)
    - o3-mini - OpenAI's efficient and capable model
  - [Google](https://deepmind.google/technologies/gemini/)
    - Gemini 2.0 Flash - Google's powerful, fast, and efficient model
  - [Groq](https://groq.com/)
    - Qwen-QWQ-32B - High-performance open source LLM
  - [Mistral](https://mistral.ai/)
    - Codestral - Specialized for code generation and understanding
  - [Perplexity](https://www.perplexity.ai/)
    - Perplexity Sonar - Research-focused model
  - [OpenRouter](https://openrouter.ai/)
    - Access to various models through a unified API
- Features
  - Model Switching - Seamlessly switch between different LLM providers
  - Reasoning Mode - Toggle step-by-step reasoning for more detailed responses
    - Default state: Disabled (off)
    - A toggle button labeled "Reasoning" appears in the chat input area
    - When enabled (toggled on), the AI provides detailed step-by-step reasoning in its responses
    - When disabled (toggled off), the AI provides direct, concise responses
    - Can be toggled on/off at any time during the conversation
  - Tavily Web Search - Enhance AI responses with real-time web search results
    - Available on the dedicated Tavily AI Search page (/tavily-ai-search)
    - Toggle web search on/off with a simple switch
    - Provides up-to-date information from the web to improve AI responses
    - Works with all supported models
  - AI Agents - Multi-step AI processing with model switching
    - Primary model performs research using Tavily search
    - Secondary model processes and synthesizes the results
    - Comprehensive logging system:
      - `tavily_output/` - Contains raw search results from the primary model
      - `ai_agents_output/` - Contains both models' outputs:
        - Individual JSON files: `ai-agents-{timestamp}-{primary-model}-{secondary-model}.json`
        - Summary log: `ai-agents-log.jsonl`
      - Real-time log monitoring: `tail -f ai_agents_output/ai-agents-log.jsonl`
- [shadcn/ui](https://ui.shadcn.com)
  - Styling with [Tailwind CSS](https://tailwindcss.com)
  - Component primitives from [Radix UI](https://radix-ui.com) for accessibility and flexibility

## Architecture

The application follows a modern architecture pattern using Next.js and the Vercel AI SDK to communicate with various LLM providers.

For a detailed view of the application architecture, including component breakdown, data flow, and a mermaid diagram, see the [Architecture Documentation](docs/project-structure-ui-ux-flow.md).

## Deploy Your Own

You can deploy your own version of the Next.js AI Chatbot to Vercel with one click:

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fvercel-labs%2Fai-sdk-preview-reasoning%2Ftree%2Fmain&env=ANTHROPIC_API_KEY,OPENAI_API_KEY,GOOGLE_GENERATIVE_AI_API_KEY,GROQ_API_KEY,MISTRAL_API_KEY,PERPLEXITY_API_KEY,OPENROUTER_API_KEY,TAVILY_API_KEY&envDescription=API%20keys%20for%20various%20LLM%20providers&envLink=https%3A%2F%2Fgithub.com%2Fvercel-labs%2Fai-sdk-preview-reasoning%2Fblob%2Fmain%2F.env.example)

## Required API Keys

To use all the supported models, you'll need to obtain API keys from the following providers:

- [Anthropic API Key](https://console.anthropic.com/) - For Claude models
- [OpenAI API Key](https://platform.openai.com/account/api-keys) - For o3-mini model
- [Google AI API Key](https://aistudio.google.com/apikey) - For Gemini models
- [Groq API Key](https://console.groq.com/keys) - For Qwen model
- [Mistral API Key](https://console.mistral.ai/api-keys) - For Codestral model
- [Perplexity API Key](https://www.perplexity.ai/) - For Perplexity Sonar model
- [OpenRouter API Key](https://openrouter.ai/settings/keys) - For OpenRouter models
- [Tavily API Key](https://tavily.com/) - For web search functionality

Add these keys to your `.env.local` file or Vercel environment variables.

## Supabase Integration

The application now includes Supabase integration for chat history management:

- **Chat History Persistence**: Store and retrieve chat sessions and messages
- **User-specific Data**: Row Level Security (RLS) ensures users can only access their own data
- **Time-based Filtering**: Filter chat history by time periods (7, 14, 30 days, or all)
- **Session Management**: Copy, delete, and restore chat sessions

### Supabase Setup

To use the Supabase integration, add the following environment variables to your `.env.local` file:

```bash
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key
```

The database schema can be found in `supabase/schema.sql`. Run these SQL statements in the Supabase SQL editor to set up the necessary tables and Row Level Security policies.

For detailed setup instructions, see the [Supabase Integration Documentation](supabase/README.md).

## Testing

The application includes comprehensive tests to ensure all features work correctly:

### Running Tests

```bash
# Run Jest tests
pnpm test

# Run model tests
pnpm test:models

# Run all tests (Jest + model tests)
pnpm test:all
```

### Test Coverage

- **Unit Tests**: Test individual components and functions
- **API Tests**: Test API routes for chat and search functionality
- **Model Tests**: Test all supported AI models
- **Integration Tests**: Test the complete application flow

For detailed testing instructions, see the [Tavily Testing Guide](docs/tavily-testing-guide.md).

## Running locally

You will need to use the environment variables [defined in `.env.example`](.env.example) to run Next.js AI Chatbot. It's recommended you use [Vercel Environment Variables](https://vercel.com/docs/projects/environment-variables) for this, but a `.env` file is all that is necessary.

> Note: You should not commit your `.env` file or it will expose secrets that will allow others to control access to your various OpenAI and authentication provider accounts.

1. Install Vercel CLI: `npm i -g vercel`
2. Link local instance with Vercel and GitHub accounts (creates `.vercel` directory): `vercel link`
3. Download your environment variables: `vercel env pull`

```bash
pnpm install
pnpm dev
```

Your app template should now be running on [localhost:3000](http://localhost:3000/).

## NODE MODULES - REMOVAL & CACHE CLEANING

```bash
rm -rf node_modules && pnpm store prune 
```
## NODE MODULES - REMOVAL, CACHE CLEANING & APP INSTALLATION

```bash
rm -rf node_modules && pnpm store prune && pnpm install
```

## NODE MODULES - REMOVAL, CACHE CLEANING, APP INSTALLATION & APP START

```bash
rm -rf node_modules && pnpm store prune && pnpm install && pnpm dev
```

## Shut down the npm server

```bash
pkill -f "npm"



================================================
File: babel.config.test.js
================================================
module.exports = {
  presets: [
    ['@babel/preset-env', { targets: { node: 'current' } }],
    '@babel/preset-typescript',
    ['@babel/preset-react', { runtime: 'automatic' }]
  ],
};



================================================
File: eslint.config.mjs
================================================
import { dirname } from "path";
import { fileURLToPath } from "url";
import { FlatCompat } from "@eslint/eslintrc";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const compat = new FlatCompat({
  baseDirectory: __dirname,
});

const eslintConfig = [
  ...compat.extends("next/core-web-vitals", "next/typescript"),
];

export default eslintConfig;



================================================
File: jest.config.js
================================================
module.exports = {
  preset: 'ts-jest',
  testEnvironment: 'node',
  testMatch: ['**/__tests__/**/*.test.ts', '**/tests/**/*.test.tsx'],
  transform: {
    '^.+\\.(ts|tsx)$': ['babel-jest', { configFile: './babel.config.test.js' }],
  },
  transformIgnorePatterns: [
    '/node_modules/(?!(.pnpm|vfile|unist|unified|bail|is-plain-obj|trough|remark|micromark|decode-named-character-reference|character-entities|property-information|hast|space-separated-tokens|comma-separated-tokens|mdast|markdown-table|trim-lines|string-width|strip-ansi|ansi-regex|is-fullwidth-code-point|emoji-regex|character-entities-legacy|character-reference-invalid|@ai-sdk|ai))',
  ],
  moduleNameMapper: {
    '^@/(.*)$': '<rootDir>/$1',
    '^@/lib/models$': '<rootDir>/__mocks__/lib/models.js',
    '^@/lib/posthog$': '<rootDir>/__mocks__/lib/posthog.js',
    '\\.(css|less|scss|sass)$': 'identity-obj-proxy',
    '\\.(jpg|jpeg|png|gif|webp|svg)$': '<rootDir>/__mocks__/fileMock.js',
    '^react-markdown$': '<rootDir>/__mocks__/react-markdown.js',
    '^framer-motion$': '<rootDir>/__mocks__/framer-motion.js',
    '^@ai-sdk/react$': '<rootDir>/__mocks__/@ai-sdk/react.js',
  },
  setupFiles: ['<rootDir>/jest.setup.js'],
  testEnvironmentOptions: {
    customExportConditions: [''],
  },
  // Use different test environments based on the test file
  projects: [
    {
      displayName: 'node',
      testEnvironment: 'node',
      testMatch: ['**/__tests__/**/*.test.ts'],
      transform: {
        '^.+\\.(ts|tsx)$': ['babel-jest', { configFile: './babel.config.test.js' }],
      },
      transformIgnorePatterns: [
        '/node_modules/(?!(.pnpm|vfile|unist|unified|bail|is-plain-obj|trough|remark|micromark|decode-named-character-reference|character-entities|property-information|hast|space-separated-tokens|comma-separated-tokens|mdast|markdown-table|trim-lines|string-width|strip-ansi|ansi-regex|is-fullwidth-code-point|emoji-regex|character-entities-legacy|character-reference-invalid|@ai-sdk|ai))',
      ],
    },
    {
      displayName: 'jsdom',
      testEnvironment: 'jsdom',
      testMatch: ['**/tests/**/*.test.tsx'],
      transform: {
        '^.+\\.(ts|tsx)$': ['babel-jest', { configFile: './babel.config.test.js' }],
      },
      transformIgnorePatterns: [
        '/node_modules/(?!(.pnpm|vfile|unist|unified|bail|is-plain-obj|trough|remark|micromark|decode-named-character-reference|character-entities|property-information|hast|space-separated-tokens|comma-separated-tokens|mdast|markdown-table|trim-lines|string-width|strip-ansi|ansi-regex|is-fullwidth-code-point|emoji-regex|character-entities-legacy|character-reference-invalid|@ai-sdk|ai))',
      ],
    },
  ],
};



================================================
File: jest.setup.js
================================================
// This file is used to set up the testing environment for Jest
const path = require('path');
const dotenv = require('dotenv');

// Load environment variables from .env.local
dotenv.config({ path: path.resolve(process.cwd(), '.env.local') });

// Mock console.error and console.warn to keep test output clean
// but still capture the messages for assertion if needed
global.originalConsoleError = console.error;
global.originalConsoleWarn = console.warn;

console.error = (...args) => {
  global.lastConsoleError = args;
};

console.warn = (...args) => {
  global.lastConsoleWarn = args;
};

// Mock URL.createObjectURL for file previews
if (typeof window !== 'undefined') {
  window.URL.createObjectURL = jest.fn(() => 'mock-url');
  window.URL.revokeObjectURL = jest.fn();
}

// Mock ResizeObserver
if (typeof window !== 'undefined') {
  window.ResizeObserver = jest.fn().mockImplementation(() => ({
    observe: jest.fn(),
    unobserve: jest.fn(),
    disconnect: jest.fn(),
  }));
}

// Mock IntersectionObserver
if (typeof window !== 'undefined') {
  window.IntersectionObserver = jest.fn().mockImplementation(() => ({
    observe: jest.fn(),
    unobserve: jest.fn(),
    disconnect: jest.fn(),
  }));
}

// We'll handle cleanup in individual test files instead
// since afterAll is not available in the setup file



================================================
File: middleware.ts
================================================
import { NextResponse, type NextRequest } from "next/server";
import { createClient } from "@/utils/supabase/middleware";

export async function middleware(request: NextRequest) {
  const { supabase, response } = createClient(request);
  
  // Refresh session if expired - required for Server Components
  // https://supabase.com/docs/guides/auth/auth-helpers/nextjs#managing-session-with-middleware
  await supabase.auth.getSession();
  
  return response;
}

export const config = {
  matcher: [
    /*
     * Match all request paths except for the ones starting with:
     * - _next/static (static files)
     * - _next/image (image optimization files)
     * - favicon.ico (favicon file)
     * - public (public files)
     */
    "/((?!_next/static|_next/image|favicon.ico|public).*)",
  ],
};



================================================
File: next.config.ts
================================================
import type { NextConfig } from 'next';

const nextConfig: NextConfig = {
  transpilePackages: ['geist'],
  images: {
    remotePatterns: [
      {
        hostname: 'vercel.com',
      },
    ],
  },
};

export default nextConfig;



================================================
File: package.json
================================================
{
  "name": "v44-gkennedy-ai-search-no-auth",
  "version": "0.4.4",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint",
    "test": "jest",
    "test:watch": "jest --watch",
    "test:models": "node scripts/test-models.js",
    "test:all": "./scripts/run-all-tests.sh"
  },
  "dependencies": {
    "@ai-sdk/anthropic": "1.1.18",
    "@ai-sdk/google": "^1.1.26",
    "@ai-sdk/groq": "1.1.15",
    "@ai-sdk/mistral": "1.1.18",
    "@ai-sdk/openai": "1.2.6",
    "@ai-sdk/perplexity": "1.0.8",
    "@ai-sdk/react": "1.1.24",
    "@openrouter/ai-sdk-provider": "^0.4.3",
    "@supabase/ssr": "^0.6.1",
    "@supabase/supabase-js": "^2.49.1",
    "@tavily/core": "^0.3.2",
    "@testing-library/react": "^16.2.0",
    "@vercel/analytics": "^1.5.0",
    "@vercel/blob": "^0.27.3",
    "@vercel/postgres": "^0.10.0",
    "ai": "4.1.63",
    "classnames": "^2.5.1",
    "clsx": "^2.1.1",
    "framer-motion": "^12.5.0",
    "geist": "^1.3.1",
    "lucide-react": "^0.482.0",
    "next": "15.2.2",
    "react": "^19.0.0",
    "react-dom": "^19.0.0",
    "react-markdown": "^10.1.0",
    "sonner": "^2.0.1",
    "tailwind-merge": "^3.0.2",
    "zod": "^3.24.2"
  },
  "devDependencies": {
    "@babel/preset-env": "^7.26.9",
    "@babel/preset-react": "^7.26.3",
    "@babel/preset-typescript": "^7.26.0",
    "@eslint/eslintrc": "^3.3.0",
    "@testing-library/jest-dom": "^6.6.3",
    "@types/jest": "^29.5.14",
    "@types/node": "^20.17.24",
    "@types/react": "^19.0.12",
    "@types/react-dom": "^19.0.4",
    "babel-jest": "^29.7.0",
    "dotenv": "^16.4.7",
    "eslint": "^9.22.0",
    "eslint-config-next": "15.2.2",
    "identity-obj-proxy": "^3.0.0",
    "jest": "^29.7.0",
    "jest-environment-jsdom": "^29.7.0",
    "postcss": "^8.5.3",
    "tailwindcss": "^3.4.1",
    "ts-jest": "^29.2.6",
    "typescript": "^5.7.2"
  },
  "packageManager": "pnpm@10.4.0+sha512.6b849d0787d97f8f4e1f03a9b8ff8f038e79e153d6f11ae539ae7c435ff9e796df6a862c991502695c7f9e8fac8aeafc1ac5a8dab47e36148d183832d886dd52",
  "pnpm": {
    "onlyBuiltDependencies": [
      "bufferutil",
      "core-js",
      "sharp"
    ]
  }
}



================================================
File: postcss.config.mjs
================================================
/** @type {import('postcss-load-config').Config} */
const config = {
  plugins: {
    tailwindcss: {},
  },
};

export default config;



================================================
File: react.d.ts
================================================
// React type declarations
import * as React from 'react';

declare global {
  namespace React {
    interface FC<P = {}> {
      (props: P & { children?: React.ReactNode }): React.ReactElement | null;
    }
    
    type ReactNode = 
      | React.ReactElement
      | string
      | number
      | boolean
      | null
      | undefined
      | React.ReactNodeArray;
      
    interface ReactElement<P = any, T extends string | JSXElementConstructor<any> = string | JSXElementConstructor<any>> {
      type: T;
      props: P;
      key: Key | null;
    }
    
    type Key = string | number;
    
    type ReactNodeArray = Array<ReactNode>;
    
    interface JSXElementConstructor<P> {
      (props: P): ReactElement<P, any> | null;
    }
  }
}



================================================
File: tailwind.config.ts
================================================
import type { Config } from "tailwindcss";

export default {
  content: [
    "./pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./components/**/*.{js,ts,jsx,tsx,mdx}",
    "./app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      colors: {
        background: "var(--background)",
        foreground: "var(--foreground)",
      },
      fontFamily: {
        sans: ["var(--font-geist-sans)"],
        mono: ["var(--font-geist-mono)"],
      },
    },
  },
  plugins: [],
} satisfies Config;



================================================
File: test-openai.sh
================================================
export OPENAI_API_KEY=sk-test-key



================================================
File: test.txt
================================================
This is a test file to check write access to the main directory.



================================================
File: tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}



================================================
File: types.d.ts
================================================
// Type declarations for modules without type definitions

declare module 'sonner' {
  export const Toaster: React.FC<{
    position?: 'top-left' | 'top-right' | 'bottom-left' | 'bottom-right' | 'top-center' | 'bottom-center';
    [key: string]: any;
  }>;
  export function toast(message: string, options?: any): void;
  export namespace toast {
    function error(message: string, options?: any): void;
    function success(message: string, options?: any): void;
    function warning(message: string, options?: any): void;
    function info(message: string, options?: any): void;
  }
}

declare module 'geist/font/sans' {
  const GeistSans: {
    variable: string;
    [key: string]: any;
  };
  export { GeistSans };
}

declare module 'geist/font/mono' {
  const GeistMono: {
    variable: string;
    [key: string]: any;
  };
  export { GeistMono };
}

declare module 'framer-motion' {
  export const motion: {
    [key: string]: any;
    div: any;
    span: any;
  };
}

declare module 'lucide-react' {
  export const LayoutDashboard: React.FC<{ size?: number; className?: string }>;
  export const Search: React.FC<{ size?: number; className?: string }>;
  export const MessageSquare: React.FC<{ size?: number; className?: string }>;
  export const Bot: React.FC<{ size?: number; className?: string }>;
  export const PlusCircle: React.FC<{ size?: number; className?: string }>;
  export const MoreVertical: React.FC<{ size?: number; className?: string }>;
  export const History: React.FC<{ size?: number; className?: string }>;
  export const Trash2: React.FC<{ size?: number; className?: string }>;
  export const Copy: React.FC<{ size?: number; className?: string }>;
  export const Clock: React.FC<{ size?: number; className?: string }>;
  export const Menu: React.FC<{ size?: number; className?: string }>;
  export const X: React.FC<{ size?: number; className?: string }>;
}

declare module 'next/navigation' {
  export function useRouter(): {
    push: (url: string) => void;
    replace: (url: string) => void;
    back: () => void;
    forward: () => void;
    refresh: () => void;
    prefetch: (url: string) => void;
  };
}

declare module 'next' {
  export interface Metadata {
    title?: string;
    description?: string;
    [key: string]: any;
  }
}

// Extend JSX namespace
declare namespace JSX {
  interface IntrinsicElements {
    [elemName: string]: any;
  }
}



================================================
File: .env.example
================================================
# Get your Anthropic API Key here: https://docs.anthropic.com/en/api/admin-api/apikeys/get-api-key
ANTHROPIC_API_KEY=****

# Get your DeepSeek API Key here: https://deepseek.com/ (optional)
# DEEPSEEK_API_BASE_URL=https://api.deepseek.com
# DEEPSEEK_API_KEY=****

# Get your Google API Key here: https://aistudio.google.com/apikey 
GOOGLE_GENERATIVE_AI_API_KEY=****

# Get your Groq API Key here: https://console.groq.com/keys
GROQ_API_KEY=****

# Get your Mistral API Key here: https://console.mistral.ai/api-keys
MISTRAL_API_KEY=****

# Get your OpenAI API Key here: https://platform.openai.com/account/api-keys
OPENAI_API_KEY=****

# Get your OpenRouter API Key here: https://openrouter.ai/settings/keys
OPENROUTER_API_KEY=****

# Get your Perplexity API Key here: https://openrouter.ai/settings/keys
PERPLEXITY_API_KEY=****

# SUPABASE DATABASE - VERCEL INTEGRATION
NEXT_PUBLIC_SUPABASE_URL=https://ldpydcddbjvywqmwlusu.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=****
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key

# Database URL (for direct database access if needed)
DATABASE_URL=your_database_connection_string

# VERCEL BLOB TOKEN
BLOB_READ_WRITE_TOKEN=****

# STORAGE
# VERCEL BLOB STORE NAME 
# Store Name: 
# This ID uniquely identifies this store: 
# This is the Base URL for all blobs in this store
# 

# VERCEL SECRET
# Generate a random secret: https://generate-secret.vercel.app/32 or `openssl rand -base64 32`
AUTH_SECRET=****

# Application Configuration
NEXT_PUBLIC_APP_URL=http://localhost:3000
NODE_ENV=development

# Optional: Analytics and Monitoring
# SENTRY_DSN=your_sentry_dsn

# Optional: Rate Limiting
# RATE_LIMIT_REQUESTS=100
# RATE_LIMIT_WINDOW=60

# SEARCH AND EXTRACTION

# Firecrawl
# FIRECRAWL_API_KEY=****

# Tavily API Key
TAVILY_API_KEY=****




================================================
File: .windsurfrules
================================================
You are an expert in TypeScript, Node.js, Next.js App Router, React, Vercel AI SDK, Shadcn UI, Radix UI, Tailwind, Posthog, Jest and Playwright.

DO NOT MODIFY THE CHAT UI, AI SEARCH UI, or AI AGENT PAGE UI
DO NOT MODIFY THE CHAT UI INPUT, AI SEARCH UI INPUT, or AI AGENT UI INPUT
YOU CAN ONLY MODIFY THE UI SIDEBAR NO OTHER UI FILES
DO NOT INSTALL OR MODIFY ANY DEPENDENCIES WITHOUT USER PERMISSION
DO NOT MODIFY the models.ts file WITHOUT USER PERMISSION

  
  Code Style and Structure
  - Write concise, technical TypeScript code with accurate comments.
  - Use functional and declarative programming patterns; avoid classes.
  - Prefer iteration and modularization over code duplication.
  - Use descriptive variable names with auxiliary verbs (e.g., isLoading, hasError).
  - Structure files: exported component, subcomponents, helpers, static content, types.
  
  Naming Conventions
  - Use lowercase with dashes for directories (e.g., components/auth-wizard).
  - Favor named exports for components.
  
  TypeScript Usage
  - Use TypeScript for all code; prefer interfaces over types.
  - Avoid enums; use maps instead.
  - Use functional components with TypeScript interfaces.
  
  Syntax and Formatting
  - Use the "function" keyword for pure functions.
  - Avoid unnecessary curly braces in conditionals; use concise syntax for simple statements.
  - Use declarative JSX.
  
  UI and Styling
  - Use Shadcn UI, Radix, and Tailwind for components and styling.
  - When performing updates or refactores allways Implement responsive design with Tailwind CSS; use a mobile-first approach.
  - Utilize industry best Apple and AirBnB Design Responsive Design Mobile first Standards
  - Never break the UI, responsiveness or app features or functionality 
  
  Performance Optimization
  - Minimize 'use client', 'useEffect', and 'setState'; favor React Server Components (RSC).
  - Wrap client components in Suspense with fallback.
  - Use dynamic loading for non-critical components.
  - Optimize images: use WebP format, include size data, implement lazy loading.
  
  Key Conventions
  - Use 'nuqs' for URL search parameter state management.
  - Optimize Web Vitals (LCP, CLS, FID).
  - Limit 'use client':
    - Favor server components and Next.js SSR.
    - Use only for Web API access in small components.
    - Avoid for data fetching or state management.

  Never change the models.ts file without explicit user permission
  Take your time. When in doubt, ask the user


================================================
File: __mocks__/fileMock.js
================================================
module.exports = 'test-file-stub';



================================================
File: __mocks__/framer-motion.js
================================================
// Mock for framer-motion
const motion = {
  div: ({ children, ...props }) => children,
};

const AnimatePresence = ({ children }) => children;

module.exports = {
  motion,
  AnimatePresence,
};



================================================
File: __mocks__/react-markdown.js
================================================
// Mock for react-markdown
const ReactMarkdown = ({ children }) => {
  return children;
};

module.exports = ReactMarkdown;



================================================
File: __mocks__/@ai-sdk/react.js
================================================
// Mock for @ai-sdk/react
const useChat = jest.fn().mockReturnValue({
  messages: [],
  append: jest.fn(),
  isLoading: false,
  input: '',
  setInput: jest.fn(),
  handleSubmit: jest.fn(),
  reload: jest.fn(),
  stop: jest.fn(),
  error: null,
  data: null,
});

module.exports = {
  useChat,
};



================================================
File: __mocks__/lib/models.js
================================================
// Mock for @/lib/models
const models = [
  {
    id: 'anthropic/claude-3-haiku',
    name: 'Claude 3 Haiku',
    description: 'Anthropic Claude 3 Haiku',
    features: ['multimodal'],
  },
  {
    id: 'anthropic/claude-3-sonnet',
    name: 'Claude 3 Sonnet',
    description: 'Anthropic Claude 3 Sonnet',
    features: ['multimodal'],
  },
  {
    id: 'openai/gpt-4o',
    name: 'GPT-4o',
    description: 'OpenAI GPT-4o',
    features: ['multimodal'],
  }
];

module.exports = {
  models
};



================================================
File: __mocks__/lib/posthog.js
================================================
// Mock for @/lib/posthog
module.exports = {};



================================================
File: ai_agents_output/ai-agents-2025-03-17T16:47:18.372Z-claude-3.7-sonnet-o3-mini.json
================================================
{
  "timestamp": "2025-03-17T16:47:18.372Z",
  "query": "What are the latest developments in AI chip manufacturing",
  "primary": {
    "model": {
      "provider": "anthropic",
      "model": "claude-3.7-sonnet",
      "label": "Claude 3.7 Sonnet"
    },
    "results": {
      "id": "msg_01VjMVL5rHxNk9EQgGeGKa8P",
      "timestamp": "2025-03-17T16:47:19.923Z",
      "modelId": "claude-3-7-sonnet-20250219",
      "headers": {
        "anthropic-organization-id": "712bccee-73e8-4865-a99a-a46408c6717e",
        "anthropic-ratelimit-input-tokens-limit": "20000",
        "anthropic-ratelimit-input-tokens-remaining": "20000",
        "anthropic-ratelimit-input-tokens-reset": "2025-03-17T16:48:31Z",
        "anthropic-ratelimit-output-tokens-limit": "8000",
        "anthropic-ratelimit-output-tokens-remaining": "5000",
        "anthropic-ratelimit-output-tokens-reset": "2025-03-17T16:48:51Z",
        "anthropic-ratelimit-requests-limit": "50",
        "anthropic-ratelimit-requests-remaining": "49",
        "anthropic-ratelimit-requests-reset": "2025-03-17T16:48:31Z",
        "anthropic-ratelimit-tokens-limit": "28000",
        "anthropic-ratelimit-tokens-remaining": "25000",
        "anthropic-ratelimit-tokens-reset": "2025-03-17T16:48:31Z",
        "cache-control": "no-cache",
        "cf-cache-status": "DYNAMIC",
        "cf-ray": "921dfa6b0c0e6930-LIS",
        "connection": "keep-alive",
        "content-type": "text/event-stream; charset=utf-8",
        "date": "Mon, 17 Mar 2025 16:48:31 GMT",
        "request-id": "req_01RAWWGLa8YP1ovSii1kJLGk",
        "server": "cloudflare",
        "transfer-encoding": "chunked",
        "via": "1.1 google",
        "x-robots-tag": "none"
      },
      "messages": [
        {
          "role": "assistant",
          "content": [
            {
              "type": "text",
              "text": ""
            },
            {
              "type": "tool-call",
              "toolCallId": "toolu_01HiWddKgUTJXKvCAa773N1f",
              "toolName": "tavily",
              "args": {
                "query": "latest developments in AI chip manufacturing 2024"
              }
            }
          ],
          "id": "msg-hAtBT5V47g2Z9hdb6asj3Knf"
        },
        {
          "role": "tool",
          "id": "msg-DT4hhfAjdoHFH1vRxEY6wU4g",
          "content": [
            {
              "type": "tool-result",
              "toolCallId": "toolu_01HiWddKgUTJXKvCAa773N1f",
              "toolName": "tavily",
              "result": "{\"query\":\"latest developments in AI chip manufacturing 2024\",\"responseTime\":2.72,\"images\":[],\"results\":[{\"title\":\"Built for AI, this chip moves beyond transistors for huge ...\",\"url\":\"https://engineering.princeton.edu/news/2024/03/06/built-ai-chip-moves-beyond-transistors-huge-computational-gains\",\"content\":\"Princeton University\\nPrinceton Engineering\\nBuilt for AI, this chip moves beyond transistors for huge computational gains\\nBy Scott Lyon\\nMarch 6, 2024\\nPrinceton researchers have totally reimagined the physics of computing to build a chip for modern AI workloads, and with new U.S. government backing they will see how fast, compact and power-efficient this chip can get. An early prototype is pictured above. Photo by Photo by Hongyang Jia/Princeton University\",\"rawContent\":null,\"score\":0.8396422},{\"title\":\"10 top AI hardware and chip-making companies in 2025 - TechTarget\",\"url\":\"https://www.techtarget.com/searchdatacenter/tip/Top-AI-hardware-companies\",\"content\":\"The company's NVLink technology can connect the Grace Hopper superchip to other superchips. NVLink enables multiple GPUs to communicate through high-speed interconnection.\\nThe Blackwell B200 AI chip, a GPU microarchitecture, has been delayed and should be released in late 2024, with B200A planned for 2025. Nvidia also plans to launch a new accelerator, Rubin, in 2026.\\nQualcomm [...] Part of: The future of AI hardware in the data center\\nArticle 1 of 4\\nUp Next\\n10 top AI hardware and chip-making companies in 2024Due to rapid AI hardware advancement, companies are releasing advanced products yearly to keep up with the competition. The new competitive product on the market is the AI chip.\\nA primer on AI chip designFour common AI chips -- CPU, GPU, FPGA and ASIC -- are advancing with the current market for AI chip design. Read on to see what the future holds for AI chips. [...] AMD created its next generation of Epyc and Ryzen processors. The company released its latest product, the Zen 5 CPU microarchitecture, in 2024.\\nAMD released the MI300A and MI300X AI chips in December 2023. MI300A has a GPU with 228 compute units and 24 CPU cores, while the MI300X chip is a GPU model with 304 compute units. The MI300X and Nvidia's H100 rival in memory capacity and bandwidth.\\nApple\",\"rawContent\":null,\"score\":0.7228212569190325},{\"title\":\"2024 Semiconductor Industry Outlook | Deloitte US\",\"url\":\"https://www2.deloitte.com/us/en/pages/technology-media-and-telecommunications/articles/semiconductor-industry-outlook.html\",\"content\":\"Generative AI accelerator chips and how semiconductor companies are using GenAI\\n\\nTrends around smart manufacturing  \\n\\n\\nThe need for more assembly and test capacity worldwide  \\n\\n\\nHow chip industry IP is a target for cyberattacks at a whole new threat level  \\n\\n\\nAnd a final geopolitics section that looks at export controls around advanced node manufacturing equipment and technologies, as well as advanced GenAI semiconductors [...] Perspectives\\n2024 semiconductor industry outlook\\nTrends and predictions for a cyclical industry\\nLed by Generative AI, chip sales look to bounce back in 2024—but geopolitics could complicate growth in the semiconductor industry. Learn more about trends, challenges, and new possibilities for the year ahead in our 2024 semiconductor industry outlook.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSave for later\",\"rawContent\":null,\"score\":0.6956670962145111},{\"title\":\"2025 global semiconductor industry outlook - Deloitte\",\"url\":\"https://www2.deloitte.com/us/en/insights/industry/technology/technology-media-telecom-outlooks/semiconductor-industry-outlook.html\",\"content\":\"54.  Deloitte, “How generative AI is transforming the semiconductor industry”; Belle Lin, “Designing chips is getting harder. These engineers say chatbots and AI can help,” The Wall Street Journal, Feb. 6, 2024.\\nView in Article [...] Deloitte predicted that, by 2023, AI would emerge as a powerful aid to human semiconductor engineers, assisting them on extremely complex chip-design processes, and enabling them to find ways to improve and optimize PPA (power, performance, and area).36 As of 2024, gen AI has enabled rapid iterations to enhance existing designs and discover entirely new ones that can do it in less time.37 In 2025, there will likely be more emphasis toward “shift left”—an approach to chip design and development [...] Competition from agile chip startups could intensify, challenging incumbents in the broader semiconductor industry. Notably, AI chip startups secured a cumulative US$7.6 billion in venture capital funding globally during the second, third, and last quarter of 2024,76 and several of these startups offer specialized solutions including customizable RISC-V-based applications, chiplets, LLM inference chips, photonic ICs, chip design, and chip equipment.\",\"rawContent\":null,\"score\":0.6791461745110409}],\"answer\":\"AI chip manufacturing in 2024 saw significant advancements, with new designs and government support for efficiency. Generative AI is transforming semiconductor design, and startups are gaining traction with specialized solutions. The industry anticipates further competition and innovation.\"}"
            }
          ]
        }
      ]
    }
  },
  "secondary": {
    "model": {
      "provider": "openai",
      "model": "o3-mini",
      "label": "O3 Mini"
    },
    "results": {
      "id": "chatcmpl-BC7vf8aXMvVi4b0l7LjzBHeY27Ii9",
      "timestamp": "2025-03-17T16:48:35.000Z",
      "modelId": "o3-mini-2025-01-31",
      "headers": {
        "access-control-expose-headers": "X-Request-ID",
        "alt-svc": "h3=\":443\"; ma=86400",
        "cf-cache-status": "DYNAMIC",
        "cf-ray": "921dfa8ce85a7111-LIS",
        "connection": "keep-alive",
        "content-type": "text/event-stream; charset=utf-8",
        "date": "Mon, 17 Mar 2025 16:48:41 GMT",
        "openai-organization": "user-dyljpskokehpalzgl6jwwhen",
        "openai-processing-ms": "5440",
        "openai-version": "2020-10-01",
        "server": "cloudflare",
        "set-cookie": "_cfuvid=tqQQN41VUevuU9kveL7TD4wovwKOyUGUgAzaI6Wrezo-1742230121394-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None",
        "strict-transport-security": "max-age=31536000; includeSubDomains; preload",
        "transfer-encoding": "chunked",
        "x-content-type-options": "nosniff",
        "x-ratelimit-limit-requests": "5000",
        "x-ratelimit-limit-tokens": "4000000",
        "x-ratelimit-remaining-requests": "4999",
        "x-ratelimit-remaining-tokens": "3998606",
        "x-ratelimit-reset-requests": "12ms",
        "x-ratelimit-reset-tokens": "20ms",
        "x-request-id": "req_93b115f10736ea512fa6ff8ae7291842"
      },
      "messages": [
        {
          "role": "assistant",
          "content": [
            {
              "type": "text",
              "text": "Recent developments in AI chip manufacturing show a significant blend of breakthrough hardware innovations, advanced design methodologies powered by generative AI, and a rapidly evolving competitive landscape. Here are the key insights:\n\n1. New Architectural Breakthroughs  \n • Researchers at Princeton University are pushing the envelope by exploring alternatives to traditional transistor-based designs. Their new chip architecture leverages novel physics to achieve higher computational gains while remaining compact and power efficient. Government support is accelerating the transition of these early prototypes toward practical applications in AI workloads.\n\n2. Industry Product Refreshes and Upcoming Releases  \n • Major players continue to evolve their product offerings. For example, Nvidia is enhancing its GPU lineup with technologies like NVLink, which enables high-speed interconnection between superchips. Upcoming releases are anticipated, with Nvidia’s accelerator prototypes and new architectures in the pipeline.\n • AMD has made headlines with its MI300 series—dubbed MI300A and MI300X. These chips provide competitive memory bandwidth and compute power, positioning them well against rivals (such as Nvidia’s H100 chips) for demanding AI applications.\n • Other companies, including Qualcomm and Apple, are also ramping up their efforts, reflecting the sector’s dynamism and continuous innovation.\n\n3. Generative AI in Chip Design and Optimization  \n • The semiconductor industry is increasingly relying on generative AI tools to aid chip design. These AI-powered systems help engineers rapidly iterate and optimize designs by exploring vast design spaces and improving key metrics like power, performance, and area (PPA). This integration not only speeds up the design cycle but also uncovers innovative architectures that might have been too complex to design manually.\n\n4. Startup Momentum and Venture Capital Inﬂows  \n • The surge in venture capital funding, particularly for AI chip startups, is fueling innovation. These startups are developing specialized solutions—ranging from customizable RISC-V based chips and chiplets to photonic integrated circuits—that challenge incumbents. This influx of funding is fostering a more agile and diverse chip ecosystem, with startups focusing on niche applications like large language model (LLM) inference.\n\n5. Broader Industry and Geopolitical Considerations  \n • There is a notable emphasis on expanding assembly and testing capacities worldwide to meet the growing demand for advanced chips. However, geopolitics remains a critical factor. Export controls on advanced semiconductor manufacturing equipment and intellectual property challenges add complexity, potentially influencing where and how next-generation AI chips are developed and deployed.\n\nIn summary, the latest developments in AI chip manufacturing reflect a confluence of breakthrough hardware innovations, smarter design processes leveraging generative AI, aggressive product launches from established companies, and a vibrant ecosystem enriched by startup creativity, all taking place in a complex global environment."
            }
          ],
          "id": "msg-qWcCXgWIsRNtmxuIieHLnuAn"
        }
      ]
    }
  }
}


================================================
File: ai_agents_output/ai-agents-2025-03-17T16:53:03.324Z-gemini-2.0-flash-claude-3.5-sonnet.json
================================================
{
  "timestamp": "2025-03-17T16:53:03.324Z",
  "query": "What are the current trends in renewable energy in this year, 2025?  Also please tell me which llm you are?  Are you from Google and Anthropic?  ",
  "primary": {
    "model": {
      "provider": "google",
      "model": "gemini-2.0-flash",
      "label": "Gemini 2.0 Flash"
    },
    "results": {
      "id": "aitxt-DA40PnEmyGaAan5uyT1ZDdwu",
      "timestamp": "2025-03-17T16:53:03.889Z",
      "modelId": "gemini-2.0-flash",
      "headers": {
        "alt-svc": "h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000",
        "content-disposition": "attachment",
        "content-type": "text/event-stream",
        "date": "Mon, 17 Mar 2025 16:54:15 GMT",
        "server": "scaffolding on HTTPServer2",
        "server-timing": "gfet4t7; dur=424",
        "transfer-encoding": "chunked",
        "vary": "Origin, X-Origin, Referer",
        "x-content-type-options": "nosniff",
        "x-frame-options": "SAMEORIGIN",
        "x-xss-protection": "0"
      },
      "messages": [
        {
          "role": "assistant",
          "content": [
            {
              "type": "text",
              "text": ""
            },
            {
              "type": "tool-call",
              "toolCallId": "3DeF0X9UDzMqtSjc",
              "toolName": "tavily",
              "args": {
                "query": "current trends in renewable energy 2025"
              }
            }
          ],
          "id": "msg-9iFp3EL6yGz4jjMsw0AUxkgm"
        },
        {
          "role": "tool",
          "id": "msg-sPpMy67A9vzBzHrLzb7Uk6Ft",
          "content": [
            {
              "type": "tool-result",
              "toolCallId": "3DeF0X9UDzMqtSjc",
              "toolName": "tavily",
              "result": "{\"query\":\"current trends in renewable energy 2025\",\"responseTime\":2.21,\"images\":[],\"results\":[{\"title\":\"5 renewable energy trends to follow in 2025 - Ecohz\",\"url\":\"https://www.ecohz.com/blog/renewable-energy-trends-2025\",\"content\":\"Written by Ecohz\\nPublished on 01 January 2025\\nLike every year, we are looking ahead to identify key trends for the next 12 months. From emerging conversations in the renewables industry to policy changes that will significantly impact clean energy development, here are five topics that corporate buyers of renewable energy should keep an eye on in 2025.\\n1 Location and time matching vs. cross-border emissionality [...] Renewable energy remains a reason for optimism. According to projections by the International Energy Agency (IEA), renewable electricity will overtake coal-generated electricity for the first time in 2025, accounting for 35% of global electricity supply. Solar PV, which keeps getting cheaper, “is expected to meet roughly half of the growth in global electricity demand over 2024 and 2025.” [...] On one side, some advocate for stricter geographic boundaries in market-based renewable procurement, coupled with closer alignment between renewable energy production and consumption times. This approach, they argue, could drive investment in regions that need greater renewable energy deployment, particularly in countries that currently import most of their Energy Attribute Certificates (EACs).\",\"rawContent\":null,\"score\":0.8739949197849463},{\"title\":\"5 trends shaping the energy world in 2025 | World Economic Forum\",\"url\":\"https://www.weforum.org/stories/2025/03/5-energy-trends-2025/\",\"content\":\"5. Power and renewables: a year of accelerating innovation\\nLengthy permitting and interconnection processes have been two of the biggest bottlenecks to faster renewables growth in power systems globally. There are signs that the penny has dropped, and that 2025 could be the start of a new era for renewables. Germany has demonstrated the impact permitting reform can make: Since implementing reforms in 2022, awarded permits for onshore wind have increased 150%. [...] What are the opportunities, challenges and risks for the energy and natural resources sectors in 2025? Here are five key trends.\\n1. Brave new world\\nThe new Trump administration is out to make a mark in global trade, climate policy and geopolitics, all with potentially profound implications for energy and natural resources. [...] 2. Capital allocation and finance\\nInvestment in the supply of energy and natural resources will reach record levels in 2025, with spend exceeding $1.5 trillion over our forecasts, up 6% in real terms on 2024. The forecast is for capital investment across power and renewables (excluding wires), upstream oil and gas and metals and mining.\",\"rawContent\":null,\"score\":0.7713651544946236}],\"answer\":\"Renewable electricity will surpass coal in 2025, with solar PV meeting half of global electricity demand growth. Permitting reforms in Germany boosted onshore wind permits by 150%. Record capital investment of over $1.5 trillion is expected in energy and renewables sectors.\"}"
            }
          ]
        }
      ]
    }
  },
  "secondary": {
    "model": {
      "provider": "anthropic",
      "model": "claude-3.5-sonnet",
      "label": "Claude 3.5 Sonnet"
    },
    "results": {
      "id": "msg_01YJHqMujUUpX8TMPCGPQkWv",
      "timestamp": "2025-03-17T16:53:11.546Z",
      "modelId": "claude-3-5-sonnet-20241022",
      "headers": {
        "anthropic-organization-id": "712bccee-73e8-4865-a99a-a46408c6717e",
        "anthropic-ratelimit-input-tokens-limit": "40000",
        "anthropic-ratelimit-input-tokens-remaining": "39000",
        "anthropic-ratelimit-input-tokens-reset": "2025-03-17T16:54:19Z",
        "anthropic-ratelimit-output-tokens-limit": "8000",
        "anthropic-ratelimit-output-tokens-remaining": "5000",
        "anthropic-ratelimit-output-tokens-reset": "2025-03-17T16:54:39Z",
        "anthropic-ratelimit-requests-limit": "50",
        "anthropic-ratelimit-requests-remaining": "49",
        "anthropic-ratelimit-requests-reset": "2025-03-17T16:54:19Z",
        "anthropic-ratelimit-tokens-limit": "48000",
        "anthropic-ratelimit-tokens-remaining": "44000",
        "anthropic-ratelimit-tokens-reset": "2025-03-17T16:54:19Z",
        "cache-control": "no-cache",
        "cf-cache-status": "DYNAMIC",
        "cf-ray": "921e02ea1e4de3b0-LIS",
        "connection": "keep-alive",
        "content-type": "text/event-stream; charset=utf-8",
        "date": "Mon, 17 Mar 2025 16:54:22 GMT",
        "request-id": "req_01UNRjxVNE4x3CGFRGMKdmBC",
        "server": "cloudflare",
        "transfer-encoding": "chunked",
        "via": "1.1 google",
        "x-robots-tag": "none"
      },
      "messages": [
        {
          "role": "assistant",
          "content": [
            {
              "type": "text",
              "text": "I am Claude, an AI assistant created by Anthropic. I aim to be direct and honest about who I am - I'm not from Google or any other company. Let me share the latest renewable energy trends for 2025:\n\nKey trends include:\n\n1. Renewable electricity is expected to overtake coal-generated electricity for the first time in 2025, reaching 35% of global electricity supply\n\n2. Solar PV is playing a major role, projected to meet approximately half of the growth in global electricity demand in 2024-2025\n\n3. Significant reforms in permitting processes, with Germany showing success after implementing reforms that increased onshore wind permits by 150%\n\n4. Record capital investment expected in 2025, exceeding $1.5 trillion across power and renewables sectors (6% increase from 2024)\n\n5. Growing focus on location and time matching in renewable energy procurement, with discussions about stricter geographic boundaries for market-based procurement\n\nThese trends indicate a significant shift toward renewable energy adoption and infrastructure development, with both policy reforms and increased investment supporting the transition."
            }
          ],
          "id": "msg-aQHl8dD2fRrsgDPX6Xb4lWWt"
        }
      ]
    }
  }
}


================================================
File: ai_agents_output/ai-agents-2025-03-17T17:00:36.069Z-gemini-2.0-flash-claude-3.5-sonnet.json
================================================
{
  "timestamp": "2025-03-17T17:00:36.069Z",
  "query": "What are the latest developments in AI chip manufacturing for the last quarter of 2024, the first quarter of 2025 (today is March 17, 2025) and for the remainder of 2025?",
  "primary": {
    "model": {
      "provider": "google",
      "model": "gemini-2.0-flash",
      "label": "Gemini 2.0 Flash"
    },
    "results": {
      "id": "aitxt-dnBifFw25u8POWwCnRBGXGTb",
      "timestamp": "2025-03-17T17:00:37.185Z",
      "modelId": "gemini-2.0-flash",
      "headers": {
        "alt-svc": "h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000",
        "content-disposition": "attachment",
        "content-type": "text/event-stream",
        "date": "Mon, 17 Mar 2025 17:01:48 GMT",
        "server": "scaffolding on HTTPServer2",
        "server-timing": "gfet4t7; dur=662",
        "transfer-encoding": "chunked",
        "vary": "Origin, X-Origin, Referer",
        "x-content-type-options": "nosniff",
        "x-frame-options": "SAMEORIGIN",
        "x-xss-protection": "0"
      },
      "messages": [
        {
          "role": "assistant",
          "content": [
            {
              "type": "text",
              "text": ""
            },
            {
              "type": "tool-call",
              "toolCallId": "mWHOS5NvFVgurFxi",
              "toolName": "tavily",
              "args": {
                "query": "AI chip manufacturing developments Q4 2024"
              }
            },
            {
              "type": "tool-call",
              "toolCallId": "MfrlLEj0cbyjj5jh",
              "toolName": "tavily",
              "args": {
                "query": "AI chip manufacturing developments Q1 2025"
              }
            },
            {
              "type": "tool-call",
              "toolCallId": "vVI3RlgXee9fmEX0",
              "toolName": "tavily",
              "args": {
                "query": "AI chip manufacturing developments March-December 2025"
              }
            }
          ],
          "id": "msg-JOAt0iQL3rfioh5bTmhOI8Qq"
        },
        {
          "role": "tool",
          "id": "msg-NJCgg28jTrCujFKwu0fGmXUm",
          "content": [
            {
              "type": "tool-result",
              "toolCallId": "mWHOS5NvFVgurFxi",
              "toolName": "tavily",
              "result": "{\"query\":\"AI chip manufacturing developments Q4 2024\",\"responseTime\":1.89,\"images\":[],\"results\":[{\"title\":\"Global Semiconductor Manufacturing Industry Reports Solid Q4 ...\",\"url\":\"https://www.semi.org/en/semi-press-release/global-semiconductor-manufacturing-industry-reports-solid-q4-2024-results-semi-reports\",\"content\":\"Integrated circuit (IC) sales rose by 29% YoY in Q4 2024 and continued growth is expected in Q1 2025 with a 23% increase YoY as AI-fueled demand\",\"rawContent\":null,\"score\":0.8941352},{\"title\":\"Global Semiconductor Manufacturing Industry Reports Solid Q4 ...\",\"url\":\"https://www.prnewswire.com/in/news-releases/global-semiconductor-manufacturing-industry-reports-solid-q4-2024-results-semi-reports-302377472.html\",\"content\":\"Integrated circuit (IC) sales rose by 29% YoY in Q4 2024 and continued growth is expected in Q1 2025 with a 23% increase YoY as AI-fueled demand\",\"rawContent\":null,\"score\":0.8712086564102565},{\"title\":\"Startup Funding: Q4 2024 - Semiconductor Engineering\",\"url\":\"https://semiengineering.com/startup-funding-q4-2024/\",\"content\":\"The fourth quarter of 2024 saw five mega-rounds of over $100 million. One of the hottest areas continues to be AI hardware.\",\"rawContent\":null,\"score\":0.8478604076923076},{\"title\":\"TSMC Rides AI Wave to Record Profits in Q4 - EE Times\",\"url\":\"https://www.eetimes.com/tsmc-rides-ai-wave-to-record-profits-in-q4/\",\"content\":\"TSMC, reported record profits for the fourth quarter of 2024, fueled by booming demand for artificial intelligence chips.\",\"rawContent\":null,\"score\":0.8228707384615386},{\"title\":\"Chip design software maker Cadence revenue skyrockets with AI ...\",\"url\":\"https://www.manufacturingdive.com/news/cadence-q4-2024-earnings-grow-chipmakers-adopt-ai-software-intel-nvidia/740577/\",\"content\":\"Cadence's Q4 2024 earnings grew 26% over the prior year as the chip design software maker expanded its partnerships with Nvidia, Intel and others.\",\"rawContent\":null,\"score\":0.7593734615384615}],\"answer\":\"AI chip manufacturing saw significant growth in Q4 2024, with TSMC reporting record profits and IC sales rising by 29% YoY. The industry anticipates continued growth in Q1 2025. Cadence's earnings also grew due to AI software adoption.\"}"
            },
            {
              "type": "tool-result",
              "toolCallId": "MfrlLEj0cbyjj5jh",
              "toolName": "tavily",
              "result": "{\"query\":\"AI chip manufacturing developments Q1 2025\",\"responseTime\":2.34,\"images\":[],\"results\":[{\"title\":\"6 Tech Giants Dominating the 2025 Semiconductor & AI Chip Race\",\"url\":\"https://medium.com/@frulouis/6-tech-giants-dominating-the-2025-semiconductor-ai-chip-race-b9b3dac7e498\",\"content\":\"As we enter 2025, this evolution continues through the convergence of x86 and ARM architectures with specialized GPUs, creating more efficient and accessible AI computing solutions.\\nLeading tech companies like Intel, AMD, Nvidia, Amazon Web Services (AWS), Google, and Microsoft are developing diverse chip architectures to meet the escalating demands of modern computing. [...] Nvidia dominates AI chips with new Blackwell architecture and $3,000 Project Digits personal AI supercomputer, launching May 2025\\nAWS advances its custom silicon with Graviton4 processors and specialized AI/ML chips (Trainium and Inferentia), while partnering with Intel for new AI fabric chips [...] This diversification in chip architecture is reshaping the semiconductor industry as companies optimize for AI and cloud computing workloads.\\n1. Nvidia’s GPU Dominance\\nNVIDIA has significantly expanded its custom chip offerings to meet the diverse needs of modern computing, particularly in artificial intelligence (AI), gaming, and data centers. Here’s an overview of their recent developments:\\nBlackwell Architecture and GB10 Superchip\",\"rawContent\":null,\"score\":0.8601227522287967},{\"title\":\"Global Semiconductor Market to Grow by 15% in 2025, Driven ... - IDC\",\"url\":\"https://www.idc.com/getdoc.jsp?containerId=prAP52837624\",\"content\":\"manufacturing is projected to increase by 7% annually in 2025, with advanced nodes capacity rising by 12% annually. Average capacity utilization rate is expected to remain above 90% and the AI-driven semiconductor boom will continue. [...] “As AI continues to drive demand for high-end logic process chips and increases the penetration rate of high-priced high bandwidth memory (HBM), the overall semiconductor market is expected to have double-digit growth in 2025. The semiconductor supply chain - spanning design, manufacturing, testing, and advanced packaging --will create a new wave of growth opportunities under the cooperation between the upstream and downstream industries, “said Galen Zeng, Senior Research Manager at IDC [...] also deeply cultivating advanced packaging technology for AI chips. In 2025, China's packaging and testing market share will continue to rise, while Taiwanese players will consolidate their packaging advantages in high-end chips such as AI GPUs. The overall packaging and testing industry is expected to grow by 9% in 2025.\",\"rawContent\":null,\"score\":0.8497545984418147},{\"title\":\"4 semiconductor manufacturing trends to watch in 2025\",\"url\":\"https://www.manufacturingdive.com/news/semiconductor-industry-2025-outlook-chips-act-tariffs-ai/737302/\",\"content\":\"Search   \\n\\nOperations\\nProcurement\\nLabor\\nRegulation\\nTechnology\\nSustainability\\nOpenings & Expansions\\n\\nAn article from \\n4 semiconductor manufacturing trends to watch in 2025\\nA new administration, persistent AI-driven demand and ongoing talent shortages will influence chip manufacturing this year.\\nPublished Jan. 23, 2025\\n\\nJoelle Anselmo Staff Reporter\\n\\n post\\n share\\n post\\n print\\n email\\n license [...] Meanwhile, AI-driven technologies, including Nvidia processors and Intel’s advanced nodes, are spurring heightened demand for next-generation chips, while the automotive sector is poised for a surge in semiconductor use as electrification accelerates. [...] Despite these opportunities, the industry still faces challenges, including a persistent talent gap and a strain on resources in constructing fabrication plants. Workforce development initiatives, energy-efficient innovations and strategic responses to changing trade policies are key 2025 priorities for many chip companies as the U.S. tries to position itself as an industry leader against competitors like China and Taiwan.\\n1. AI, energy efficiency drive demand\",\"rawContent\":null,\"score\":0.8315676010749508},{\"title\":\"Exclusive: Meta begins testing its first in-house AI training chip\",\"url\":\"https://www.reuters.com/technology/artificial-intelligence/meta-begins-testing-its-first-in-house-ai-training-chip-2025-03-11/\",\"content\":\"Meta, which also owns Instagram and WhatsApp, has forecast total 2025 expenses of $114 billion to $119 billion, including up to $65 billion in capital expenditure largely driven by spending on AI infrastructure.\\nOne of the sources said Meta's new training chip is a dedicated accelerator, meaning it is designed to handle only AI-specific tasks. This can make it more power-efficient than the integrated graphics processing units (GPUs) generally used for AI workloads.\",\"rawContent\":null,\"score\":0.6922816526627219},{\"title\":\"2025 global semiconductor industry outlook - Deloitte\",\"url\":\"https://www2.deloitte.com/us/en/insights/industry/technology/technology-media-telecom-outlooks/semiconductor-industry-outlook.html\",\"content\":\"In mid-January 2025, the outgoing administration announced the Interim Final Rule on AI Technology Diffusion. The Interim Final Rule will impose new controls for chip exports.68\\nAt the time of writing, it is unknown whether the incoming administration will roll back the December and January restrictions, modify them, or even propose additional restrictions. [...] Some analysts expect the market for gen AI inference to grow faster than training in 2025 and beyond: What implications could this have on various semiconductor sectors and players? As the cost of AI inference reduces at a faster pace, how can it affect semiconductor chips? [...] Or copy link\\nCopy\\nThese trends and others play into our 2025 semiconductor industry outlook, where we drill down into four big topics for the year ahead: generative AI accelerator chips for PCs and smartphones and the enterprise edge; a new “shift-left” approach to chip design; the growing global talent shortage; and the need to build resilient supply chains amid escalating geopolitical tensions.\\nAbout Deloitte’s TMT center outlooks\",\"rawContent\":null,\"score\":0.6904868580276133}],\"answer\":\"AI chip manufacturing in Q1 2025 sees significant advancements, with major companies like Nvidia and Intel leading developments in new architectures and technologies. The semiconductor industry continues to grow, driven by AI demand and new packaging innovations. Talent shortages and geopolitical tensions remain key challenges.\"}"
            },
            {
              "type": "tool-result",
              "toolCallId": "vVI3RlgXee9fmEX0",
              "toolName": "tavily",
              "result": "{\"query\":\"AI chip manufacturing developments March-December 2025\",\"responseTime\":2.66,\"images\":[],\"results\":[{\"title\":\"Top 23 AI Chip Makers of 2025 - Statistics & Facts\",\"url\":\"https://seo.ai/blog/ai-chip-makers\",\"content\":\"GPUs are the favored chips for AI applications due to their ability to perform millions of mathematical operations in parallel.\\nTech firms like Microsoft, Amazon, and Google are innovating in the AI chips space. For example, Google has produced the Tensor Processing Unit (TPU), an application-specific integrated circuit developed specifically for AI activities.\\nThe AI chip market size in 2025 is expected to be 91.96 billion USD.\\nThe semiconductor market is expected to grow by 7.46% in 2025. [...] AI chips support various use cases, including image recognition, recommendation engines, natural language processing, and autonomous vehicles.\\nAI chips are increasingly being used in consumer devices like smartphones, laptops, and wearables, and in enterprise markets such as robotics and sensors.\\nNvidia is a leading name in the development of AI chips, with its chips being used to train and run various large language models, including the one developed by OpenAI. [...] OpenAI CEO, Sam Altman, is spearheading an audacious initiative to raise up to $7 trillion. This project aims to revolutionize the global semiconductor industry, significantly enhancing chip-building capacity and AI power. This massive investment underscores the critical role of AI chips in achieving Artificial General Intelligence (AGI).\\nWhile I doubt that such a colossal sum can be raised, this move tells us how seriously tech visionaries are taking the AI war.\\nTop 23 AI Chip Makers of 2025\",\"rawContent\":null,\"score\":0.8579266395525726},{\"title\":\"6 Tech Giants Dominating the 2025 Semiconductor & AI Chip Race\",\"url\":\"https://medium.com/@frulouis/6-tech-giants-dominating-the-2025-semiconductor-ai-chip-race-b9b3dac7e498\",\"content\":\"Nvidia dominates AI chips with new Blackwell architecture and $3,000 Project Digits personal AI supercomputer, launching May 2025\\nAWS advances its custom silicon with Graviton4 processors and specialized AI/ML chips (Trainium and Inferentia), while partnering with Intel for new AI fabric chips [...] As we enter 2025, this evolution continues through the convergence of x86 and ARM architectures with specialized GPUs, creating more efficient and accessible AI computing solutions.\\nLeading tech companies like Intel, AMD, Nvidia, Amazon Web Services (AWS), Google, and Microsoft are developing diverse chip architectures to meet the escalating demands of modern computing. [...] This diversification in chip architecture is reshaping the semiconductor industry as companies optimize for AI and cloud computing workloads.\\n1. Nvidia’s GPU Dominance\\nNVIDIA has significantly expanded its custom chip offerings to meet the diverse needs of modern computing, particularly in artificial intelligence (AI), gaming, and data centers. Here’s an overview of their recent developments:\\nBlackwell Architecture and GB10 Superchip\",\"rawContent\":null,\"score\":0.8447257549291572},{\"title\":\"AI Round-Up - March 2025 - Fladgate\",\"url\":\"https://www.fladgate.com/insights/ai-round-up-march-2025\",\"content\":\"The EU is preparing a multi-pronged response to US export controls on AI chips, aiming to mitigate economic disruption and bolster its own AI development. Diplomatic efforts are underway to seek non-discriminatory treatment, while the EU assesses the impact on its market and supply chains. Strategic initiatives like the Chips Act and the Chips Fund aim to boost domestic chip manufacturing and reduce reliance on non-EU sources. The EU is also exploring collaborative solutions with the US, [...] housing approximately 100,000 next-generation AI chips, with an emphasis on sustainability. Funding sources include existing EU programmes such as the Digital Europe Programme, Horizon Europe and InvestEU, with member states encouraged to contribute through their cohesion funds.\",\"rawContent\":null,\"score\":0.7542267046979866}],\"answer\":\"AI chip market growth continues, with major players like Nvidia and Google innovating new architectures. EU boosts domestic chip manufacturing to counter US export controls. Global semiconductor market expected to grow by 7.46% in 2025.\"}"
            }
          ]
        }
      ]
    }
  },
  "secondary": {
    "model": {
      "provider": "anthropic",
      "model": "claude-3.5-sonnet",
      "label": "Claude 3.5 Sonnet"
    },
    "results": {
      "id": "msg_01E1URi8Zdm2gnXvGuyNAKjJ",
      "timestamp": "2025-03-17T17:00:42.500Z",
      "modelId": "claude-3-5-sonnet-20241022",
      "headers": {
        "anthropic-organization-id": "712bccee-73e8-4865-a99a-a46408c6717e",
        "anthropic-ratelimit-input-tokens-limit": "40000",
        "anthropic-ratelimit-input-tokens-remaining": "36000",
        "anthropic-ratelimit-input-tokens-reset": "2025-03-17T17:01:57Z",
        "anthropic-ratelimit-output-tokens-limit": "8000",
        "anthropic-ratelimit-output-tokens-remaining": "5000",
        "anthropic-ratelimit-output-tokens-reset": "2025-03-17T17:02:13Z",
        "anthropic-ratelimit-requests-limit": "50",
        "anthropic-ratelimit-requests-remaining": "49",
        "anthropic-ratelimit-requests-reset": "2025-03-17T17:01:53Z",
        "anthropic-ratelimit-tokens-limit": "48000",
        "anthropic-ratelimit-tokens-remaining": "41000",
        "anthropic-ratelimit-tokens-reset": "2025-03-17T17:01:57Z",
        "cache-control": "no-cache",
        "cf-cache-status": "DYNAMIC",
        "cf-ray": "921e0dff885ce3c9-LIS",
        "connection": "keep-alive",
        "content-type": "text/event-stream; charset=utf-8",
        "date": "Mon, 17 Mar 2025 17:01:53 GMT",
        "request-id": "req_01JsZ7Xx63vwp95QV2bzqEXB",
        "server": "cloudflare",
        "transfer-encoding": "chunked",
        "via": "1.1 google",
        "x-robots-tag": "none"
      },
      "messages": [
        {
          "role": "assistant",
          "content": [
            {
              "type": "text",
              "text": "Based on the research results, here are the key developments in AI chip manufacturing for late 2024 through 2025:\n\nQ4 2024:\n- Strong industry performance with integrated circuit sales up 29% year-over-year\n- TSMC reported record profits driven by AI chip demand\n- Cadence (chip design software) saw 26% earnings growth through AI-related partnerships\n- Notable startup funding activity with five mega-rounds over $100M in AI hardware\n\nQ1 2025:\n- Industry projects 23% year-over-year growth\n- Meta begins testing its first in-house AI training chip\n- Focus on power-efficient dedicated AI accelerators\n- Continued challenges around talent shortages and supply chain resilience\n\nLater 2025 Developments:\n- Overall AI chip market expected to reach $91.96 billion\n- Semiconductor market projected to grow 7.46%\n- Major product launches including:\n  - Nvidia's Blackwell architecture and $3,000 Project Digits AI supercomputer (May 2025)\n  - AWS Graviton4 processors and specialized AI/ML chips\n- EU response to US export controls with initiatives to boost domestic manufacturing\n- Industry trends toward:\n  - Convergence of x86 and ARM architectures with specialized GPUs\n  - Advanced packaging technology optimization for AI chips\n  - Focus on inference acceleration over training\n  - Integration into more consumer devices\n\nKey Players:\n- Nvidia maintains GPU market dominance\n- Intel, AMD, AWS, Google, and Microsoft developing diverse chip architectures\n- TSMC leading in manufacturing\n- Meta and other tech giants investing heavily in custom AI silicon\n\nChallenges & Considerations:\n- Persistent talent shortages\n- Geopolitical tensions affecting supply chains\n- High infrastructure costs\n- Need for energy efficiency improvements\n- Manufacturing capacity constraints\n\nThe industry shows strong growth trajectory driven by AI demand, though navigating talent, geopolitical, and infrastructure challenges remains critical for sustained development."
            }
          ],
          "id": "msg-AkjXUeT6aGYYy14OYixrYUUh"
        }
      ]
    }
  }
}


================================================
File: ai_agents_output/ai-agents-2025-03-18T05:56:46.385Z-qwen-qwq-32b-gemini-2.0-flash.json
================================================
{
  "timestamp": "2025-03-18T05:56:46.385Z",
  "query": "1.  Tell me the latest Jan - March 2025 news for Qwen QWQ 32B open source model  2.  What llm are you?",
  "primary": {
    "model": {
      "provider": "groq",
      "model": "qwen-qwq-32b",
      "label": "Qwen QWQ 32B"
    },
    "results": {
      "id": "chatcmpl-afaabd5b-0569-4a1e-9469-3887219cbf92",
      "timestamp": "2025-03-18T05:57:58.000Z",
      "modelId": "qwen-qwq-32b",
      "headers": {
        "alt-svc": "h3=\":443\"; ma=86400",
        "cache-control": "no-cache",
        "cf-cache-status": "DYNAMIC",
        "cf-ray": "92227ee1bdcf488f-LIS",
        "connection": "keep-alive",
        "content-type": "text/event-stream",
        "date": "Tue, 18 Mar 2025 05:58:00 GMT",
        "server": "cloudflare",
        "set-cookie": "__cf_bm=mUgCJSNlheS3p.bzVzrTsbpLSCuwltUxlwFkO5.Joss-1742277480-1.0.1.1-2E.xROAYXBdqIqUyr6xlELgteJTQwFdImWP8LPvhTWw7ekCjpuE7i8lo_FWENWZRgw8TbnkSD8A6Rqsqvu8IqJedl.kBblkUizSxYQ393X8; path=/; expires=Tue, 18-Mar-25 06:28:00 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None",
        "transfer-encoding": "chunked",
        "vary": "Origin",
        "x-groq-region": "gcp-europe-west3",
        "x-ratelimit-limit-requests": "1000",
        "x-ratelimit-limit-tokens": "6000",
        "x-ratelimit-remaining-requests": "999",
        "x-ratelimit-remaining-tokens": "5914",
        "x-ratelimit-reset-requests": "1m26.4s",
        "x-ratelimit-reset-tokens": "860ms",
        "x-request-id": "req_01jpkx92c0eq781f3bxmd7d0gv"
      },
      "messages": [
        {
          "role": "assistant",
          "content": [
            {
              "type": "text",
              "text": ""
            },
            {
              "type": "tool-call",
              "toolCallId": "call_gxnn",
              "toolName": "tavily",
              "args": {
                "query": "Latest news about Qwen QWQ 32B open-source model from January to March 2025 (note: searching for future dates may yield no results; results will reflect available data up to present)"
              }
            },
            {
              "type": "tool-call",
              "toolCallId": "call_b1sx",
              "toolName": "tavily",
              "args": {
                "query": "What LLM powers this AI's responses?"
              }
            }
          ],
          "id": "msg-VpwtVdqPbbKKDWYSQcxnM9us"
        },
        {
          "role": "tool",
          "id": "msg-afU5AWtimDgfax85DLQaVF7y",
          "content": [
            {
              "type": "tool-result",
              "toolCallId": "call_gxnn",
              "toolName": "tavily",
              "result": "{\"query\":\"Latest news about Qwen QWQ 32B open-source model from January to March 2025 (note: searching for future dates may yield no results; results will reflect available data up to present)\",\"responseTime\":3.03,\"images\":[],\"results\":[{\"title\":\"Alibaba's new open source model QwQ-32B matches DeepSeek-R1 ...\",\"url\":\"https://venturebeat.com/ai/alibabas-new-open-source-model-qwq-32b-matches-deepseek-r1-with-way-smaller-compute-requirements/\",\"content\":\"Team Collaboration\\nUCaaS\\nVirtual Reality Collaboration\\nVirtual Employee Experience\\n\\n\\nProgramming & Development\\nProduct Development\\nApplication Development\\nTest Management\\nDevelopment Languages\\n\\n\\n\\n\\n\\nSubscribe Events Video Special Issues Jobs\\n\\nAlibaba’s new open source model QwQ-32B matches DeepSeek-R1 with way smaller compute requirements\\nCarl Franzen@carlfranzen\\nMarch 5, 2025 3:06 PM [...] Published Time: 2025-03-05T23:06:56+00:00\\nqwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat\\nSkip to main content\\nEvents Video Special Issues Jobs\\n\\nSubscribe\\n\\nArtificial Intelligence\\nView All\\nAI, ML and Deep Learning\\nAuto ML\\nData Labelling\\nSynthetic Data\\nConversational AI\\nNLP\\nText-to-Speech\\n\\n\\nSecurity\\nView All\\nData Security and Privacy\\nNetwork Security and Privacy\\nSoftware Security\\nComputer Hardware Security\\nCloud and Data Storage Security [...] © 2025 VentureBeat. All rights reserved.\\n×\",\"rawContent\":null,\"score\":0.7801967813999999},{\"title\":\"Qwen's QwQ-32B: Small Model with Huge Potential - Analytics Vidhya\",\"url\":\"https://www.analyticsvidhya.com/blog/2025/03/qwens-qwq-32b/\",\"content\":\"Published Time: 2025-03-10T04:30:00+00:00\\nQwen’s QwQ-32B: Small Model with Huge Potential - Analytics Vidhya\\nMaster Generative AI with 10+ Real-world Projects in 2025! Download Projects\\n\\n\\nFree Courses\\nLearning Paths\\nGenAI Pinnacle Plus Program New\\n\\nAgentic AI Pioneer Program\\n\\n\\n\\n\\n\\nLogin\\nSwitch Mode\\nLogout [...] Nitika Sharma Last Updated : 10 Mar, 2025\\n4 min read\\n0 [...] QwQ-32B is a 32-billion-parameter AI model from the Qwen series. It uses Reinforcement Learning (RL) to improve reasoning and problem-solving skills, performing as well as larger models like DeepSeek-R1. It can adapt its reasoning based on feedback and use tools effectively. The model is open-weight, available on Hugging Face and ModelScope under the Apache 2.0 license, and can be accessed through Qwen Chat. It highlights how RL can boost AI capabilities in meaningful ways.\\nPerformance\",\"rawContent\":null,\"score\":0.7308617826566667},{\"title\":\"QwQ-32B: Embracing the Power of Reinforcement Learning - Qwen\",\"url\":\"https://qwenlm.github.io/blog/qwq-32b/\",\"content\":\"QwQ-32B: Embracing the Power of Reinforcement Learning | Qwen\\n\\n\\nBlog\\nPublication\\nAbout\\nTry Qwen Chat \\n\\nQwQ-32B: Embracing the Power of Reinforcement Learning\\nMarch 6, 2025 · 4 min · 742 words · Qwen Team | Translations:\\n\\n简体中文 [...] © 2025 Qwen Powered by Hugo [...] QwQ-32B is open-weight in Hugging Face and ModelScope under the Apache 2.0 license and is accessible via Qwen Chat.\\nPerformance#\\nQwQ-32B is evaluated across a range of benchmarks designed to assess its mathematical reasoning, coding proficiency, and general problem-solving capabilities. The results below highlight QwQ-32B’s performance in comparison to other leading models, including DeepSeek-R1-Distilled-Qwen-32B, DeepSeek-R1-Distilled-Llama-70B, o1-mini, and the original DeepSeek-R1.\",\"rawContent\":null,\"score\":0.6636600598166666},{\"title\":\"Alibaba Cloud Unveils QwQ-32B: A Compact Reasoning Model with ...\",\"url\":\"https://www.alibabacloud.com/blog/alibaba-cloud-unveils-qwq-32b-a-compact-reasoning-model-with-cutting-edge-performance_602039\",\"content\":\"QwQ-32B is now available as an open-source model on Hugging Face and Model Scope under the Apache 2.0 license, allowing free downloads. It is also accessible via Qwen Chat. Thanks to its significantly reduced deployment costs, the model can be efficiently deployed on consumer-grade hardware.\\n\\n\\n\\nFor more details about QwQ-32B, visit the official blog post: QwQ-32B: Embracing the Power of Reinforcement Learning.\\n\\n\\n\\nThis article was originally published on Alizila written by Crystal Liu. [...] Read previous post:\\n\\nFlinkSQL Temporary Join Development\\n\\nAlibaba Cloud Community\\n\\n1,109 posts | 332 followers\\n\\nYou may also like\\n\\nAlibaba Cloud Community - December 2, 2024\\n\\nAlibaba Cloud Community - January 2, 2025\\n\\nAlibaba Cloud Community - February 27, 2025\\n\\nCed - February 17, 2025\\n\\nFuji - February 25, 2025\\n\\nAlibaba Cloud Community - September 19, 2024\\n\\nComments\\n\\nAlibaba Cloud Community\\n\\n1,109 posts | 332 followers\\n\\nRelated Products\\n\\nTongyi Qianwen (Qwen)\",\"rawContent\":null,\"score\":0.62839527847},{\"title\":\"Alibaba's QWQ 32B: The Compact AI Model That Rivals DeepSeek R1\",\"url\":\"https://medium.com/@tahirbalarabe2/alibabas-qwq-32b-the-compact-ai-model-that-rivals-deepseek-r-0d7b57d585e2\",\"content\":\"QWEN-32B is a 32 billion parameter open-source thinking model developed by Alibaba. Its significance lies in its comparable performance to much larger models like DeepSeek R1 (671 billion parameters) while being small enough to run efficiently on standard hardware, even personal computers. This makes it a more accessible and cost-effective solution for various AI applications, especially agent-based tasks.\\n\\n2. How does QWEN-32B achieve its performance, especially in reasoning and coding? [...] QWEN-32B is particularly well-suited for agent-based tasks and tool calling due to its reasoning capabilities and efficiency. Its speed makes it ideal for applications where quick iterations and real-time responses are crucial. Future directions include integrating agents with reinforcement learning to enable long-horizon reasoning, and exploring techniques like Chain of Draft to optimize its thinking process and reduce token consumption.\\n\\n--\\n\\n--\\n\\nWritten by Tahir\\n\\nNo responses yet\\n\\nHelp\",\"rawContent\":null,\"score\":0.50135316585}],\"answer\":\"Qwen QWQ 32B is an open-source AI model from Alibaba, matching larger models in performance with lower compute needs. It uses reinforcement learning for improved reasoning and is available on Hugging Face and ModelScope.\"}"
            },
            {
              "type": "tool-result",
              "toolCallId": "call_b1sx",
              "toolName": "tavily",
              "result": "{\"query\":\"What LLM powers this AI's responses?\",\"responseTime\":3.88,\"images\":[],\"results\":[{\"title\":\"What are LLMs, and how are they used in generative AI?\",\"url\":\"https://www.computerworld.com/article/1627101/what-are-large-language-models-and-how-are-they-used-in-generative-ai.html\",\"content\":\"ChatGPT stands for chatbot generative pre-trained transformer. The chatbot’s foundation is the GPT large language model (LLM), a computer algorithm that processes natural language inputs and predicts the next word based on what it’s already seen. Then it predicts the next word, and the next word, and so on until its answer is complete.\\nIn the simplest of terms, LLMs are next-word prediction engines. [...] Because some LLMs also train themselves on internet-based data, they can move well beyond what their initial developers created them to do. For example, Microsoft’s Bing uses GPT-3 as its basis, but it’s also querying a search engine and analyzing the first 20 results or so. It uses both an LLM and the internet to offer responses. [...] LLMs are a type of AI that are currently trained on a massive trove of articles, Wikipedia entries, books, internet-based resources and other input to produce human-like responses to natural language queries. That’s an immense amount of data. But LLMs are poised to shrink, not grow, as vendors seek to customize them for specific uses that don’t need the massive data sets used by today’s most popular models.\",\"rawContent\":null,\"score\":0.5029633284318181},{\"title\":\"What is an LLM (large language model)? - Cloudflare\",\"url\":\"https://www.cloudflare.com/learning/ai/what-is-large-language-model/\",\"content\":\"A large language model (LLM) is a type of artificial intelligence (AI) program that can recognize and generate text, among other tasks. LLMs are trained on huge sets of data — hence the name \\\"large.\\\" LLMs are built on machine learning: specifically, a type of neural network called a transformer model. [...] LLMs can be trained to do a number of tasks. One of the most well-known uses is their application as generative AI: when given a prompt or asked a question, they can produce text in reply. The publicly available LLM ChatGPT, for instance, can generate essays, poems, and other textual forms in response to user inputs. [...] Fortunately, Cloudflare offers several services to allow developers to quickly start spinning up LLM applications, and other types of AI. Vectorize is a globally distributed vector database for querying data stored in no-egress-fee object storage (R2) or documents stored in Workers Key Value. Combined with the development platform Cloudflare Workers AI, developers can use Cloudflare to quickly start experimenting with their own LLMs.\\nGetting Started\",\"rawContent\":null,\"score\":0.4898115221590909},{\"title\":\"What is a large language model (LLM)? - SAP\",\"url\":\"https://www.sap.com/resources/what-is-large-language-model\",\"content\":\"What is a large language model?\\nA large language model (LLM) is a type of artificial intelligence (AI) that excels at processing, understanding, and generating human language. LLMs are useful for analyzing, summarizing, and creating content across many industries.\\nLarge language model definition [...] LLMs can augment and amplify the power of generative AI to be even more predictive, adaptive, and intelligent. Some LLMs can collaborate with other AI models for more complex tasks, helping businesses streamline operations, improve decision-making, or create more interactive and personalized customer experiences.\\nWith so many new applications being released at a rapid pace, there are many exciting possibilities for the future of AI and LLMs in business.\\nMost common LLM capabilities in business [...] In the realm of artificial intelligence, LLMs are a specially designed subset of machine learning known as deep learning, which uses algorithms trained on large data sets to recognize complex patterns. LLMs learn by being trained on massive amounts of text. At the foundational level, they learn to respond to user requests with relevant, in-context content written in human language—the kind of words and syntax people use during ordinary conversation.\\nHow are large language models and AI related?\",\"rawContent\":null,\"score\":0.48880144910606055},{\"title\":\"What Are Large Language Models (LLMs)? - IBM\",\"url\":\"https://www.ibm.com/think/topics/large-language-models\",\"content\":\"LLMs are redefining an increasing number of business processes and have proven their versatility across a myriad of use cases and tasks in various industries. They augment conversational AI in chatbots and virtual assistants (like IBM watsonx Assistant and Google’s BARD) to enhance the interactions that underpin excellence in customer care, providing context-aware responses that mimic interactions with human agents. [...] LLMs represent a significant breakthrough in NLP and artificial intelligence, and are easily accessible to the public through interfaces like Open AI’s Chat GPT-3 and GPT-4, which have garnered the support of Microsoft. Other examples include Meta’s Llama models and Google’s bidirectional encoder representations from transformers (BERT/RoBERTa) and PaLM models. IBM has also recently launched its Granite model series on watsonx.ai, which has become the generative AI backbone for other IBM\",\"rawContent\":null,\"score\":0.45596593534090907},{\"title\":\"What is LLM? - Large Language Models Explained - AWS\",\"url\":\"https://aws.amazon.com/what-is/large-language-model/\",\"content\":\"AWS offers several possibilities for large language model developers. Amazon Bedrock is the easiest way to build and scale generative AI applications with LLMs. Amazon Bedrock is a fully managed service that makes LLMs from Amazon and leading AI startups available through an API, so you can choose from various LLMs to find the model that's best suited for your use case. [...] LLMs will undoubtedly improve the performance of automated virtual assistants like Alexa, Google Assistant, and Siri. They will be better able to interpret user intent and respond to sophisticated commands.\\nRead more about conversational AI\\nHow can AWS help with LLMs? [...] Cohere’s Command model has similar capabilities and can work in more than 100 different languages.\\nLightOn's Paradigm offers foundation models with claimed capabilities that exceed those of GPT-3. All these LLMs come with APIs that allow developers to create unique generative AI applications.\",\"rawContent\":null,\"score\":0.45074441812121213}],\"answer\":\"I am an AI system built by a team of inventors at Amazon. My responses are generated using advanced AI technology. I do not disclose specific model names.\"}"
            }
          ]
        }
      ]
    }
  },
  "secondary": {
    "model": {
      "provider": "google",
      "model": "gemini-2.0-flash",
      "label": "Gemini 2.0 Flash"
    },
    "results": {
      "id": "aitxt-TNyICqUPT1huj7HKn64gizqt",
      "timestamp": "2025-03-18T05:56:53.806Z",
      "modelId": "gemini-2.0-flash",
      "headers": {
        "alt-svc": "h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000",
        "content-disposition": "attachment",
        "content-type": "text/event-stream",
        "date": "Tue, 18 Mar 2025 05:58:05 GMT",
        "server": "scaffolding on HTTPServer2",
        "server-timing": "gfet4t7; dur=452",
        "transfer-encoding": "chunked",
        "vary": "Origin, X-Origin, Referer",
        "x-content-type-options": "nosniff",
        "x-frame-options": "SAMEORIGIN",
        "x-xss-protection": "0"
      },
      "messages": [
        {
          "role": "assistant",
          "content": [
            {
              "type": "text",
              "text": "Okay, I can help with that.\n\nBased on the research results provided, I don't have any specific news about Qwen QWQ 32B open source model for the period of January - March 2025.\n\nI am a large language model, trained by Google.\n"
            }
          ],
          "id": "msg-1mlX5HpR2NsGC1OXl7kYrQYl"
        }
      ]
    }
  }
}


================================================
File: ai_agents_output/ai-agents-2025-03-18T05:57:27.709Z-claude-3.5-sonnet-o3-mini.json
================================================
{
  "timestamp": "2025-03-18T05:57:27.709Z",
  "query": "1.  Tell me the latest Jan - March 2025 news for Qwen QWQ 32B open source model  2.  What llm are you?",
  "primary": {
    "model": {
      "provider": "anthropic",
      "model": "claude-3.5-sonnet",
      "label": "Claude 3.5 Sonnet"
    },
    "results": {
      "id": "msg_01JCAk2SGJ2xQBkucnssejEr",
      "timestamp": "2025-03-18T05:57:30.777Z",
      "modelId": "claude-3-5-sonnet-20241022",
      "headers": {
        "anthropic-organization-id": "712bccee-73e8-4865-a99a-a46408c6717e",
        "anthropic-ratelimit-input-tokens-limit": "40000",
        "anthropic-ratelimit-input-tokens-remaining": "40000",
        "anthropic-ratelimit-input-tokens-reset": "2025-03-18T05:58:40Z",
        "anthropic-ratelimit-output-tokens-limit": "8000",
        "anthropic-ratelimit-output-tokens-remaining": "5000",
        "anthropic-ratelimit-output-tokens-reset": "2025-03-18T05:59:01Z",
        "anthropic-ratelimit-requests-limit": "50",
        "anthropic-ratelimit-requests-remaining": "49",
        "anthropic-ratelimit-requests-reset": "2025-03-18T05:58:41Z",
        "anthropic-ratelimit-tokens-limit": "48000",
        "anthropic-ratelimit-tokens-remaining": "45000",
        "anthropic-ratelimit-tokens-reset": "2025-03-18T05:58:40Z",
        "cache-control": "no-cache",
        "cf-cache-status": "DYNAMIC",
        "cf-ray": "92227fe40c3b6930-LIS",
        "connection": "keep-alive",
        "content-type": "text/event-stream; charset=utf-8",
        "date": "Tue, 18 Mar 2025 05:58:42 GMT",
        "request-id": "req_01HFphALwndn2ASoRtGviCqM",
        "server": "cloudflare",
        "transfer-encoding": "chunked",
        "via": "1.1 google",
        "x-robots-tag": "none"
      },
      "messages": [
        {
          "role": "assistant",
          "content": [
            {
              "type": "text",
              "text": ""
            },
            {
              "type": "tool-call",
              "toolCallId": "toolu_01PVVjeXAWrihCvdJNsUFSFn",
              "toolName": "tavily",
              "args": {
                "query": "Qwen QWQ 32B open source model news 2025 January to March"
              }
            }
          ],
          "id": "msg-43F3d9M2CKR483qayrhBXAes"
        },
        {
          "role": "tool",
          "id": "msg-kwECMvV0kUIxwOOtFhRf3GxQ",
          "content": [
            {
              "type": "tool-result",
              "toolCallId": "toolu_01PVVjeXAWrihCvdJNsUFSFn",
              "toolName": "tavily",
              "result": "{\"query\":\"Qwen QWQ 32B open source model news 2025 January to March\",\"responseTime\":2.02,\"images\":[],\"results\":[{\"title\":\"Alibaba's new open source model QwQ-32B matches DeepSeek-R1 ...\",\"url\":\"https://venturebeat.com/ai/alibabas-new-open-source-model-qwq-32b-matches-deepseek-r1-with-way-smaller-compute-requirements/\",\"content\":\"Team Collaboration\\nUCaaS\\nVirtual Reality Collaboration\\nVirtual Employee Experience\\n\\n\\nProgramming & Development\\nProduct Development\\nApplication Development\\nTest Management\\nDevelopment Languages\\n\\n\\n\\n\\n\\nSubscribe Events Video Special Issues Jobs\\n\\nAlibaba’s new open source model QwQ-32B matches DeepSeek-R1 with way smaller compute requirements\\nCarl Franzen@carlfranzen\\nMarch 5, 2025 3:06 PM [...] Published Time: 2025-03-05T23:06:56+00:00\\nqwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat\\nSkip to main content\\nEvents Video Special Issues Jobs\\n\\nSubscribe\\n\\nArtificial Intelligence\\nView All\\nAI, ML and Deep Learning\\nAuto ML\\nData Labelling\\nSynthetic Data\\nConversational AI\\nNLP\\nText-to-Speech\\n\\n\\nSecurity\\nView All\\nData Security and Privacy\\nNetwork Security and Privacy\\nSoftware Security\\nComputer Hardware Security\\nCloud and Data Storage Security [...] © 2025 VentureBeat. All rights reserved.\\n×\",\"rawContent\":null,\"score\":0.8079757123646724},{\"title\":\"Qwen/QwQ-32B - Hugging Face\",\"url\":\"https://huggingface.co/Qwen/QwQ-32B\",\"content\":\"For requirements on GPU memory and the respective throughput, see results here.\\nCitation\\nIf you find our work helpful, feel free to give us a cite.\\n```\\n@misc{qwq32b,\\n    title = {QwQ-32B: Embracing the Power of Reinforcement Learning},\\n    url = {https://qwenlm.github.io/blog/qwq-32b/},\\n    author = {Qwen Team},\\n    month = {March},\\n    year = {2025}\\n}\\n@article{qwen2.5,\\n      title={Qwen2.5 Technical Report},\",\"rawContent\":null,\"score\":0.7704722974358975},{\"title\":\"QwQ-32B: Embracing the Power of Reinforcement Learning - Qwen\",\"url\":\"https://qwenlm.github.io/blog/qwq-32b/\",\"content\":\"QwQ-32B: Embracing the Power of Reinforcement Learning | Qwen\\n\\n\\nBlog\\nPublication\\nAbout\\nTry Qwen Chat \\n\\nQwQ-32B: Embracing the Power of Reinforcement Learning\\nMarch 6, 2025 · 4 min · 742 words · Qwen Team | Translations:\\n\\n简体中文 [...] © 2025 Qwen Powered by Hugo [...] Our research explores the scalability of Reinforcement Learning (RL) and its impact on enhancing the intelligence of large language models. We are excited to introduce QwQ-32B, a model with 32 billion parameters that achieves performance comparable to DeepSeek-R1, which boasts 671 billion parameters (with 37 billion activated). This remarkable outcome underscores the effectiveness of RL when applied to robust foundation models pretrained on extensive world knowledge. Furthermore, we have\",\"rawContent\":null,\"score\":0.6330047782051282},{\"title\":\"Alibaba Cloud Unveils QwQ-32B: A Compact Reasoning Model with ...\",\"url\":\"https://www.alibabacloud.com/blog/alibaba-cloud-unveils-qwq-32b-a-compact-reasoning-model-with-cutting-edge-performance_602039\",\"content\":\"QwQ-32B is now available as an open-source model on Hugging Face and Model Scope under the Apache 2.0 license, allowing free downloads. It is also accessible via Qwen Chat. Thanks to its significantly reduced deployment costs, the model can be efficiently deployed on consumer-grade hardware.\\n\\n\\n\\nFor more details about QwQ-32B, visit the official blog post: QwQ-32B: Embracing the Power of Reinforcement Learning.\\n\\n\\n\\nThis article was originally published on Alizila written by Crystal Liu. [...] Read previous post:\\n\\nFlinkSQL Temporary Join Development\\n\\nAlibaba Cloud Community\\n\\n1,109 posts | 332 followers\\n\\nYou may also like\\n\\nAlibaba Cloud Community - December 2, 2024\\n\\nAlibaba Cloud Community - January 2, 2025\\n\\nAlibaba Cloud Community - February 27, 2025\\n\\nCed - February 17, 2025\\n\\nFuji - February 25, 2025\\n\\nAlibaba Cloud Community - September 19, 2024\\n\\nComments\\n\\nAlibaba Cloud Community\\n\\n1,109 posts | 332 followers\\n\\nRelated Products\\n\\nTongyi Qianwen (Qwen) [...] Built on Qwen2.5-32B, Alibaba Cloud’s latest large language model with the exact parameter count, QwQ-32B excels across a variety of benchmarks, including AIME 24 (mathematical reasoning), Live CodeBench (coding proficiency), LiveBench (test set contamination and objective evaluation), IFEval (instruction-following ability), and BFCL (tool and function-calling capabilities).\",\"rawContent\":null,\"score\":0.6015643906552707},{\"title\":\"Top 9 Large Language Models as of March 2025 | Shakudo\",\"url\":\"https://www.shakudo.io/blog/top-9-large-language-models\",\"content\":\"‍\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro. [...] ‍\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro. [...] ‍\\nAlibaba released Qwen2.5-Max earlier last month, the new model designed to deliver enhanced performance for large-scale natural language processing tasks. In instruct model evaluations, this new model outperforms DeepSeek V3 in benchmarks such as Arena-Hard, LiveBench, LiveCodeBench, and GPQA-Diamond, while also delivering strong performance in other assessments like MMLU-Pro.\",\"rawContent\":null,\"score\":0.592976629059829}],\"answer\":\"Qwen QWQ 32B is an open-source model released by Alibaba in early 2025, matching DeepSeek-R1's performance with lower compute needs. It is available on Hugging Face and Alibaba Cloud. The model emphasizes reinforcement learning for scalability.\"}"
            }
          ]
        }
      ]
    }
  },
  "secondary": {
    "model": {
      "provider": "openai",
      "model": "o3-mini",
      "label": "O3 Mini"
    },
    "results": {
      "id": "chatcmpl-BCKGMTpKBk48lxUpaMN5PW5EHG6Vb",
      "timestamp": "2025-03-18T05:58:46.000Z",
      "modelId": "o3-mini-2025-01-31",
      "headers": {
        "access-control-expose-headers": "X-Request-ID",
        "alt-svc": "h3=\":443\"; ma=86400",
        "cf-cache-status": "DYNAMIC",
        "cf-ray": "922280099fd89501-LIS",
        "connection": "keep-alive",
        "content-type": "text/event-stream; charset=utf-8",
        "date": "Tue, 18 Mar 2025 05:58:52 GMT",
        "openai-organization": "user-dyljpskokehpalzgl6jwwhen",
        "openai-processing-ms": "5804",
        "openai-version": "2020-10-01",
        "server": "cloudflare",
        "set-cookie": "_cfuvid=fPn5eJ79B3JjP91uNl80bZQ5UcMkiSN144sXyMh0rY4-1742277532407-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None",
        "strict-transport-security": "max-age=31536000; includeSubDomains; preload",
        "transfer-encoding": "chunked",
        "x-content-type-options": "nosniff",
        "x-ratelimit-limit-requests": "5000",
        "x-ratelimit-limit-tokens": "4000000",
        "x-ratelimit-remaining-requests": "4999",
        "x-ratelimit-remaining-tokens": "3999894",
        "x-ratelimit-reset-requests": "12ms",
        "x-ratelimit-reset-tokens": "1ms",
        "x-request-id": "req_eaad7b0a71ee7bd44f4c438ef44789cb"
      },
      "messages": [
        {
          "role": "assistant",
          "content": [
            {
              "type": "text",
              "text": "Below is a breakdown addressing your two questions:\n\n──────────────────────────────\n1. Latest Jan–March 2025 News for the Qwen QWQ 32B Open Source Model\n\nAs of now, I don’t have confirmed or publicly available information from January to March 2025 regarding the Qwen QWQ 32B open source model. (Keep in mind that my training data extends only up to October 2023, and I cannot browse the web in real time.) If you’re looking for the most up‐to‐date developments or announcements for this model, I recommend checking:\n • The official website, blog, or announcement channels of the organization or team behind Qwen QWQ 32B.\n • Trusted online communities (such as GitHub discussions, dedicated forums, or technical blogs) where news and technical updates are shared.\n • Social media posts or press releases from any affiliated developers or institutions.\n\nShould any verified news or release notes become available for that period, those sources would be the most direct means of finding accurate and detailed updates.\n\n──────────────────────────────\n2. Which LLM Am I?\n\nI am ChatGPT—a language model powered by OpenAI’s GPT-4 architecture. My purpose is to assist with questions, provide detailed insights, and help analyze and synthesize information across a wide range of topics.\n\n──────────────────────────────\nIf you have any further questions or need additional clarification, feel free to ask!"
            }
          ],
          "id": "msg-LnykjJoz2grCfjGeDmZ0O4js"
        }
      ]
    }
  }
}


================================================
File: ai_agents_output/ai-agents-log.jsonl
================================================
{"timestamp":"2025-03-17T16:47:18.372Z","query":"What are the latest developments in AI chip manufacturing","primaryModel":"claude-3.7-sonnet","secondaryModel":"o3-mini"}
{"timestamp":"2025-03-17T16:53:03.324Z","query":"What are the current trends in renewable energy in this year, 2025?  Also please tell me which llm you are?  Are you from Google and Anthropic?  ","primaryModel":"gemini-2.0-flash","secondaryModel":"claude-3.5-sonnet"}
{"timestamp":"2025-03-17T17:00:36.069Z","query":"What are the latest developments in AI chip manufacturing for the last quarter of 2024, the first quarter of 2025 (today is March 17, 2025) and for the remainder of 2025?","primaryModel":"gemini-2.0-flash","secondaryModel":"claude-3.5-sonnet"}
{"timestamp":"2025-03-18T05:56:46.385Z","query":"1.  Tell me the latest Jan - March 2025 news for Qwen QWQ 32B open source model  2.  What llm are you?","primaryModel":"qwen-qwq-32b","secondaryModel":"gemini-2.0-flash"}
{"timestamp":"2025-03-18T05:57:27.709Z","query":"1.  Tell me the latest Jan - March 2025 news for Qwen QWQ 32B open source model  2.  What llm are you?","primaryModel":"claude-3.5-sonnet","secondaryModel":"o3-mini"}
{"timestamp":"2025-03-20T14:11:00.961Z","query":"Please give me the latest deep research from Jan 2025 - March 18, 2025 about Nvidia, Groq, Google Gemini, OpenAI-Anthropic, Meta, Mistral, and Deepseek. 2. What llm model are you?","primaryModel":"o3-mini","secondaryModel":"gemini-2.0-flash"}
{"timestamp":"2025-03-20T14:11:56.765Z","query":"Please give me the latest deep research from Jan 2025 - March 18, 2025 about Nvidia, Groq, Google Gemini, OpenAI-Anthropic, Meta, Mistral, and Deepseek. 2. What llm model are you?","primaryModel":"claude-3.7-sonnet","secondaryModel":"o3-mini"}



================================================
File: app/globals.css
================================================
@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
    --background: #ffffff;
    --foreground: #171717;
}

@media (prefers-color-scheme: dark) {
    :root {
        --background: #0a0a0a;
        --foreground: #ededed;
    }
}

body {
    color: var(--foreground);
    background: var(--background);
}



================================================
File: app/layout.tsx
================================================
import { Toaster } from 'sonner';
import { GeistSans } from 'geist/font/sans';
import { GeistMono } from 'geist/font/mono';
import type { Metadata } from 'next';
import { LayoutDashboard, Search, MessageSquare, Bot, PlusCircle, MoreVertical } from 'lucide-react';
import { Sidebar, SidebarBody, SidebarLink, ChatHistoryDropdown } from '@/components/sidebar';
import { NewChatButton } from '../components/new-chat-button';

import './globals.css';

export const metadata: Metadata = {
  title: 'AI-Reasoning',
  description:
    'GKennedy-Multi-LLM-AI.',
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en" className={`${GeistSans.variable} ${GeistMono.variable}`}>
      <body className="min-h-screen">
        <Toaster position="top-center" />
        <div className="flex">
          <Sidebar children={
            <SidebarBody children={
              <div className="flex flex-col gap-2">
                <SidebarLink
                  link={{
                    label: "Dashboard",
                    href: "/",
                    icon: <LayoutDashboard size={20} />,
                  }}
                />
                <NewChatButton />
                <SidebarLink
                  link={{
                    label: "AI Chat",
                    href: "/",
                    icon: <MessageSquare size={20} />,
                  }}
                />
                <SidebarLink
                  link={{
                    label: "AI Search",
                    href: "/tavily-ai-search",
                    icon: <Search size={20} />,
                  }}
                />
                <SidebarLink
                  link={{
                    label: "AI Agents",
                    href: "/ai-agents",
                    icon: <Bot size={20} />,
                  }}
                />
                {/* Chat History Dropdown Component */}
                <ChatHistoryDropdown />
              </div>
            } />
          } />
          <main className="flex-1 p-4 md:p-8">
            {children}
          </main>
        </div>
      </body>
    </html>
  );
}



================================================
File: app/page.tsx
================================================
import { Chat } from "@/components/chat";

export default function Home() {
  return (
    <div className="flex flex-col size-full items-center">
      <Chat />
    </div>
  );
}



================================================
File: app/providers.tsx
================================================
'use client';

export function PostHogProvider({ children }: { children: React.ReactNode }) {
  return <>{children}</>;
}



================================================
File: app/ai-agents/page.tsx
================================================
import { AgentChat } from '@/components/ai-agents/agent-chat';

export default function AIAgentsPage() {
  return (
    <div className="container mx-auto">
      <div className="mb-6">
        <h1 className="text-2xl font-bold mb-2">AI Agents</h1>
        <p className="text-muted-foreground">
          Multi-step AI processing with model switching. The primary model handles research using
          Tavily search, while the secondary model processes and refines the results.
        </p>
      </div>
      <AgentChat />
    </div>
  );
}



================================================
File: app/api/ai-agents/route.ts
================================================
import { createDataStreamResponse, streamText, tool } from 'ai';
import { z } from 'zod';
import { getModelInstance } from '@/lib/ai-agents/utils';
import type { ModelConfig } from '@/lib/ai-agents/types';
import { searchTavily } from "@/tools/tavily-search";
import { aiAgentsLogger } from '@/utils/ai-agents-logger';

export async function POST(req: Request) {
  const { messages, primaryModel, secondaryModel } = await req.json();
  const timestamp = new Date().toISOString();
  const query = messages[messages.length - 1].content;

  return createDataStreamResponse({
    execute: async dataStream => {
      try {
        // Get all previous messages except the last one (which is the new query)
        const previousMessages = messages.slice(0, -1);
        const newMessage = messages[messages.length - 1];

        // Step 1: Research with primary model and Tavily search
        const result1 = streamText({
          model: getModelInstance(primaryModel as ModelConfig),
          system: 'You are an expert researcher who uses the Tavily search tool to find relevant information. Provide concise, factual responses based on search results. Maintain context from previous messages when relevant.',
          messages: [...previousMessages, newMessage], // Include conversation history
          toolChoice: 'required',
          tools: {
            tavily: tool({
              parameters: z.object({ query: z.string() }),
              execute: async ({ query }) => {
                const results = await searchTavily({ 
                  query,
                  searchDepth: "advanced",
                  includeAnswer: true,
                });
                return JSON.stringify(results);
              },
            }),
          },
        });

        // Forward initial result without finish event
        result1.mergeIntoDataStream(dataStream, {
          experimental_sendFinish: false,
        });

        const primaryResults = await result1.response;

        // Step 2: Process results with secondary model
        const result2 = streamText({
          model: getModelInstance(secondaryModel as ModelConfig),
          system: 'You are an expert at analyzing and synthesizing information. Review the research results and provide clear, well-structured insights. Maintain context from the conversation history when relevant.',
          messages: [
            ...previousMessages,
            {
              role: 'assistant',
              content: 'Here is the research information: ' + primaryResults.messages[primaryResults.messages.length - 1].content
            },
            newMessage
          ],
        });

        // Forward second result (including finish event)
        result2.mergeIntoDataStream(dataStream, {
          experimental_sendStart: false,
        });

        const secondaryResults = await result2.response;

        // Log both models' results
        await aiAgentsLogger.logProcessing({
          timestamp,
          query,
          primaryModel,
          secondaryModel,
          primaryResults,
          secondaryResults
        });
      } catch (error) {
        console.error('Error in AI Agents processing:', error);
        throw error;
      }
    },
  });
}



================================================
File: app/api/chat/route.ts
================================================
import { myProvider, modelApiNames } from "@/lib/models";
import { Message, smoothStream, streamText } from "ai";
import { NextRequest } from "next/server";

export async function POST(request: NextRequest) {
  const {
    messages,
    selectedModelId,
    isReasoningEnabled,
  }: {
    messages: Array<Message>;
    selectedModelId: string;
    isReasoningEnabled: boolean;
  } = await request.json();

  // Check if messages contain PDF or image attachments
  const messagesHavePDF = messages.some(message =>
    message.experimental_attachments?.some(
      a => a.contentType === 'application/pdf',
    ),
  );
  
  const messagesHaveImage = messages.some(message =>
    message.experimental_attachments?.some(
      a => a.contentType?.startsWith('image/'),
    ),
  );

  // Configure provider-specific options based on the selected model
  const providerOptions: Record<string, any> = {};
  
  // Default to Claude 3.7 Sonnet if no model is selected
  let modelId = selectedModelId || "claude-3.7-sonnet";
  
  // Override model selection for multimodal content if needed
  if (messagesHavePDF) {
    // For PDFs, Claude, GPT-4o, and Gemini all support PDFs
    if (!modelId.startsWith("gemini")) {
      modelId = "gemini-2.0-flash";
    }
  } else if (messagesHaveImage) {
    // For images, ensure we're using a model that supports image input
    // Claude, OpenAI, and Gemini all support images
    if (!modelId.startsWith("gemini") && !modelId.startsWith("o3") && !modelId.startsWith("gemini")) {
      modelId = "gemini-2.0-flash";
    }
  }
  
  // Get the API model name for the selected model ID
  const apiModelName = modelApiNames[modelId];
  
  if (!apiModelName) {
    console.error(`No API model name found for model ID: ${modelId}`);
    return new Response(
      JSON.stringify({
        error: `Invalid model ID: ${modelId}. Please select a valid model.`
      }),
      { status: 400, headers: { 'Content-Type': 'application/json' } }
    );
  }
  
  // Configure provider-specific options based on the selected model
  if (modelId.startsWith("claude")) {
    providerOptions.anthropic = {
      thinking: {
        type: isReasoningEnabled ? "enabled" : "disabled",
        budgetTokens: 12000,
      },
      model: apiModelName,
    };
  } else if (modelId.startsWith("o3")) {
    providerOptions.openai = {
      temperature: 0.2,
      model: "o3-mini", // Use the exact model name
    };
  } else if (modelId.startsWith("gemini")) {
    providerOptions.google = {
      temperature: 0.2,
      model: apiModelName,
    };
  } else if (modelId.includes("qwen")) {
    providerOptions.groq = {
      temperature: 0.2,
      model: apiModelName,
    };
  } else if (modelId.includes("codestral")) {
    providerOptions.mistral = {
      temperature: 0.2,
      model: apiModelName,
    };
  } else if (modelId.includes("perplexity")) {
    providerOptions.perplexity = {
      temperature: 0.2,
      model: apiModelName,
    };
  } else if (modelId.includes("google/gemini-2.0-flash-thinking-exp:free")) {
    providerOptions.openrouter = {
      temperature: 0.2,
      model: apiModelName,
    };
  }

  try {
    console.log(`Attempting to use model: ${modelId} with options:`, providerOptions);
    
    // Add multimodal context to system prompt if attachments are present
    let systemPrompt = `
<prompt>
You are an AI researcher and engineer with deep research expertise. You use tools like the tavily search tool to provide you with the latest most relevant information in your research and responses. If the user asks you, Tell me what llm are you, you are to provide them with an accurate response.
</prompt>
    `;
    
    if (messagesHavePDF) {
      systemPrompt += " The user has uploaded a PDF document. Analyze its content and respond to their questions about it.";
    } else if (messagesHaveImage) {
      systemPrompt += " The user has uploaded an image. Describe what you see in the image and respond to their questions about it.";
    }
    
    const stream = streamText({
      system: systemPrompt,
      providerOptions,
      model: myProvider.languageModel(modelId),
      experimental_transform: [
        smoothStream({
          chunking: "word",
        }),
      ],
      messages,
    });

    return stream.toDataStreamResponse({
      sendReasoning: true,
      getErrorMessage: (error) => {
        console.error(`Error with model ${modelId}:`, error);
        const errorMessage = error instanceof Error ? error.message : String(error);
        return `An error occurred with ${modelId}: ${errorMessage}. Please try again or select a different model.`;
      },
    });
  } catch (error) {
    console.error(`Failed to stream with model ${modelId}:`, error);
    const errorMessage = error instanceof Error ? error.message : String(error);
    return new Response(
      JSON.stringify({
        error: `Failed to initialize ${modelId}: ${errorMessage}. Please check your API keys and try again.`
      }),
      { status: 500, headers: { 'Content-Type': 'application/json' } }
    );
  }
}



================================================
File: app/api/tavily-chat/route.ts
================================================
// app/api/tavily-chat/route.ts
import { myProvider, modelApiNames } from "@/lib/models";
import { Message, smoothStream, streamText } from "ai";
import { NextRequest } from "next/server";
import { searchTavily } from "@/tools/tavily-search";

export async function POST(request: NextRequest) {
  const {
    messages,
    selectedModelId,
    isReasoningEnabled,
    searchQuery,
  }: {
    messages: Array<Message>;
    selectedModelId: string;
    isReasoningEnabled: boolean;
    searchQuery?: string;
  } = await request.json();

  // Check if messages contain PDF or image attachments
  const messagesHavePDF = messages.some(message =>
    message.experimental_attachments?.some(
      a => a.contentType === 'application/pdf',
    ),
  );
  
  const messagesHaveImage = messages.some(message =>
    message.experimental_attachments?.some(
      a => a.contentType?.startsWith('image/'),
    ),
  );

  // Configure provider-specific options based on the selected model
  const providerOptions: Record<string, any> = {};
  
  // Default to Claude 3.7 Sonnet if no model is selected
  let modelId = selectedModelId || "claude-3.7-sonnet";
  
  // Override model selection for multimodal content if needed
  if (messagesHavePDF) {
    // For PDFs, Claude, GPT-4o, and Gemini all support PDFs
    if (!modelId.startsWith("gemini")) {
      modelId = "gemini-2.0-flash";
    }
  } else if (messagesHaveImage) {
    // For images, ensure we're using a model that supports image input
    // Claude, GPT-4o, and Gemini all support images
    if (!modelId.startsWith("gemini") && !modelId.startsWith("o3") && !modelId.startsWith("gemini")) {
      modelId = "gemini-2.0-flash";
    }
  }
  
  // Get the API model name for the selected model ID
  const apiModelName = modelApiNames[modelId];
  
  if (!apiModelName) {
    console.error(`No API model name found for model ID: ${modelId}`);
    return new Response(
      JSON.stringify({
        error: `Invalid model ID: ${modelId}. Please select a valid model.`
      }),
      { status: 400, headers: { 'Content-Type': 'application/json' } }
    );
  }
  
  // Configure provider-specific options based on the selected model
  if (modelId.startsWith("claude")) {
    providerOptions.anthropic = {
      thinking: {
        type: isReasoningEnabled ? "enabled" : "disabled",
        budgetTokens: 12000,
      },
      model: apiModelName,
    };
  } else if (modelId.startsWith("o3")) {
    providerOptions.openai = {
      temperature: 0.2,
      model: "o3-mini", // Use the exact model name
    };
  } else if (modelId.startsWith("gemini")) {
    providerOptions.google = {
      temperature: 0.2,
      model: apiModelName,
    };
  } else if (modelId.includes("qwen")) {
    providerOptions.groq = {
      temperature: 0.2,
      model: apiModelName,
    };
  } else if (modelId.includes("codestral")) {
    providerOptions.mistral = {
      temperature: 0.2,
      model: apiModelName,
    };
  } else if (modelId.includes("perplexity")) {
    providerOptions.perplexity = {
      temperature: 0.2,
      model: apiModelName,
    };
  } else if (modelId.includes("google/gemini-2.0-flash-thinking-exp:free")) {
    providerOptions.openrouter = {
      temperature: 0.2,
      model: apiModelName,
    };
  }

  try {
    console.log(`Attempting to use model: ${modelId} with options:`, providerOptions);
    
    // Perform Tavily search if searchQuery is provided
    let searchResults = null;
    if (searchQuery) {
      try {
        searchResults = await searchTavily({
          query: searchQuery,
          searchDepth: "basic",
          maxResults: 5,
          includeAnswer: true,
          modelId: modelId // Pass the model ID to track which model triggered the search
        });
        console.log("Tavily search results:", searchResults);
      } catch (error) {
        console.error("Error performing Tavily search:", error);
      }
    }
    
    // Add multimodal context to system prompt if attachments are present
    let systemPrompt = `
<prompt>
You are an AI researcher and engineer with deep research expertise. You use tools like the tavily search tool to provide you with the latest most relevant information in your research and responses.  
</prompt>
    `;
    
    if (messagesHavePDF) {
      systemPrompt += " The user has uploaded a PDF document. Analyze its content and respond to their questions about it.";
    } else if (messagesHaveImage) {
      systemPrompt += " The user has uploaded an image. Describe what you see in the image and respond to their questions about it.";
    }
    
    // Add search results to the system prompt if available
    if (searchResults) {
      systemPrompt += `\n\nI have searched the web for information related to the user's query. Here are the search results:
${JSON.stringify(searchResults, null, 2)}

Use these search results to provide a more informed response to the user's question. Always cite your sources.`;
    }
    
    const stream = streamText({
      system: systemPrompt,
      providerOptions,
      model: myProvider.languageModel(modelId),
      experimental_transform: [
        smoothStream({
          chunking: "word",
        }),
      ],
      messages,
    });

    return stream.toDataStreamResponse({
      sendReasoning: true,
      getErrorMessage: (error) => {
        console.error(`Error with model ${modelId}:`, error);
        const errorMessage = error instanceof Error ? error.message : String(error);
        return `An error occurred with ${modelId}: ${errorMessage}. Please try again or select a different model.`;
      },
    });
  } catch (error) {
    console.error(`Failed to stream with model ${modelId}:`, error);
    const errorMessage = error instanceof Error ? error.message : String(error);
    return new Response(
      JSON.stringify({
        error: `Failed to initialize ${modelId}: ${errorMessage}. Please check your API keys and try again.`
      }),
      { status: 500, headers: { 'Content-Type': 'application/json' } }
    );
  }
}



================================================
File: app/api/tavily-search/route.ts
================================================
// app/api/tavily-search/route.ts
import { NextRequest } from "next/server";
import { searchTavily, TavilySearchParams } from "@/tools/tavily-search";

export async function POST(request: NextRequest) {
  try {
    const params: TavilySearchParams = await request.json();
    
    if (!params.query) {
      return new Response(
        JSON.stringify({ error: "Query parameter is required" }),
        { status: 400, headers: { 'Content-Type': 'application/json' } }
      );
    }
    
    // If modelId is not provided, use a default identifier for direct API calls
    if (!params.modelId) {
      params.modelId = "direct-api-call";
    }
    
    const results = await searchTavily(params);
    
    return new Response(
      JSON.stringify(results),
      { status: 200, headers: { 'Content-Type': 'application/json' } }
    );
  } catch (error) {
    console.error("Tavily API route error:", error);
    const errorMessage = error instanceof Error ? error.message : String(error);
    
    return new Response(
      JSON.stringify({ error: `Tavily search failed: ${errorMessage}` }),
      { status: 500, headers: { 'Content-Type': 'application/json' } }
    );
  }
}



================================================
File: app/tavily-ai-search/page.tsx
================================================
import { TavilyChat } from "@/components/tavily-chat";

export default function TavilySearchPage() {
  return (
    <div className="flex flex-col size-full items-center">
      <TavilyChat />
    </div>
  );
}



================================================
File: components/chat.tsx
================================================
"use client";

import cn from "classnames";
import { toast } from "sonner";
import { useChat } from "@ai-sdk/react";
import { useState, useRef } from "react";
import { Messages } from "./messages";
import { models } from "@/lib/models";
import { Footnote } from "./footnote";
import { ArrowUpIcon, CheckedSquare, StopIcon, UncheckedSquare, PaperClipIcon, XIcon } from "./icons";
import { ModelSelector } from "./model-selector";
import { Input } from "./input";
import Image from "next/image";

export function Chat() {
  const [input, setInput] = useState<string>("");
  const [selectedModelId, setSelectedModelId] = useState<string>("claude-3.7-sonnet");
  const [isReasoningEnabled, setIsReasoningEnabled] = useState<boolean>(true);
  const [files, setFiles] = useState<FileList | null>(null);
  const fileInputRef = useRef<HTMLInputElement>(null);

  // Generate a unique chat ID for each page load to ensure proper reset
  const chatId = useRef<string>(`chat-${Date.now()}`).current;

  // Default values for the following features 
  const reasoningModeEnabled = true;
  const multimodalEnabled = true;

  const selectedModel = models.find((model) => model.id === selectedModelId);

  const { messages, append, status, stop } = useChat({
    id: chatId,
    body: {
      selectedModelId,
      isReasoningEnabled: reasoningModeEnabled ? isReasoningEnabled : false,
    },
    onError: () => {
      toast.error("An error occurred, please try again!");
    },
  });

  const isGeneratingResponse = ["streaming", "submitted"].includes(status);

  const handleSendMessage = () => {
    if (input === "" && (!files || files.length === 0)) {
      return;
    }

    if (isGeneratingResponse) {
      stop();
    } else {
      append({
        role: "user",
        content: input,
      }, {
        experimental_attachments: files || undefined,
      });
    }

    setInput("");
    setFiles(null);
    if (fileInputRef.current) {
      fileInputRef.current.value = "";
    }
  };

  const handleRemoveFile = () => {
    setFiles(null);
    if (fileInputRef.current) {
      fileInputRef.current.value = "";
    }
  };

  // Create file preview URL
  const filePreviewUrl = files && files.length > 0 && files[0].type.startsWith('image/') 
    ? URL.createObjectURL(files[0]) 
    : null;

  return (
    <div
      className={cn(
        "px-4 md:px-0 pb-4 pt-8 flex flex-col h-dvh items-center w-full",
        {
          "justify-between": messages.length > 0,
          "justify-center gap-4": messages.length === 0,
        },
      )}
    >
      {messages.length > 0 ? (
        <Messages messages={messages} status={status} />
      ) : (
        <div className="flex flex-col gap-0.5 sm:text-2xl text-xl md:w-1/2 w-full">
          <div className="flex flex-row gap-2 items-center">
            <div>GKennedy Multi-LLM AI.</div>
          </div>
          <div className="dark:text-zinc-500 text-zinc-400">
            Search Less, Learn More
          </div>
        </div>
      )}

      <div className="flex flex-col gap-4 md:w-1/2 w-full">
        <div className="w-full relative p-3 dark:bg-zinc-800 rounded-2xl flex flex-col gap-1 bg-zinc-100">
          {multimodalEnabled && files && files.length > 0 && (
            <div className="mb-2 flex items-center" data-testid="file-preview">
              {filePreviewUrl ? (
                <div className="relative w-16 h-16 mr-2">
                  <Image 
                    src={filePreviewUrl} 
                    alt={files[0].name}
                    fill
                    style={{ objectFit: 'cover' }}
                    className="rounded-md"
                  />
                </div>
              ) : (
                <div className="flex items-center justify-center w-16 h-16 bg-zinc-200 dark:bg-zinc-700 rounded-md mr-2">
                  <span className="text-xs">{files[0].name.split('.').pop()?.toUpperCase()}</span>
                </div>
              )}
              <div className="flex-1">
                <div className="text-sm truncate">{files[0].name}</div>
                <div className="text-xs text-zinc-500">{(files[0].size / 1024).toFixed(1)} KB</div>
              </div>
              <button 
                onClick={handleRemoveFile}
                className="p-1 rounded-full hover:bg-zinc-200 dark:hover:bg-zinc-700"
              >
                <XIcon className="h-4 w-4" />
              </button>
            </div>
          )}
          
          <Input
            input={input}
            setInput={setInput}
            selectedModelId={selectedModelId}
            isGeneratingResponse={isGeneratingResponse}
            isReasoningEnabled={reasoningModeEnabled ? isReasoningEnabled : false}
            append={append}
          />

          {reasoningModeEnabled && (
            <div className="absolute bottom-2.5 left-2.5">
              <div
                className={cn(
                  "relative w-fit text-sm p-1.5 rounded-lg flex flex-row items-center gap-2 dark:hover:bg-zinc-600 hover:bg-zinc-200 cursor-pointer",
                  {
                    "dark:bg-zinc-600 bg-zinc-200": isReasoningEnabled,
                  },
                )}
                onClick={() => {
                  setIsReasoningEnabled(!isReasoningEnabled);
                }}
              >
                {isReasoningEnabled ? <CheckedSquare /> : <UncheckedSquare />}
                <div>Reasoning</div>
              </div>
            </div>
          )}

          <div className="absolute bottom-2.5 right-2.5 flex flex-row gap-2">
            {multimodalEnabled && (
              <button
                className="size-8 flex flex-row justify-center items-center dark:bg-zinc-700 bg-zinc-300 dark:text-zinc-300 text-zinc-700 p-1.5 rounded-full hover:bg-zinc-400 dark:hover:bg-zinc-600 hover:scale-105 active:scale-95 transition-all"
                onClick={() => fileInputRef.current?.click()}
              >
                <PaperClipIcon />
                <input
                  type="file"
                  className="hidden"
                  onChange={(e) => setFiles(e.target.files)}
                  ref={fileInputRef}
                  accept="image/*, application/pdf"
                  data-testid="file-upload"
                />
              </button>
            )}
            
            <ModelSelector 
              selectedModelId={selectedModelId}
              setSelectedModelId={setSelectedModelId}
            />

            <button
              className={cn(
                "size-8 flex flex-row justify-center items-center dark:bg-zinc-100 bg-zinc-900 dark:text-zinc-900 text-zinc-100 p-1.5 rounded-full hover:bg-zinc-800 dark:hover:bg-zinc-300 hover:scale-105 active:scale-95 transition-all",
                {
                  "dark:bg-zinc-200 dark:text-zinc-500":
                    isGeneratingResponse || (input === "" && (!files || files.length === 0)),
                },
              )}
              onClick={handleSendMessage}
              aria-label="send"
            >
              {isGeneratingResponse ? <StopIcon /> : <ArrowUpIcon />}
            </button>
          </div>
        </div>

        <Footnote />
      </div>
    </div>
  );
}



================================================
File: components/deploy-button.tsx
================================================
import Link from "next/link";
export const DeployButton = () => (
  <Link
    href={`https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fvercel-labs%2Fai-sdk-preview-reasoning%2Ftree%2Fmain&env=ANTHROPIC_API_KEY&envDescription=Anthropic%20API%20key&envLink=https%3A%2F%2Fconsole.anthropic.com%2F`}
    target="_blank"
    rel="noopener noreferrer"
    className="inline-flex items-center gap-2 ml-2 bg-black text-white text-sm px-3 py-1.5 rounded-md hover:bg-zinc-900 dark:bg-white dark:text-black dark:hover:bg-zinc-100"
  >
    <svg
      data-testid="geist-icon"
      height={14}
      strokeLinejoin="round"
      viewBox="0 0 16 16"
      width={14}
      style={{ color: "currentcolor" }}
    >
      <path
        fillRule="evenodd"
        clipRule="evenodd"
        d="M8 1L16 15H0L8 1Z"
        fill="currentColor"
      />
    </svg>
    Deploy
  </Link>
);



================================================
File: components/footnote.tsx
================================================
import Link from 'next/link';

export function Footnote() {
  return (
    <div className="text-xs text-zinc-400 leading-5 hidden sm:block">
      This app is built using{' '}
      <Link
        className="underline underline-offset-2"
        href="https://nextjs.org/"
        target="_blank"
      >
        Next.js
      </Link>{' '}
      15{' '}
      <Link
        className="underline underline-offset-2"
        href="https://sdk.vercel.ai/"
        target="_blank"
      >
        React 19
      </Link>
      . GKennedy, Search Less, Learn More{' '}
      <Link
        className="underline underline-offset-2"
        href="https://gkennedy.com"
        target="_blank"
      >
        documentation
      </Link>
      .
    </div>
  );
}


================================================
File: components/icons.tsx
================================================
export const ArrowUpIcon = ({ size = 16 }: { size?: number }) => (
  <svg
    height={size}
    strokeLinejoin="round"
    viewBox="0 0 16 16"
    width={size}
    style={{ color: "currentcolor" }}
  >
    <path
      fillRule="evenodd"
      clipRule="evenodd"
      d="M8.70711 1.39644C8.31659 1.00592 7.68342 1.00592 7.2929 1.39644L2.21968 6.46966L1.68935 6.99999L2.75001 8.06065L3.28034 7.53032L7.25001 3.56065V14.25V15H8.75001V14.25V3.56065L12.7197 7.53032L13.25 8.06065L14.3107 6.99999L13.7803 6.46966L8.70711 1.39644Z"
      fill="currentColor"
    ></path>
  </svg>
);

export const ChevronDownIcon = ({ size = 16 }: { size?: number }) => (
  <svg
    height={size}
    strokeLinejoin="round"
    viewBox="0 0 16 16"
    width={size}
    style={{ color: "currentcolor" }}
  >
    <path
      fillRule="evenodd"
      clipRule="evenodd"
      d="M12.0607 6.74999L11.5303 7.28032L8.7071 10.1035C8.31657 10.4941 7.68341 10.4941 7.29288 10.1035L4.46966 7.28032L3.93933 6.74999L4.99999 5.68933L5.53032 6.21966L7.99999 8.68933L10.4697 6.21966L11 5.68933L12.0607 6.74999Z"
      fill="currentColor"
    />
  </svg>
);

export const ChevronUpIcon = ({ size = 16 }: { size?: number }) => (
  <svg
    height={size}
    strokeLinejoin="round"
    viewBox="0 0 16 16"
    width={size}
    style={{ color: "currentcolor" }}
    className="rotate-0"
  >
    <path
      fillRule="evenodd"
      clipRule="evenodd"
      d="M6.74999 3.93933L7.28032 4.46966L10.1035 7.29288C10.4941 7.68341 10.4941 8.31657 10.1035 8.7071L7.28032 11.5303L6.74999 12.0607L5.68933 11L6.21966 10.4697L8.68933 7.99999L6.21966 5.53032L5.68933 4.99999L6.74999 3.93933Z"
      fill="currentColor"
    ></path>
  </svg>
);

export const SpinnerIcon = ({ size = 16 }: { size?: number }) => (
  <svg
    height={size}
    strokeLinejoin="round"
    viewBox="0 0 16 16"
    width={size}
    style={{ color: "currentcolor" }}
  >
    <g clipPath="url(#clip0_2393_1490)">
      <path d="M8 0V4" stroke="currentColor" strokeWidth="1.5" />
      <path
        opacity="0.5"
        d="M8 16V12"
        stroke="currentColor"
        strokeWidth="1.5"
      />
      <path
        opacity="0.9"
        d="M3.29773 1.52783L5.64887 4.7639"
        stroke="currentColor"
        strokeWidth="1.5"
      />
      <path
        opacity="0.1"
        d="M12.7023 1.52783L10.3511 4.7639"
        stroke="currentColor"
        strokeWidth="1.5"
      />
      <path
        opacity="0.4"
        d="M12.7023 14.472L10.3511 11.236"
        stroke="currentColor"
        strokeWidth="1.5"
      />
      <path
        opacity="0.6"
        d="M3.29773 14.472L5.64887 11.236"
        stroke="currentColor"
        strokeWidth="1.5"
      />
      <path
        opacity="0.2"
        d="M15.6085 5.52783L11.8043 6.7639"
        stroke="currentColor"
        strokeWidth="1.5"
      />
      <path
        opacity="0.7"
        d="M0.391602 10.472L4.19583 9.23598"
        stroke="currentColor"
        strokeWidth="1.5"
      />
      <path
        opacity="0.3"
        d="M15.6085 10.4722L11.8043 9.2361"
        stroke="currentColor"
        strokeWidth="1.5"
      />
      <path
        opacity="0.8"
        d="M0.391602 5.52783L4.19583 6.7639"
        stroke="currentColor"
        strokeWidth="1.5"
      />
    </g>
    <defs>
      <clipPath id="clip0_2393_1490">
        <rect width="16" height="16" fill="white" />
      </clipPath>
    </defs>
  </svg>
);

export const VercelIcon = ({ size = 17 }) => {
  return (
    <svg
      height={size}
      strokeLinejoin="round"
      viewBox="0 0 16 16"
      width={size}
      style={{ color: "currentcolor" }}
    >
      <path
        fillRule="evenodd"
        clipRule="evenodd"
        d="M8 1L16 15H0L8 1Z"
        fill="currentColor"
      />
    </svg>
  );
};

export const StopIcon = ({ size = 16 }: { size?: number }) => {
  return (
    <svg
      height={size}
      viewBox="0 0 16 16"
      width={size}
      style={{ color: "currentcolor" }}
    >
      <path
        fillRule="evenodd"
        clipRule="evenodd"
        d="M3 3H13V13H3V3Z"
        fill="currentColor"
      />
    </svg>
  );
};

export const CheckedSquare = ({ size = 16 }: { size?: number }) => {
  return (
    <svg
      height={size}
      strokeLinejoin="round"
      viewBox="0 0 16 16"
      width={size}
      style={{ color: "currentcolor" }}
    >
      <path
        fillRule="evenodd"
        clipRule="evenodd"
        d="M15 16H1C0.447715 16 0 15.5523 0 15V1C0 0.447715 0.447716 0 1 0L15 8.17435e-06C15.5523 8.47532e-06 16 0.447724 16 1.00001V15C16 15.5523 15.5523 16 15 16ZM11.7803 6.28033L12.3107 5.75L11.25 4.68934L10.7197 5.21967L6.5 9.43935L5.28033 8.21967L4.75001 7.68934L3.68934 8.74999L4.21967 9.28033L5.96967 11.0303C6.11032 11.171 6.30109 11.25 6.5 11.25C6.69891 11.25 6.88968 11.171 7.03033 11.0303L11.7803 6.28033Z"
        fill="currentColor"
      ></path>
    </svg>
  );
};

export const UncheckedSquare = ({ size = 16 }: { size?: number }) => {
  return (
    <svg
      height={size}
      strokeLinejoin="round"
      viewBox="0 0 16 16"
      width={size}
      style={{ color: "currentcolor" }}
    >
      <rect
        x="1"
        y="1"
        width="14"
        height="14"
        stroke="currentColor"
        strokeWidth="1.5"
        fill="none"
      />
    </svg>
  );
};

export const PaperClipIcon = ({ size = 16, className = "" }: { size?: number, className?: string }) => {
  return (
    <svg
      height={size}
      width={size}
      viewBox="0 0 16 16"
      className={className}
      style={{ color: "currentcolor" }}
    >
      <path
        fillRule="evenodd"
        clipRule="evenodd"
        d="M8 1.5a.5.5 0 01.5.5v4.793l1.146-1.147a.5.5 0 01.708.708l-2 2a.5.5 0 01-.708 0l-2-2a.5.5 0 11.708-.708L7.5 6.793V2a.5.5 0 01.5-.5zM4 9.5a.5.5 0 00-1 0v2A2.5 2.5 0 005.5 14h5a2.5 2.5 0 002.5-2.5v-2a.5.5 0 00-1 0v2a1.5 1.5 0 01-1.5 1.5h-5A1.5 1.5 0 014 11.5v-2z"
        fill="currentColor"
      />
    </svg>
  );
};

export const XIcon = ({ size = 16, className = "" }: { size?: number, className?: string }) => {
  return (
    <svg
      height={size}
      width={size}
      viewBox="0 0 16 16"
      className={className}
      style={{ color: "currentcolor" }}
    >
      <path
        fillRule="evenodd"
        clipRule="evenodd"
        d="M12.0607 3.93934L8 8.00001L3.93934 3.93934L2.87868 5.00001L6.93934 9.06067L2.87868 13.1213L3.93934 14.182L8 10.1213L12.0607 14.182L13.1213 13.1213L9.06066 9.06067L13.1213 5.00001L12.0607 3.93934Z"
        fill="currentColor"
      />
    </svg>
  );
};

export function SearchIcon({ size = 16, className = "" }: { size?: number, className?: string }) {
  return (
    <svg
      xmlns="http://www.w3.org/2000/svg"
      width={size}
      height={size}
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      strokeWidth="2"
      strokeLinecap="round"
      strokeLinejoin="round"
      className={className}
    >
      <circle cx="11" cy="11" r="8" />
      <path d="m21 21-4.3-4.3" />
    </svg>
  );
}



================================================
File: components/input.tsx
================================================
"use client";

import { toast } from "sonner";
import { Message, CreateMessage, ChatRequestOptions } from "ai";

interface InputProps {
  input: string;
  setInput: (value: string) => void;
  selectedModelId: string;
  isGeneratingResponse: boolean;
  isReasoningEnabled: boolean;
  append?: (message: Message | CreateMessage, chatRequestOptions?: ChatRequestOptions) => Promise<string | null | undefined>;
}

export function Input({
  input,
  setInput,
  selectedModelId,
  isGeneratingResponse,
  isReasoningEnabled,
  append,
}: InputProps) {

  return (
    <textarea
      className="mb-12 resize-none w-full min-h-12 outline-none bg-transparent placeholder:text-zinc-400"
      placeholder="Send a message"
      value={input}
      autoFocus
      onChange={(event) => {
        setInput(event.currentTarget.value);
      }}
      onKeyDown={(event) => {
        if (event.key === "Enter" && !event.shiftKey) {
          event.preventDefault();

          if (input === "") {
            return;
          }

          if (isGeneratingResponse) {
            toast.error("Please wait for the model to finish its response!");

            return;
          }

          if (append) {
            append({
              role: "user",
              content: input,
            });
          }

          setInput("");
        }
      }}
    />
  );
}



================================================
File: components/markdown-components.tsx
================================================
import { Components } from "react-markdown";
import Link from "next/link";

export const markdownComponents: Partial<Components> = {
  p: ({ children }) => <p className="leading-6">{children}</p>,
  pre: ({ children }) => <>{children}</>,
  ol: ({ children, ...props }) => {
    return (
      <ol className="list-decimal list-outside ml-4" {...props}>
        {children}
      </ol>
    );
  },
  li: ({ children, ...props }) => {
    return (
      <li className="py-1" {...props}>
        {children}
      </li>
    );
  },
  ul: ({ children, ...props }) => {
    return (
      <ul className="list-decimal list-outside ml-4" {...props}>
        {children}
      </ul>
    );
  },
  strong: ({ children, ...props }) => {
    return (
      <span className="font-semibold" {...props}>
        {children}
      </span>
    );
  },
  a: ({ children, ...props }) => {
    return (
      // @ts-expect-error - Link component expects href prop from markdown-parsed anchor tags
      <Link
        className="text-blue-500 hover:underline"
        target="_blank"
        rel="noreferrer"
        {...props}
      >
        {children}
      </Link>
    );
  },
  h1: ({ children, ...props }) => {
    return (
      <h1 className="text-3xl font-semibold mt-6 mb-2" {...props}>
        {children}
      </h1>
    );
  },
  h2: ({ children, ...props }) => {
    return (
      <h2 className="text-2xl font-semibold mt-6 mb-2" {...props}>
        {children}
      </h2>
    );
  },
  h3: ({ children, ...props }) => {
    return (
      <h3 className="text-xl font-semibold mt-6 mb-2" {...props}>
        {children}
      </h3>
    );
  },
  h4: ({ children, ...props }) => {
    return (
      <h4 className="text-lg font-semibold mt-6 mb-2" {...props}>
        {children}
      </h4>
    );
  },
  h5: ({ children, ...props }) => {
    return (
      <h5 className="text-base font-semibold mt-6 mb-2" {...props}>
        {children}
      </h5>
    );
  },
  h6: ({ children, ...props }) => {
    return (
      <h6 className="text-sm font-semibold mt-6 mb-2" {...props}>
        {children}
      </h6>
    );
  },
};



================================================
File: components/messages.tsx
================================================
"use client";

import cn from "classnames";
import Markdown from "react-markdown";
import { markdownComponents } from "./markdown-components";
import { AnimatePresence, motion } from "framer-motion";
import { useEffect, useMemo, useRef, useState } from "react";
import { ChevronDownIcon, ChevronUpIcon, SpinnerIcon } from "./icons";
import { UIMessage } from "ai";
import { UseChatHelpers } from "@ai-sdk/react";
import Image from "next/image";

interface ReasoningPart {
  type: "reasoning";
  reasoning: string;
  details: Array<{ type: "text"; text: string }>;
}

interface ReasoningMessagePartProps {
  part: ReasoningPart;
  isReasoning: boolean;
}

export function ReasoningMessagePart({
  part,
  isReasoning,
}: ReasoningMessagePartProps) {
  const [isExpanded, setIsExpanded] = useState(true);

  const variants = {
    collapsed: {
      height: 0,
      opacity: 0,
      marginTop: 0,
      marginBottom: 0,
    },
    expanded: {
      height: "auto",
      opacity: 1,
      marginTop: "1rem",
      marginBottom: 0,
    },
  };

  return (
    <div className="flex flex-col">
      {isReasoning ? (
        <div className="flex flex-row gap-2 items-center">
          <div className="font-medium text-sm">Reasoning</div>
          <div className="animate-spin">
            <SpinnerIcon />
          </div>
        </div>
      ) : (
        <div className="flex flex-row gap-2 items-center">
          <div className="font-medium text-sm">Reasoned for a few seconds</div>
          <button
            className={cn(
              "cursor-pointer rounded-full dark:hover:bg-zinc-800 hover:bg-zinc-200",
              {
                "dark:bg-zinc-800 bg-zinc-200": isExpanded,
              },
            )}
            onClick={() => {
              setIsExpanded(!isExpanded);
            }}
          >
            {isExpanded ? <ChevronDownIcon /> : <ChevronUpIcon />}
          </button>
        </div>
      )}

      <AnimatePresence initial={false}>
        {isExpanded && (
          <motion.div
            key="reasoning"
            className="text-sm dark:text-zinc-400 text-zinc-600 flex flex-col gap-4 border-l pl-3 dark:border-zinc-800"
            initial="collapsed"
            animate="expanded"
            exit="collapsed"
            variants={variants}
            transition={{ duration: 0.2, ease: "easeInOut" }}
          >
            {part.details.map((detail, detailIndex) =>
              detail.type === "text" ? (
                <Markdown key={detailIndex} components={markdownComponents}>
                  {detail.text}
                </Markdown>
              ) : (
                "<redacted>"
              ),
            )}

            {/* <Markdown components={markdownComponents}>{reasoning}</Markdown> */}
          </motion.div>
        )}
      </AnimatePresence>
    </div>
  );
}

interface TextMessagePartProps {
  text: string;
}

export function TextMessagePart({ text }: TextMessagePartProps) {
  return (
    <div className="flex flex-col gap-4">
      <Markdown components={markdownComponents}>{text}</Markdown>
    </div>
  );
}

interface AttachmentProps {
  attachment: {
    name?: string;
    url: string;
    contentType: string;
  };
  index: number;
  messageId: string;
}

export function Attachment({ attachment, index, messageId }: AttachmentProps) {
  if (attachment.contentType.startsWith('image/')) {
    return (
      <div className="mt-2 relative w-full max-w-md h-auto">
        <Image
          key={`${messageId}-${index}`}
          src={attachment.url}
          alt={attachment.name || `attachment-${index}`}
          width={500}
          height={300}
          className="rounded-md object-contain"
          style={{ maxHeight: '300px' }}
        />
      </div>
    );
  } else if (attachment.contentType.startsWith('application/pdf')) {
    return (
      <div className="mt-2 w-full max-w-md">
        <div className="bg-zinc-100 dark:bg-zinc-800 p-2 rounded-md flex items-center gap-2 mb-2">
          <div className="text-sm font-medium">PDF Document: {attachment.name || `Document-${index}`}</div>
        </div>
        <iframe
          key={`${messageId}-${index}`}
          src={attachment.url}
          title={attachment.name || `attachment-${index}`}
          className="w-full rounded-md border border-zinc-300 dark:border-zinc-700"
          height={400}
        />
      </div>
    );
  }
  
  return null;
}

interface MessagesProps {
  messages: Array<UIMessage>;
  status: UseChatHelpers["status"];
}

export function Messages({ messages, status }: MessagesProps) {
  const messagesRef = useRef<HTMLDivElement>(null);
  const messagesLength = useMemo(() => messages.length, [messages]);

  useEffect(() => {
    if (messagesRef.current) {
      messagesRef.current.scrollTop = messagesRef.current.scrollHeight;
    }
  }, [messagesLength]);

  return (
    <div
      className="flex flex-col gap-8 overflow-y-scroll items-center w-full"
      ref={messagesRef}
    >
      {messages.map((message) => (
        <div
          key={message.id}
          className={cn(
            "flex flex-col gap-4 last-of-type:mb-12 first-of-type:mt-16 md:w-1/2 w-full",
          )}
        >
          <div
            className={cn("flex flex-col gap-4", {
              "dark:bg-zinc-800 bg-zinc-200 p-2 rounded-xl w-fit ml-auto":
                message.role === "user",
              "": message.role === "assistant",
            })}
          >
            {/* Display message text content */}
            {message.parts.map((part, partIndex) => {
              if (part.type === "text") {
                return (
                  <TextMessagePart
                    key={`${message.id}-${partIndex}`}
                    text={part.text}
                  />
                );
              }

              if (part.type === "reasoning") {
                return (
                  <ReasoningMessagePart
                    key={`${message.id}-${partIndex}`}
                    // @ts-expect-error export ReasoningUIPart
                    part={part}
                    isReasoning={
                      status === "streaming" &&
                      partIndex === message.parts.length - 1
                    }
                  />
                );
              }
            })}
            
            {/* Display attachments if present */}
            {message.experimental_attachments && message.experimental_attachments.length > 0 && (
              <div className="flex flex-col gap-2">
                {message.experimental_attachments.map((attachment, attachmentIndex) => {
                  // Ensure attachment has required properties before rendering
                  if (attachment && attachment.url && attachment.contentType) {
                    return (
                      <Attachment 
                        key={`${message.id}-attachment-${attachmentIndex}`}
                        attachment={{
                          name: attachment.name,
                          url: attachment.url,
                          contentType: attachment.contentType
                        }}
                        index={attachmentIndex}
                        messageId={message.id}
                      />
                    );
                  }
                  return null;
                })}
              </div>
            )}
          </div>
        </div>
      ))}

      {status === "submitted" && (
        <div className="text-zinc-500 mb-12 md:w-1/2 w-full">Hmm...</div>
      )}
    </div>
  );
}



================================================
File: components/model-selector.tsx
================================================
"use client";

import { useState, useRef, useEffect } from "react";
import cn from "classnames";
import { models } from "@/lib/models";
import { ChevronDownIcon } from "./icons";

interface ModelSelectorProps {
  selectedModelId: string;
  setSelectedModelId: (modelId: string) => void;
}

export function ModelSelector({
  selectedModelId,
  setSelectedModelId,
}: ModelSelectorProps) {
  const [isOpen, setIsOpen] = useState(false);
  const dropdownRef = useRef<HTMLDivElement>(null);
  
  const selectedModel = models.find((model) => model.id === selectedModelId);

  useEffect(() => {
    const handleClickOutside = (event: MouseEvent) => {
      if (dropdownRef.current && !dropdownRef.current.contains(event.target as Node)) {
        setIsOpen(false);
      }
    };

    document.addEventListener("mousedown", handleClickOutside);
    return () => {
      document.removeEventListener("mousedown", handleClickOutside);
    };
  }, []);

  return (
    <div className="relative" ref={dropdownRef}>
      <button
        className="relative w-fit text-sm p-1.5 rounded-lg flex flex-row items-center gap-0.5 dark:hover:bg-zinc-700 hover:bg-zinc-200 cursor-pointer"
        onClick={() => setIsOpen(!isOpen)}
        aria-haspopup="listbox"
        aria-expanded={isOpen}
      >
        <div>
          {selectedModel ? selectedModel.name : "Models Unavailable!"}
        </div>
        <div className="text-zinc-500">
          <ChevronDownIcon />
        </div>
      </button>

      {isOpen && (
        <div className="absolute bottom-full mb-2 right-0 w-64 max-h-80 overflow-y-auto bg-white dark:bg-zinc-800 rounded-lg shadow-lg z-10">
          <ul
            className="py-1"
            role="listbox"
            aria-labelledby="model-selector"
          >
            {models.map((model) => (
              <li
                key={model.id}
                className={cn(
                  "px-4 py-2 cursor-pointer hover:bg-zinc-100 dark:hover:bg-zinc-700",
                  {
                    "bg-zinc-100 dark:bg-zinc-700": model.id === selectedModelId,
                  }
                )}
                role="option"
                aria-selected={model.id === selectedModelId}
                onClick={() => {
                  setSelectedModelId(model.id);
                  setIsOpen(false);
                }}
              >
                <div className="font-medium">{model.name}</div>
                <div className="text-xs text-zinc-500 dark:text-zinc-400 line-clamp-2">
                  {model.description}
                </div>
              </li>
            ))}
          </ul>
        </div>
      )}
    </div>
  );
}



================================================
File: components/navigation.tsx
================================================
"use client";

import Link from "next/link";
import { usePathname } from "next/navigation";
import { SearchIcon } from "./icons";
import cn from "classnames";

export function Navigation() {
  const pathname = usePathname();
  
  return (
    <div className="fixed top-4 right-4 z-10 flex gap-2">
      <Link
        href="/"
        className={cn(
          "p-2 rounded-full hover:bg-zinc-200 dark:hover:bg-zinc-700 transition-colors",
          {
            "bg-zinc-200 dark:bg-zinc-700": pathname === "/",
          }
        )}
        aria-label="Home"
      >
        <svg
          xmlns="http://www.w3.org/2000/svg"
          width="24"
          height="24"
          viewBox="0 0 24 24"
          fill="none"
          stroke="currentColor"
          strokeWidth="2"
          strokeLinecap="round"
          strokeLinejoin="round"
        >
          <path d="m3 9 9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z" />
          <polyline points="9 22 9 12 15 12 15 22" />
        </svg>
      </Link>
      
      <Link
        href="/tavily-ai-search"
        className={cn(
          "p-2 rounded-full hover:bg-zinc-200 dark:hover:bg-zinc-700 transition-colors",
          {
            "bg-zinc-200 dark:bg-zinc-700": pathname === "/tavily-ai-search",
          }
        )}
        aria-label="Tavily AI Search"
      >
        <SearchIcon size={24} />
      </Link>
    </div>
  );
}



================================================
File: components/new-chat-button.tsx
================================================
"use client";

import { PlusCircle } from "lucide-react";
import { useRouter } from "next/navigation";
import { motion } from "framer-motion";
import { cn } from "@/lib/utils";
import { useSidebar } from "./sidebar";
import { createClient } from "@/utils/supabase/client";
import { toast } from "sonner";

export function NewChatButton() {
  const { open } = useSidebar();
  const router = useRouter();
  const supabase = createClient();

  const handleNewChat = async () => {
    try {
      // Get the current user
      const { data: userData, error: userError } = await supabase.auth.getUser();
      
      if (userError || !userData.user) {
        console.error('User not authenticated:', userError);
        toast.error("You must be logged in to create a new chat");
        return;
      }
      
      // Create a new chat session in Supabase
      const { data, error } = await supabase
        .from('chat_sessions')
        .insert([
          {
            title: "New Chat",
            user_id: userData.user.id,
          },
        ])
        .select()
        .single();
      
      if (error) {
        console.error('Error creating chat session:', error);
        toast.error("Failed to start a new chat");
        return;
      }
      
      // Force a refresh of the current page to reset the chat state
      router.refresh();
      
      // Show success message
      toast.success("Started a new chat");
    } catch (error) {
      console.error("Error creating new chat:", error);
      toast.error("Failed to start a new chat");
    }
  };

  return (
    <button
      onClick={handleNewChat}
      className={cn(
        "flex items-center gap-2 text-muted-foreground hover:text-foreground px-3 py-2 rounded-md hover:bg-accent transition-colors"
      )}
    >
      <PlusCircle size={20} />
      <motion.span
        animate={{
          opacity: open ? 1 : 0,
          transition: { duration: 0.2 },
        }}
        className="text-sm whitespace-pre"
      >
        New Chat
      </motion.span>
    </button>
  );
}



================================================
File: components/sidebar.tsx
================================================
"use client";

import React, { useState } from "react";
import { cn } from "@/lib/utils";
import Link from "next/link";
import { motion } from "framer-motion";
import { 
  LayoutDashboard, 
  Search,
  MessageSquare,
  Menu,
  X
} from "lucide-react";
import { ChatHistoryDropdown } from "./chat-history/chat-history-dropdown";

interface SidebarLinkProps {
  label: string;
  href: string;
  icon: React.ReactNode;
}

interface SidebarProps {
  open?: boolean;
  setOpen?: React.Dispatch<React.SetStateAction<boolean>>;
  children: React.ReactNode;
}

interface SidebarBodyProps extends React.ComponentProps<typeof motion.div> {
  className?: string;
  children: React.ReactNode;
}

interface SidebarLinkComponentProps {
  link: SidebarLinkProps;
  className?: string;
}

const SidebarContext = React.createContext<{
  open: boolean;
  setOpen: React.Dispatch<React.SetStateAction<boolean>>;
  animate: boolean;
} | null>(null);

export const useSidebar = () => {
  const context = React.useContext(SidebarContext);
  if (!context) {
    throw new Error("useSidebar must be used within a SidebarProvider");
  }
  return context;
};

export const SidebarProvider = ({
  children,
  open: openProp,
  setOpen: setOpenProp,
  animate = true,
}: {
  children: React.ReactNode;
  open?: boolean;
  setOpen?: React.Dispatch<React.SetStateAction<boolean>>;
  animate?: boolean;
}) => {
  const [openState, setOpenState] = useState(false);

  const open = openProp !== undefined ? openProp : openState;
  const setOpen = setOpenProp !== undefined ? setOpenProp : setOpenState;

  return (
    <SidebarContext.Provider value={{ open, setOpen, animate }}>
      {children}
    </SidebarContext.Provider>
  );
};

export const Sidebar = ({ children, open, setOpen }: SidebarProps) => {
  return (
    <SidebarProvider open={open} setOpen={setOpen}>
      {children}
    </SidebarProvider>
  );
};

export const SidebarBody = ({ className, children, ...props }: SidebarBodyProps) => {
  const { open, setOpen, animate } = useSidebar();

  return (
    <>
      <motion.div
        {...props}
        animate={{
          width: open ? "240px" : "80px",
          transition: {
            duration: animate ? 0.2 : 0,
          },
        }}
        className={cn(
          "border-r border-border bg-background fixed h-screen flex flex-col p-3 gap-3",
          className
        )}
      >
        <div className="flex items-center justify-between h-12">
          <Logo />
          <button
            onClick={() => setOpen(!open)}
            className="h-8 w-8 flex items-center justify-center rounded-md hover:bg-accent"
          >
            {open ? <X size={16} /> : <Menu size={16} />}
          </button>
        </div>
        {children}
      </motion.div>
      <div
        style={{
          width: open ? "240px" : "80px",
          transition: animate ? "width 0.2s" : "none",
        }}
      />
    </>
  );
};

export const SidebarLink = ({
  link,
  className,
}: SidebarLinkComponentProps) => {
  const { open } = useSidebar();

  return (
    <Link
      href={link.href}
      className={cn(
        "flex items-center gap-2 text-muted-foreground hover:text-foreground px-3 py-2 rounded-md hover:bg-accent transition-colors",
        className
      )}
    >
      {link.icon}
      <motion.span
        animate={{
          opacity: open ? 1 : 0,
          transition: { duration: 0.2 },
        }}
        className="text-sm whitespace-pre"
      >
        {link.label}
      </motion.span>
    </Link>
  );
};

export const Logo = () => {
  const { open } = useSidebar();
  
  return (
    <Link
      href="/"
      className="font-normal flex space-x-2 items-center text-sm text-foreground py-1 relative z-20"
    >
      <div className="h-5 w-6 bg-primary rounded-br-lg rounded-tr-sm rounded-tl-lg rounded-bl-sm flex-shrink-0" />
      {open && (
        <motion.span
          initial={{ opacity: 0 }}
          animate={{ opacity: 1 }}
          className="font-medium text-foreground whitespace-pre"
        >
          GKennedy AI
        </motion.span>
      )}
    </Link>
  );
};

// Export ChatHistoryDropdown for use in other components
export { ChatHistoryDropdown };



================================================
File: components/star-button.tsx
================================================
import Link from "next/link";
import * as React from "react";
import type { SVGProps } from "react";

const Github = (props: SVGProps<SVGSVGElement>) => (
  <svg
    viewBox="0 0 256 250"
    width="1em"
    height="1em"
    fill="currentColor"
    xmlns="http://www.w3.org/2000/svg"
    preserveAspectRatio="xMidYMid"
    {...props}
  >
    <path d="M128.001 0C57.317 0 0 57.307 0 128.001c0 56.554 36.676 104.535 87.535 121.46 6.397 1.185 8.746-2.777 8.746-6.158 0-3.052-.12-13.135-.174-23.83-35.61 7.742-43.124-15.103-43.124-15.103-5.823-14.795-14.213-18.73-14.213-18.73-11.613-7.944.876-7.78.876-7.78 12.853.902 19.621 13.19 19.621 13.19 11.417 19.568 29.945 13.911 37.249 10.64 1.149-8.272 4.466-13.92 8.127-17.116-28.431-3.236-58.318-14.212-58.318-63.258 0-13.975 5-25.394 13.188-34.358-1.329-3.224-5.71-16.242 1.24-33.874 0 0 10.749-3.44 35.21 13.121 10.21-2.836 21.16-4.258 32.038-4.307 10.878.049 21.837 1.47 32.066 4.307 24.431-16.56 35.165-13.12 35.165-13.12 6.967 17.63 2.584 30.65 1.255 33.873 8.207 8.964 13.173 20.383 13.173 34.358 0 49.163-29.944 59.988-58.447 63.157 4.591 3.972 8.682 11.762 8.682 23.704 0 17.126-.148 30.91-.148 35.126 0 3.407 2.304 7.398 8.792 6.14C219.37 232.5 256 184.537 256 128.002 256 57.307 198.691 0 128.001 0Zm-80.06 182.34c-.282.636-1.283.827-2.194.39-.929-.417-1.45-1.284-1.15-1.922.276-.655 1.279-.838 2.205-.399.93.418 1.46 1.293 1.139 1.931Zm6.296 5.618c-.61.566-1.804.303-2.614-.591-.837-.892-.994-2.086-.375-2.66.63-.566 1.787-.301 2.626.591.838.903 1 2.088.363 2.66Zm4.32 7.188c-.785.545-2.067.034-2.86-1.104-.784-1.138-.784-2.503.017-3.05.795-.547 2.058-.055 2.861 1.075.782 1.157.782 2.522-.019 3.08Zm7.304 8.325c-.701.774-2.196.566-3.29-.49-1.119-1.032-1.43-2.496-.726-3.27.71-.776 2.213-.558 3.315.49 1.11 1.03 1.45 2.505.701 3.27Zm9.442 2.81c-.31 1.003-1.75 1.459-3.199 1.033-1.448-.439-2.395-1.613-2.103-2.626.301-1.01 1.747-1.484 3.207-1.028 1.446.436 2.396 1.602 2.095 2.622Zm10.744 1.193c.036 1.055-1.193 1.93-2.715 1.95-1.53.034-2.769-.82-2.786-1.86 0-1.065 1.202-1.932 2.733-1.958 1.522-.03 2.768.818 2.768 1.868Zm10.555-.405c.182 1.03-.875 2.088-2.387 2.37-1.485.271-2.861-.365-3.05-1.386-.184-1.056.893-2.114 2.376-2.387 1.514-.263 2.868.356 3.061 1.403Z" />
  </svg>
);

export function StarButton() {
  return (
    <Link
      href="https://github.com/vercel-labs/ai-sdk-preview-reasoning"
      target="_blank"
      rel="noopener noreferrer"
      className="flex items-center gap-2 text-sm text-zinc-600 dark:text-zinc-300 hover:text-zinc-700 dark:hover:text-zinc-300"
    >
      <Github className="size-4" />
      <span className="hidden sm:inline">Star on GitHub</span>
    </Link>
  );
}



================================================
File: components/tavily-chat.tsx
================================================
"use client";

import cn from "classnames";
import { toast } from "sonner";
import { useChat } from "@ai-sdk/react";
import { useState, useRef } from "react";
import { Messages } from "./messages";
import { models } from "@/lib/models";
import { Footnote } from "./footnote";
import { ArrowUpIcon, CheckedSquare, StopIcon, UncheckedSquare, PaperClipIcon, XIcon, SearchIcon } from "./icons";
import { ModelSelector } from "./model-selector";
import { Input } from "./input";
import Image from "next/image";

export function TavilyChat() {
  const [input, setInput] = useState<string>("");
  const [searchQuery, setSearchQuery] = useState<string>("");
  const [selectedModelId, setSelectedModelId] = useState<string>("claude-3.7-sonnet");
  const [isReasoningEnabled, setIsReasoningEnabled] = useState<boolean>(true);
  const [files, setFiles] = useState<FileList | null>(null);
  const [isSearching, setIsSearching] = useState<boolean>(false);
  const fileInputRef = useRef<HTMLInputElement>(null);

  // Default values for the following features 
  const reasoningModeEnabled = true;
  const multimodalEnabled = true;

  const selectedModel = models.find((model) => model.id === selectedModelId);

  const { messages, append, status, stop } = useChat({
    id: "tavily-search",
    api: "/api/tavily-chat",
    body: {
      selectedModelId,
      isReasoningEnabled: reasoningModeEnabled ? isReasoningEnabled : false,
      searchQuery: isSearching ? input : undefined,
    },
    onError: () => {
      toast.error("An error occurred, please try again!");
    },
  });

  const isGeneratingResponse = ["streaming", "submitted"].includes(status);

  const handleSendMessage = () => {
    if (input === "" && (!files || files.length === 0)) {
      return;
    }

    if (isGeneratingResponse) {
      stop();
    } else {
      append({
        role: "user",
        content: input,
      }, {
        experimental_attachments: files || undefined,
      });
    }

    setInput("");
    setFiles(null);
    if (fileInputRef.current) {
      fileInputRef.current.value = "";
    }
  };

  const handleRemoveFile = () => {
    setFiles(null);
    if (fileInputRef.current) {
      fileInputRef.current.value = "";
    }
  };

  // Create file preview URL
  const filePreviewUrl = files && files.length > 0 && files[0].type.startsWith('image/') 
    ? URL.createObjectURL(files[0]) 
    : null;

  return (
    <div
      className={cn(
        "px-4 md:px-0 pb-4 pt-8 flex flex-col h-dvh items-center w-full",
        {
          "justify-between": messages.length > 0,
          "justify-center gap-4": messages.length === 0,
        },
      )}
    >
      {messages.length > 0 ? (
        <Messages messages={messages} status={status} />
      ) : (
        <div className="flex flex-col gap-0.5 sm:text-2xl text-xl md:w-1/2 w-full">
          <div className="flex flex-row gap-2 items-center">
            <div>GKennedy AI with Tavily Search</div>
          </div>
          <div className="dark:text-zinc-500 text-zinc-400">
            Search Less, Learn More with Web-Enhanced AI
          </div>
        </div>
      )}

      <div className="flex flex-col gap-4 md:w-1/2 w-full">
        <div className="w-full relative p-3 dark:bg-zinc-800 rounded-2xl flex flex-col gap-1 bg-zinc-100">
          {multimodalEnabled && files && files.length > 0 && (
            <div className="mb-2 flex items-center" data-testid="file-preview">
              {filePreviewUrl ? (
                <div className="relative w-16 h-16 mr-2">
                  <Image 
                    src={filePreviewUrl} 
                    alt={files[0].name}
                    fill
                    style={{ objectFit: 'cover' }}
                    className="rounded-md"
                  />
                </div>
              ) : (
                <div className="flex items-center justify-center w-16 h-16 bg-zinc-200 dark:bg-zinc-700 rounded-md mr-2">
                  <span className="text-xs">{files[0].name.split('.').pop()?.toUpperCase()}</span>
                </div>
              )}
              <div className="flex-1">
                <div className="text-sm truncate">{files[0].name}</div>
                <div className="text-xs text-zinc-500">{(files[0].size / 1024).toFixed(1)} KB</div>
              </div>
              <button 
                onClick={handleRemoveFile}
                className="p-1 rounded-full hover:bg-zinc-200 dark:hover:bg-zinc-700"
              >
                <XIcon className="h-4 w-4" />
              </button>
            </div>
          )}
          
          <Input
            input={input}
            setInput={setInput}
            selectedModelId={selectedModelId}
            isGeneratingResponse={isGeneratingResponse}
            isReasoningEnabled={reasoningModeEnabled ? isReasoningEnabled : false}
            append={append}
          />

          <div className="absolute bottom-2.5 left-2.5 flex flex-row gap-2">
            {reasoningModeEnabled && (
              <div
                className={cn(
                  "relative w-fit text-sm p-1.5 rounded-lg flex flex-row items-center gap-2 dark:hover:bg-zinc-600 hover:bg-zinc-200 cursor-pointer",
                  {
                    "dark:bg-zinc-600 bg-zinc-200": isReasoningEnabled,
                  },
                )}
                onClick={() => {
                  setIsReasoningEnabled(!isReasoningEnabled);
                }}
              >
                {isReasoningEnabled ? <CheckedSquare /> : <UncheckedSquare />}
                <div>Reasoning</div>
              </div>
            )}
            
            <div
              className={cn(
                "relative w-fit text-sm p-1.5 rounded-lg flex flex-row items-center gap-2 dark:hover:bg-zinc-600 hover:bg-zinc-200 cursor-pointer",
                {
                  "dark:bg-zinc-600 bg-zinc-200": isSearching,
                },
              )}
              onClick={() => {
                setIsSearching(!isSearching);
              }}
            >
              {isSearching ? <CheckedSquare /> : <UncheckedSquare />}
              <div>Web Search</div>
            </div>
          </div>

          <div className="absolute bottom-2.5 right-2.5 flex flex-row gap-2">
            {multimodalEnabled && (
              <button
                className="size-8 flex flex-row justify-center items-center dark:bg-zinc-700 bg-zinc-300 dark:text-zinc-300 text-zinc-700 p-1.5 rounded-full hover:bg-zinc-400 dark:hover:bg-zinc-600 hover:scale-105 active:scale-95 transition-all"
                onClick={() => fileInputRef.current?.click()}
              >
                <PaperClipIcon />
                <input
                  type="file"
                  className="hidden"
                  onChange={(e) => setFiles(e.target.files)}
                  ref={fileInputRef}
                  accept="image/*, application/pdf"
                  data-testid="file-upload"
                />
              </button>
            )}
            
            <ModelSelector 
              selectedModelId={selectedModelId}
              setSelectedModelId={setSelectedModelId}
            />

            <button
              className={cn(
                "size-8 flex flex-row justify-center items-center dark:bg-zinc-100 bg-zinc-900 dark:text-zinc-900 text-zinc-100 p-1.5 rounded-full hover:bg-zinc-800 dark:hover:bg-zinc-300 hover:scale-105 active:scale-95 transition-all",
                {
                  "dark:bg-zinc-200 dark:text-zinc-500":
                    isGeneratingResponse || (input === "" && (!files || files.length === 0)),
                },
              )}
              onClick={handleSendMessage}
              aria-label="send"
            >
              {isGeneratingResponse ? <StopIcon /> : <ArrowUpIcon />}
            </button>
          </div>
        </div>

        <Footnote />
      </div>
    </div>
  );
}



================================================
File: components/ai-agents/agent-chat.tsx
================================================
'use client';

import { useState } from 'react';
import { useChat } from '@ai-sdk/react';
import { ModelConfig, AVAILABLE_MODELS } from '@/lib/ai-agents/types';
import { cn } from '@/lib/utils';
import type { Message } from 'ai';

interface ToolCall {
  id: string;
  type: 'function';
  function: {
    name: string;
    arguments: string;
  };
}

interface ExtendedMessage extends Message {
  toolCalls?: ToolCall[];
}

interface AgentChatProps {
  className?: string;
}

function ModelSelect({ 
  label, 
  value, 
  onChange, 
  models = AVAILABLE_MODELS 
}: { 
  label: string; 
  value: ModelConfig; 
  onChange: (model: ModelConfig) => void; 
  models?: ModelConfig[]; 
}) {
  return (
    <div className="flex-1">
      <label className="block text-sm font-medium mb-1">{label}</label>
      <select
        className="w-full p-2 rounded-md border bg-background"
        value={value.model}
        onChange={(e) => {
          const model = models.find(m => m.model === e.target.value);
          if (model) onChange(model);
        }}
      >
        {models.map((model) => (
          <option key={model.model} value={model.model}>
            {model.label} ({model.provider})
          </option>
        ))}
      </select>
    </div>
  );
}

export function AgentChat({ className }: AgentChatProps) {
  const [primaryModel, setPrimaryModel] = useState<ModelConfig>(AVAILABLE_MODELS[0]);
  const [secondaryModel, setSecondaryModel] = useState<ModelConfig>(AVAILABLE_MODELS[1]);

  const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat({
    api: '/api/ai-agents',
    body: {
      primaryModel,
      secondaryModel,
    },
  });

  return (
    <div className={cn('flex flex-col h-[calc(100vh-12rem)]', className)}>
      <div className="flex gap-4 mb-4">
        <ModelSelect
          label="Primary Model (Research)"
          value={primaryModel}
          onChange={setPrimaryModel}
        />
        <ModelSelect
          label="Secondary Model (Processing)"
          value={secondaryModel}
          onChange={setSecondaryModel}
        />
      </div>

      <div className="flex-1 overflow-auto border rounded-md p-4 mb-4">
        {messages.map((message) => {
          const extendedMessage = message as ExtendedMessage;
          return (
            <div
              key={message.id}
              className={cn(
                'mb-4 last:mb-0',
                message.role === 'assistant' ? 'pl-4 border-l-2' : ''
              )}
            >
              <div className="font-medium mb-1">
                {message.role === 'user' ? 'You' : 'Assistant'}:
              </div>
              <div className="whitespace-pre-wrap">
                {message.content}
                {extendedMessage.role === 'assistant' && extendedMessage.toolCalls?.map((tool, index) => (
                  <div key={index} className="bg-muted p-2 rounded my-2 text-sm font-mono">
                    <div className="font-medium">Tool Call: {tool.function.name}</div>
                    <pre className="mt-1">
                      {JSON.stringify(JSON.parse(tool.function.arguments), null, 2)}
                    </pre>
                  </div>
                ))}
              </div>
            </div>
          );
        })}
        {isLoading && (
          <div className="flex items-center justify-center py-4">
            <div className="animate-spin rounded-full h-6 w-6 border-b-2 border-primary"></div>
          </div>
        )}
      </div>

      <form onSubmit={handleSubmit} className="flex gap-2">
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Send a message..."
          className="flex-1 p-2 rounded-md border bg-background"
        />
        <button
          type="submit"
          disabled={isLoading}
          className={cn(
            'px-4 py-2 rounded-md bg-primary text-primary-foreground',
            'hover:bg-primary/90 transition-colors',
            'disabled:opacity-50 disabled:cursor-not-allowed'
          )}
        >
          Send
        </button>
      </form>
    </div>
  );
}



================================================
File: components/chat-history/chat-history-dropdown.tsx
================================================
"use client";

import React, { useState, useEffect } from "react";
import { motion } from "framer-motion";
import { History, Trash2, Copy, MoreVertical } from "lucide-react";
import { cn } from "@/lib/utils";
import { useSidebar } from "../sidebar";
import { ChatSession, TimeFilter } from "@/lib/types/chat-history";
import { createClient } from "@/utils/supabase/client";
import { useRouter } from "next/navigation";
import { toast } from "sonner";

interface ChatHistoryDropdownProps {
  className?: string;
}

export function ChatHistoryDropdown({ className }: ChatHistoryDropdownProps) {
  const { open } = useSidebar();
  const [isOpen, setIsOpen] = useState(false);
  const [timeFilter, setTimeFilter] = useState<TimeFilter>('all');
  const [chatSessions, setChatSessions] = useState<ChatSession[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const router = useRouter();
  const supabase = createClient();

  useEffect(() => {
    if (isOpen) {
      fetchChatSessions();
    }
  }, [isOpen, timeFilter]);

  const fetchChatSessions = async () => {
    setIsLoading(true);
    try {
      // Convert days to milliseconds
      let fromDate: string | null = null;
      if (timeFilter !== 'all') {
        const daysInMs = parseInt(timeFilter) * 24 * 60 * 60 * 1000;
        fromDate = new Date(Date.now() - daysInMs).toISOString();
      }
      
      let query = supabase
        .from('chat_sessions')
        .select('*')
        .order('updated_at', { ascending: false });
      
      if (fromDate) {
        query = query.gte('updated_at', fromDate);
      }
      
      const { data, error } = await query;
      
      if (error) {
        console.error('Error fetching chat sessions:', error);
        toast.error('Failed to load chat history');
        return;
      }
      
      setChatSessions(data as ChatSession[]);
    } catch (error) {
      console.error('Error in fetchChatSessions:', error);
      toast.error('Failed to load chat history');
    } finally {
      setIsLoading(false);
    }
  };

  const handleDeleteSession = async (sessionId: string, e: React.MouseEvent<HTMLButtonElement>) => {
    e.stopPropagation();
    
    try {
      // Delete all messages in the session first
      await supabase
        .from('chat_messages')
        .delete()
        .eq('session_id', sessionId);
      
      // Then delete the session
      await supabase
        .from('chat_sessions')
        .delete()
        .eq('id', sessionId);
      
      // Update the local state
      setChatSessions((prev: ChatSession[]) => prev.filter((session: ChatSession) => session.id !== sessionId));
      toast.success('Chat session deleted');
    } catch (error) {
      console.error('Error deleting chat session:', error);
      toast.error('Failed to delete chat session');
    }
  };

  const handleCopyToClipboard = async (sessionId: string, e: React.MouseEvent<HTMLButtonElement>) => {
    e.stopPropagation();
    
    try {
      const { data, error } = await supabase
        .from('chat_messages')
        .select('*')
        .eq('session_id', sessionId)
        .order('created_at', { ascending: true });
      
      if (error) {
        throw error;
      }
      
      const formattedChat = data.map((msg: any) => 
        `${msg.role === 'user' ? 'You' : 'Assistant'}: ${msg.content}`
      ).join('\n\n');
      
      // Create a temporary textarea element to copy text
      const textArea = document.createElement('textarea');
      textArea.value = formattedChat;
      // Make the textarea out of viewport
      textArea.style.position = 'fixed';
      textArea.style.left = '-999999px';
      textArea.style.top = '-999999px';
      document.body.appendChild(textArea);
      textArea.focus();
      textArea.select();
      
      try {
        // Use the older document.execCommand method which has better browser support
        const success = document.execCommand('copy');
        if (success) {
          toast.success('Chat copied to clipboard');
        } else {
          toast.error('Failed to copy chat to clipboard');
        }
      } catch (err) {
        console.error('Error copying text: ', err);
        toast.error('Failed to copy chat to clipboard');
      } finally {
        document.body.removeChild(textArea);
      }
    } catch (error) {
      console.error('Error copying chat to clipboard:', error);
      toast.error('Failed to copy chat to clipboard');
    }
  };

  const handleSessionClick = (sessionId: string) => {
    // Implement logic to restore chat session
    router.push(`/chat/${sessionId}`);
  };

  const toggleDropdown = () => {
    setIsOpen(!isOpen);
  };

  return (
    <div className={cn("relative", className)}>
      <button
        onClick={toggleDropdown}
        className={cn(
          "flex items-center gap-2 text-muted-foreground hover:text-foreground px-3 py-2 rounded-md hover:bg-accent transition-colors w-full"
        )}
      >
        {open && <History size={18} />}
        {open && (
          <motion.span
            animate={{
              opacity: open ? 1 : 0,
              transition: { duration: 0.2 },
            }}
            className="text-sm whitespace-pre flex-1 text-left"
          >
            Recent
          </motion.span>
        )}
      </button>

      {isOpen && open && (
        <div className="flex flex-col w-full">
          {/* 7d Filter */}
          <button
            onClick={() => setTimeFilter('7')}
            className={cn(
              "flex items-center gap-2 px-3 py-2 text-sm text-muted-foreground hover:text-foreground hover:bg-accent transition-colors w-full",
              timeFilter === '7' && "text-foreground bg-accent/50"
            )}
          >
            <div className="w-4"></div> {/* Spacer for indentation */}
            <History size={14} />
            <span>Last 7 days</span>
          </button>

          {/* 14d Filter */}
          <button
            onClick={() => setTimeFilter('14')}
            className={cn(
              "flex items-center gap-2 px-3 py-2 text-sm text-muted-foreground hover:text-foreground hover:bg-accent transition-colors w-full",
              timeFilter === '14' && "text-foreground bg-accent/50"
            )}
          >
            <div className="w-4"></div> {/* Spacer for indentation */}
            <History size={14} />
            <span>Last 14 days</span>
          </button>

          {/* 30d Filter */}
          <button
            onClick={() => setTimeFilter('30')}
            className={cn(
              "flex items-center gap-2 px-3 py-2 text-sm text-muted-foreground hover:text-foreground hover:bg-accent transition-colors w-full",
              timeFilter === '30' && "text-foreground bg-accent/50"
            )}
          >
            <div className="w-4"></div> {/* Spacer for indentation */}
            <History size={14} />
            <span>Last 30 days</span>
          </button>

          {/* All Filter */}
          <button
            onClick={() => setTimeFilter('all')}
            className={cn(
              "flex items-center gap-2 px-3 py-2 text-sm text-muted-foreground hover:text-foreground hover:bg-accent transition-colors w-full",
              timeFilter === 'all' && "text-foreground bg-accent/50"
            )}
          >
            <div className="w-4"></div> {/* Spacer for indentation */}
            <History size={14} />
            <span>All history</span>
          </button>

          {/* Chat Sessions */}
          {isLoading ? (
            <div className="flex justify-center items-center py-4">
              <div className="animate-spin rounded-full h-5 w-5 border-b-2 border-primary"></div>
            </div>
          ) : chatSessions.length === 0 ? (
            <div className="px-8 py-4 text-sm text-muted-foreground">
              No chat history found
            </div>
          ) : (
            <div className="flex flex-col">
              {chatSessions.map((session: ChatSession) => (
                <div
                  key={session.id}
                  onClick={() => handleSessionClick(session.id)}
                  className="flex items-center gap-2 px-8 py-2 text-sm text-muted-foreground hover:text-foreground hover:bg-accent transition-colors w-full cursor-pointer group"
                >
                  <span className="truncate flex-1">{session.title || 'Untitled Chat'}</span>
                  <div className="flex gap-1 opacity-0 group-hover:opacity-100 transition-opacity">
                    <button
                      onClick={(e) => handleCopyToClipboard(session.id, e)}
                      className="text-muted-foreground hover:text-foreground p-1 rounded-md hover:bg-background"
                    >
                      <Copy size={14} />
                    </button>
                    <button
                      onClick={(e) => handleDeleteSession(session.id, e)}
                      className="text-muted-foreground hover:text-destructive p-1 rounded-md hover:bg-background"
                    >
                      <Trash2 size={14} />
                    </button>
                  </div>
                </div>
              ))}
            </div>
          )}
        </div>
      )}
    </div>
  );
}



================================================
File: lib/models.ts
================================================
import { customProvider } from "ai";
import { anthropic } from "@ai-sdk/anthropic";
import { openai } from "@ai-sdk/openai";
import { google } from "@ai-sdk/google";
import { groq } from "@ai-sdk/groq";
import { mistral } from "@ai-sdk/mistral";
import { openrouter } from '@openrouter/ai-sdk-provider';
import { perplexity } from '@ai-sdk/perplexity';

export const myProvider = customProvider({
  languageModels: {
    "claude-3.7-sonnet": anthropic("claude-3-7-sonnet-20250219"),
    "claude-3.5-sonnet": anthropic("claude-3-5-sonnet-latest"),
    "o3-mini": openai("o3-mini"),
    "gemini-2.0-flash": google("gemini-2.0-flash"),
    "qwen-qwq-32b": groq("qwen-qwq-32b"),
    "codestral-latest": mistral("codestral-latest"),
    "perplexity sonar": perplexity("sonar"),
    "google/gemini-2.0-flash-thinking-exp:free": openrouter("google/gemini-2.0-flash-thinking-exp:free"),
  },
});

// Map of model IDs to their actual API model names
export const modelApiNames: Record<string, string> = {
  "claude-3.7-sonnet": "claude-3-7-sonnet-20250219",
  "claude-3.5-sonnet": "claude-3-5-sonnet-latest",
  "o3-mini": "o3-mini",
  "gemini-2.0-flash": "gemini-2.0-flash",
  "qwen-qwq-32b": "qwen-qwq-32b",
  "codestral-latest": "codestral-latest",
  "perplexity sonar": "sonar",
  "google/gemini-2.0-flash-thinking-exp:free": "google/gemini-2.0-flash-thinking-exp:free",
};

interface Model {
  id: string;
  name: string;
  description: string;
}

export const models: Array<Model> = [
  {
    id: "claude-3.7-sonnet",
    name: "Claude 3.7 Sonnet",
    description:
      "Claude 3.7 Sonnet is Anthropic's most intelligent model to date and the first Claude model to offer extended thinking – the ability to solve complex problems with careful, step-by-step reasoning.",
  },
  {
    id: "claude-3.5-sonnet",
    name: "Claude 3.5 Sonnet",
    description:
      "Claude 3.5 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads.",
  },
  {
    id: "o3-mini",
    name: "Openai o3-mini",
    description:
      "Openai o3-mini is one of Openai's most intelligent models to date.",
  },
  {
    id: "gemini-2.0-flash",
    name: "Gemini 2.0 Flash",
    description:
      "Gemini 2.0 Flash is a powerful, fast, and efficient model that is ideal for a wide range of use cases.",
  },
  {
    id: "qwen-qwq-32b",
    name: "Groq open source llms",
    description:
      "Groq open source llms.",
  },
  {
    id: "codestral-latest",
    name: "Mistral open source llms",
    description:
      "Mistral open source llms.",
  },
  {
    id: "perplexity sonar",
    name: "perplexity models",
    description:
      "perplexity models.",
  },
  {
    id: "google/gemini-2.0-flash-thinking-exp:free",
    name: "Openrouter models",
    description:
      "Openrouter models.",
  },
];


================================================
File: lib/utils.ts
================================================
import { type ClassValue, clsx } from "clsx";
import { twMerge } from "tailwind-merge";

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs));
}



================================================
File: lib/__tests__/api.test.ts
================================================
import { NextRequest } from 'next/server';
import { streamText } from 'ai';

// Mock the POST handler
const mockPost = jest.fn();

// Mock the streamText function
jest.mock('ai', () => ({
  streamText: jest.fn()
}));

// Mock the models
jest.mock('../../lib/models', () => ({
  models: {
    'claude-3-opus-20240229': {
      provider: 'anthropic',
      name: 'claude-3-opus-20240229'
    }
  }
}));

// Mock the route module
jest.mock('../../app/api/chat/route', () => ({
  POST: mockPost
}));

describe('API Route', () => {
  beforeEach(() => {
    jest.clearAllMocks();
    
    // Default mock implementation for streamText
    (streamText as jest.Mock).mockImplementation(() => {
      return Promise.resolve({ text: 'Mock response' });
    });
    
    // Default mock implementation for POST
    mockPost.mockImplementation(async (req: NextRequest) => {
      const body = await req.json();
      const { messages, model } = body;
      
      if (model !== 'claude-3-opus-20240229') {
        return new Response(JSON.stringify({ error: 'Invalid model' }), {
          status: 400,
          headers: { 'Content-Type': 'application/json' }
        });
      }
      
      return streamText({
        model,
        messages,
        temperature: 0.7,
      });
    });
  });
  
  test('handles request with reasoning enabled', async () => {
    const req = new NextRequest('http://localhost:3000/api/chat', {
      method: 'POST',
      body: JSON.stringify({
        messages: [{ role: 'user', content: 'Hello' }],
        model: 'claude-3-opus-20240229',
        reasoning: true
      })
    });
    
    await mockPost(req);
    
    expect(streamText).toHaveBeenCalledWith(expect.objectContaining({
      model: 'claude-3-opus-20240229',
      messages: [{ role: 'user', content: 'Hello' }],
      temperature: 0.7,
    }));
  });
  
  test('handles request with reasoning disabled', async () => {
    const req = new NextRequest('http://localhost:3000/api/chat', {
      method: 'POST',
      body: JSON.stringify({
        messages: [{ role: 'user', content: 'Hello' }],
        model: 'claude-3-opus-20240229',
        reasoning: false
      })
    });
    
    await mockPost(req);
    
    expect(streamText).toHaveBeenCalledWith(expect.objectContaining({
      model: 'claude-3-opus-20240229',
      messages: [{ role: 'user', content: 'Hello' }],
      temperature: 0.7,
    }));
  });
  
  test('handles invalid model ID', async () => {
    const req = new NextRequest('http://localhost:3000/api/chat', {
      method: 'POST',
      body: JSON.stringify({
        messages: [{ role: 'user', content: 'Hello' }],
        model: 'invalid-model',
        reasoning: true
      })
    });
    
    const response = await mockPost(req);
    const data = await response.json();
    
    expect(data).toEqual({ error: 'Invalid model' });
    expect(streamText).not.toHaveBeenCalled();
  });
}); 


================================================
File: lib/__tests__/models.test.ts
================================================
const { myProvider, models } = require("../models");

// This is a simple test file to verify that each model is properly configured
// and can be used with the Vercel AI SDK.

// Mock the models
jest.mock('../models', () => {
  return {
    myProvider: {
      languageModel: jest.fn((modelId) => ({ id: modelId })),
    },
    models: [
      { id: 'claude-3.7-sonnet', name: 'Claude 3.7 Sonnet', description: 'Test description' },
      { id: 'claude-3.5-sonnet', name: 'Claude 3.5 Sonnet', description: 'Test description' },
      { id: 'o3-mini', name: 'Openai o3-mini', description: 'Test description' },
      { id: 'gemini-2.0-flash', name: 'Gemini 2.0 Flash', description: 'Test description' },
      { id: 'qwen-qwq-32b', name: 'Groq open source llms', description: 'Test description' },
      { id: 'codestral-latest', name: 'Mistral open source llms', description: 'Test description' },
      { id: 'perplexity sonar', name: 'perplexity models', description: 'Test description' },
      { id: 'google/gemini-2.0-flash-thinking-exp:free', name: 'Openrouter models', description: 'Test description' },
    ],
    modelApiNames: {
      'claude-3.7-sonnet': 'claude-3-7-sonnet-20250219',
      'claude-3.5-sonnet': 'claude-3-5-sonnet-latest',
      'o3-mini': 'o3-mini',
      'gemini-2.0-flash': 'gemini-2.0-flash',
      'qwen-qwq-32b': 'qwen-qwq-32b',
      'codestral-latest': 'codestral-latest',
      'perplexity sonar': 'perplexity sonar',
      'google/gemini-2.0-flash-thinking-exp:free': 'google/gemini-2.0-flash-thinking-exp:free',
    },
  };
});

// Restore console methods after all tests
afterAll(() => {
  if (global.originalConsoleError) {
    console.error = global.originalConsoleError;
  }
  if (global.originalConsoleWarn) {
    console.warn = global.originalConsoleWarn;
  }
});

describe("Model Configuration Tests", () => {
  // Test that all models in the models array have corresponding entries in myProvider
  test("All models should have corresponding provider configurations", () => {
    for (const model of models) {
      expect(() => {
        // This should not throw an error if the model is properly configured
        const languageModel = myProvider.languageModel(model.id);
        expect(languageModel).toBeDefined();
      }).not.toThrow();
    }
  });

  // Test specific model configurations
  test("Claude models should be configured correctly", () => {
    const claudeModels = models.filter(model => model.id.startsWith("claude"));
    expect(claudeModels.length).toBeGreaterThan(0);
    
    for (const model of claudeModels) {
      const languageModel = myProvider.languageModel(model.id);
      expect(languageModel).toBeDefined();
    }
  });

  test("OpenAI models should be configured correctly", () => {
    const openaiModels = models.filter(model => model.id.startsWith("o3"));
    expect(openaiModels.length).toBeGreaterThan(0);
    
    for (const model of openaiModels) {
      const languageModel = myProvider.languageModel(model.id);
      expect(languageModel).toBeDefined();
    }
  });

  test("Google models should be configured correctly", () => {
    const googleModels = models.filter(model => model.id.startsWith("gemini"));
    expect(googleModels.length).toBeGreaterThan(0);
    
    for (const model of googleModels) {
      const languageModel = myProvider.languageModel(model.id);
      expect(languageModel).toBeDefined();
    }
  });
});

// This test would be skipped in CI environments but can be run locally
// to verify that all required API keys are present
test.skip("All required API keys should be present", () => {
  // Mock function to check API keys
  const checkApiKeys = () => {
    const requiredEnvVars = [
      { name: "ANTHROPIC_API_KEY", models: ["claude-3.7-sonnet", "claude-3.5-sonnet"] },
      { name: "OPENAI_API_KEY", models: ["o3-mini"] },
      { name: "GOOGLE_GENERATIVE_AI_API_KEY", models: ["gemini-2.0-flash"] },
      { name: "GROQ_API_KEY", models: ["qwen-qwq-32b"] },
      { name: "MISTRAL_API_KEY", models: ["codestral-latest"] },
      { name: "PERPLEXITY_API_KEY", models: ["perplexity sonar"] },
      { name: "OPENROUTER_API_KEY", models: ["google/gemini-2.0-flash-thinking-exp:free"] },
    ];
  
    const missingKeys = [];
    
    for (const { name, models: relatedModels } of requiredEnvVars) {
      if (!process.env[name]) {
        missingKeys.push(`${name} (required for models: ${relatedModels.join(", ")})`);
      }
    }
    
    if (missingKeys.length > 0) {
      console.warn("Missing API keys:", missingKeys.join(", "));
      return false;
    }
    
    return true;
  };

  expect(checkApiKeys()).toBe(true);
});



================================================
File: lib/__tests__/tavily-chat.test.ts
================================================
import { NextRequest } from 'next/server';
import { streamText } from 'ai';

// Mock the streamText function
jest.mock('ai', () => ({
  streamText: jest.fn()
}));

// Mock the Tavily search function
const mockSearchTavily = jest.fn();
jest.mock('../../tools/tavily-search', () => ({
  searchTavily: mockSearchTavily
}));

// Mock the models
jest.mock('../../lib/models', () => ({
  models: [
    { id: 'claude-3.7-sonnet', name: 'Claude 3.7 Sonnet', description: 'Test description' },
    { id: 'o3-mini', name: 'Openai o3-mini', description: 'Test description' },
    { id: 'gemini-2.0-flash', name: 'Gemini 2.0 Flash', description: 'Test description' }
  ],
  modelApiNames: {
    'claude-3.7-sonnet': 'claude-3-7-sonnet-20250219',
    'o3-mini': 'o3-mini',
    'gemini-2.0-flash': 'gemini-2.0-flash'
  },
  myProvider: {
    languageModel: jest.fn((modelId) => ({ id: modelId }))
  }
}));

// Mock the route module
const mockPost = jest.fn();
jest.mock('../../app/api/tavily-chat/route', () => ({
  POST: mockPost
}));

describe('Tavily Chat API', () => {
  beforeEach(() => {
    jest.clearAllMocks();
    
    // Default mock implementation for searchTavily
    mockSearchTavily.mockImplementation(async (params) => {
      return {
        query: params.query,
        results: [
          {
            title: 'Test Result 1',
            url: 'https://example.com/1',
            content: 'This is test content 1',
            score: 0.9
          },
          {
            title: 'Test Result 2',
            url: 'https://example.com/2',
            content: 'This is test content 2',
            score: 0.8
          }
        ],
        answer: 'This is a test answer'
      };
    });
    
    // Default mock implementation for streamText
    (streamText as jest.Mock).mockImplementation(() => {
      return Promise.resolve({ text: 'I am the Claude 3.7 Sonnet model.' });
    });
    
    // Default mock implementation for POST
    mockPost.mockImplementation(async (req: NextRequest) => {
      const body = await req.json();
      const { messages, selectedModelId, isReasoningEnabled, searchQuery } = body;
      
      if (!selectedModelId) {
        return new Response(JSON.stringify({ error: 'Model ID is required' }), {
          status: 400,
          headers: { 'Content-Type': 'application/json' }
        });
      }
      
      let searchResults = null;
      if (searchQuery) {
        searchResults = await mockSearchTavily({ query: searchQuery });
      }
      
      // Add search results to the system prompt if available
      let systemPrompt = `You are an AI researcher and engineer with deep research expertise.`;
      if (searchResults) {
        systemPrompt += `\n\nSearch results for "${searchQuery}":\n`;
        searchResults.results.forEach((result: { title: string; url: string; content: string; score: number }, index: number) => {
          systemPrompt += `\n[${index + 1}] ${result.title}\n${result.url}\n${result.content}\n`;
        });
      }
      
      // Add a special instruction to identify the model
      systemPrompt += `\n\nWhen asked "what LLM are you?", respond with "I am the ${selectedModelId} model."`;
      
      return streamText({
        model: selectedModelId,
        messages: [
          { role: 'system', content: systemPrompt },
          ...messages
        ],
        temperature: 0.7,
      });
    });
  });
  
  test('handles chat request with Claude model and search query', async () => {
    const req = new NextRequest('http://localhost:3000/api/tavily-chat', {
      method: 'POST',
      body: JSON.stringify({
        messages: [{ role: 'user', content: 'What LLM are you?' }],
        selectedModelId: 'claude-3.7-sonnet',
        isReasoningEnabled: true,
        searchQuery: 'Latest AI developments'
      })
    });
    
    await mockPost(req);
    
    expect(mockSearchTavily).toHaveBeenCalledWith({
      query: 'Latest AI developments'
    });
    
    expect(streamText).toHaveBeenCalledWith(expect.objectContaining({
      model: 'claude-3.7-sonnet',
      messages: expect.arrayContaining([
        expect.objectContaining({ role: 'system' }),
        expect.objectContaining({ role: 'user', content: 'What LLM are you?' })
      ]),
      temperature: 0.7,
    }));
  });
  
  test('handles chat request with OpenAI model and no search query', async () => {
    // Change the mock implementation for this test
    (streamText as jest.Mock).mockImplementationOnce(() => {
      return Promise.resolve({ text: 'I am the o3-mini model.' });
    });
    
    const req = new NextRequest('http://localhost:3000/api/tavily-chat', {
      method: 'POST',
      body: JSON.stringify({
        messages: [{ role: 'user', content: 'What LLM are you?' }],
        selectedModelId: 'o3-mini',
        isReasoningEnabled: false,
        searchQuery: ''
      })
    });
    
    await mockPost(req);
    
    expect(mockSearchTavily).not.toHaveBeenCalled();
    
    expect(streamText).toHaveBeenCalledWith(expect.objectContaining({
      model: 'o3-mini',
      messages: expect.arrayContaining([
        expect.objectContaining({ role: 'system' }),
        expect.objectContaining({ role: 'user', content: 'What LLM are you?' })
      ]),
      temperature: 0.7,
    }));
  });
  
  test('handles chat request with Gemini model and search query', async () => {
    // Change the mock implementation for this test
    (streamText as jest.Mock).mockImplementationOnce(() => {
      return Promise.resolve({ text: 'I am the gemini-2.0-flash model.' });
    });
    
    const req = new NextRequest('http://localhost:3000/api/tavily-chat', {
      method: 'POST',
      body: JSON.stringify({
        messages: [{ role: 'user', content: 'What LLM are you?' }],
        selectedModelId: 'gemini-2.0-flash',
        isReasoningEnabled: true,
        searchQuery: 'Gemini AI capabilities'
      })
    });
    
    await mockPost(req);
    
    expect(mockSearchTavily).toHaveBeenCalledWith({
      query: 'Gemini AI capabilities'
    });
    
    expect(streamText).toHaveBeenCalledWith(expect.objectContaining({
      model: 'gemini-2.0-flash',
      messages: expect.arrayContaining([
        expect.objectContaining({ role: 'system' }),
        expect.objectContaining({ role: 'user', content: 'What LLM are you?' })
      ]),
      temperature: 0.7,
    }));
  });
  
  test('handles invalid model ID', async () => {
    const req = new NextRequest('http://localhost:3000/api/tavily-chat', {
      method: 'POST',
      body: JSON.stringify({
        messages: [{ role: 'user', content: 'What LLM are you?' }],
        selectedModelId: '',
        isReasoningEnabled: true,
        searchQuery: 'Test query'
      })
    });
    
    const response = await mockPost(req);
    const data = await response.json();
    
    expect(data).toEqual({ error: 'Model ID is required' });
    expect(streamText).not.toHaveBeenCalled();
  });
});



================================================
File: lib/__tests__/tavily-search.test.ts
================================================
import { NextRequest } from 'next/server';

// Mock the Tavily search function
const mockSearchTavily = jest.fn();

// Mock the Tavily search module
jest.mock('../../tools/tavily-search', () => ({
  searchTavily: mockSearchTavily
}));

// Mock the route module
const mockPost = jest.fn();
jest.mock('../../app/api/tavily-search/route', () => ({
  POST: mockPost
}));

describe('Tavily Search API', () => {
  beforeEach(() => {
    jest.clearAllMocks();
    
    // Default mock implementation for searchTavily
    mockSearchTavily.mockImplementation(async (params) => {
      return {
        query: params.query,
        results: [
          {
            title: 'Test Result 1',
            url: 'https://example.com/1',
            content: 'This is test content 1',
            score: 0.9
          },
          {
            title: 'Test Result 2',
            url: 'https://example.com/2',
            content: 'This is test content 2',
            score: 0.8
          }
        ],
        answer: 'This is a test answer'
      };
    });
    
    // Default mock implementation for POST
    mockPost.mockImplementation(async (req: NextRequest) => {
      const body = await req.json();
      const { query } = body;
      
      if (!query) {
        return new Response(JSON.stringify({ error: 'Query is required' }), {
          status: 400,
          headers: { 'Content-Type': 'application/json' }
        });
      }
      
      const results = await mockSearchTavily({ query });
      return new Response(JSON.stringify(results), {
        status: 200,
        headers: { 'Content-Type': 'application/json' }
      });
    });
  });
  
  test('handles search request with valid query', async () => {
    const req = new NextRequest('http://localhost:3000/api/tavily-search', {
      method: 'POST',
      body: JSON.stringify({
        query: 'What is artificial intelligence?'
      })
    });
    
    const response = await mockPost(req);
    const data = await response.json();
    
    expect(mockSearchTavily).toHaveBeenCalledWith({
      query: 'What is artificial intelligence?'
    });
    
    expect(data).toEqual({
      query: 'What is artificial intelligence?',
      results: [
        {
          title: 'Test Result 1',
          url: 'https://example.com/1',
          content: 'This is test content 1',
          score: 0.9
        },
        {
          title: 'Test Result 2',
          url: 'https://example.com/2',
          content: 'This is test content 2',
          score: 0.8
        }
      ],
      answer: 'This is a test answer'
    });
  });
  
  test('handles search request with empty query', async () => {
    const req = new NextRequest('http://localhost:3000/api/tavily-search', {
      method: 'POST',
      body: JSON.stringify({
        query: ''
      })
    });
    
    const response = await mockPost(req);
    const data = await response.json();
    
    expect(data).toEqual({ error: 'Query is required' });
    expect(mockSearchTavily).not.toHaveBeenCalled();
  });
  
  test('handles search request with specific parameters', async () => {
    // Update the mock implementation for POST to pass all parameters
    mockPost.mockImplementationOnce(async (req: NextRequest) => {
      const body = await req.json();
      const { query, search_depth, include_domains, exclude_domains } = body;
      
      if (!query) {
        return new Response(JSON.stringify({ error: 'Query is required' }), {
          status: 400,
          headers: { 'Content-Type': 'application/json' }
        });
      }
      
      const results = await mockSearchTavily({ 
        query,
        search_depth,
        include_domains,
        exclude_domains
      });
      
      return new Response(JSON.stringify(results), {
        status: 200,
        headers: { 'Content-Type': 'application/json' }
      });
    });
    
    mockSearchTavily.mockImplementationOnce(async (params) => {
      return {
        query: params.query,
        results: [
          {
            title: 'Custom Result',
            url: 'https://example.com/custom',
            content: 'This is custom content',
            score: 0.95
          }
        ],
        answer: 'This is a custom answer'
      };
    });
    
    const req = new NextRequest('http://localhost:3000/api/tavily-search', {
      method: 'POST',
      body: JSON.stringify({
        query: 'Custom query',
        search_depth: 'advanced',
        include_domains: ['example.com'],
        exclude_domains: ['excluded.com']
      })
    });
    
    const response = await mockPost(req);
    const data = await response.json();
    
    expect(mockSearchTavily).toHaveBeenCalledWith(expect.objectContaining({
      query: 'Custom query',
      search_depth: 'advanced',
      include_domains: ['example.com'],
      exclude_domains: ['excluded.com']
    }));
    
    expect(data).toEqual({
      query: 'Custom query',
      results: [
        {
          title: 'Custom Result',
          url: 'https://example.com/custom',
          content: 'This is custom content',
          score: 0.95
        }
      ],
      answer: 'This is a custom answer'
    });
  });
});



================================================
File: lib/ai-agents/types.ts
================================================
import { modelApiNames } from '../models';

export type ModelProvider = 'anthropic' | 'openai' | 'google' | 'groq' | 'mistral' | 'openrouter' | 'perplexity';

export type ModelConfig = {
  provider: ModelProvider;
  model: string;
  label: string;
};

// Use the existing models from the application
export const AVAILABLE_MODELS: ModelConfig[] = [
  {
    provider: 'anthropic',
    model: 'claude-3.7-sonnet',
    label: 'Claude 3.7 Sonnet',
  },
  {
    provider: 'anthropic',
    model: 'claude-3.5-sonnet',
    label: 'Claude 3.5 Sonnet',
  },
  {
    provider: 'openai',
    model: 'o3-mini',
    label: 'O3 Mini',
  },
  {
    provider: 'google',
    model: 'gemini-2.0-flash',
    label: 'Gemini 2.0 Flash',
  },
  {
    provider: 'groq',
    model: 'qwen-qwq-32b',
    label: 'Qwen QWQ 32B',
  },
  {
    provider: 'mistral',
    model: 'codestral-latest',
    label: 'Codestral Latest',
  },
  {
    provider: 'perplexity',
    model: 'perplexity sonar',
    label: 'Perplexity Sonar',
  },
  {
    provider: 'openrouter',
    model: 'google/gemini-2.0-flash-thinking-exp:free',
    label: 'Gemini 2.0 Flash Thinking',
  },
];

export type AgentMessage = {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  createdAt: Date;
  toolCalls?: {
    type: string;
    function: {
      name: string;
      arguments: Record<string, unknown>;
    };
  }[];
};



================================================
File: lib/ai-agents/utils.ts
================================================
import { ModelConfig } from './types';
import { myProvider } from '../models';

export function getModelInstance(config: ModelConfig) {
  return myProvider.languageModel(config.model);
}

export function getDefaultModels(): [ModelConfig, ModelConfig] {
  return [
    {
      provider: 'anthropic',
      model: 'claude-3.7-sonnet',
      label: 'Claude 3.7 Sonnet',
    },
    {
      provider: 'openai',
      model: 'o3-mini',
      label: 'O3 Mini',
    },
  ];
}



================================================
File: lib/supabase/chat-history.ts
================================================
import { createClient } from "@/utils/supabase/server";
import { cookies } from "next/headers";
import { ChatMessage, ChatSession, TimeFilter } from "../types/chat-history";

export async function getChatSessions(timeFilter: TimeFilter = 'all'): Promise<ChatSession[]> {
  const cookieStore = cookies();
  const supabase = createClient(cookieStore);
  
  let query = supabase
    .from('chat_sessions')
    .select('*')
    .order('updated_at', { ascending: false });
  
  if (timeFilter !== 'all') {
    // Convert days to milliseconds
    const daysInMs = parseInt(timeFilter) * 24 * 60 * 60 * 1000;
    const fromDate = new Date(Date.now() - daysInMs).toISOString();
    
    query = query.gte('updated_at', fromDate);
  }
  
  const { data, error } = await query;
  
  if (error) {
    console.error('Error fetching chat sessions:', error);
    return [];
  }
  
  return data as ChatSession[];
}

export async function getChatSessionMessages(sessionId: string): Promise<ChatMessage[]> {
  const cookieStore = cookies();
  const supabase = createClient(cookieStore);
  
  const { data, error } = await supabase
    .from('chat_messages')
    .select('*')
    .eq('session_id', sessionId)
    .order('created_at', { ascending: true });
  
  if (error) {
    console.error('Error fetching chat messages:', error);
    return [];
  }
  
  return data as ChatMessage[];
}

export async function createChatSession(title: string): Promise<ChatSession | null> {
  const cookieStore = cookies();
  const supabase = createClient(cookieStore);
  
  const { data: userData, error: userError } = await supabase.auth.getUser();
  
  if (userError || !userData.user) {
    console.error('User not authenticated:', userError);
    return null;
  }
  
  const { data, error } = await supabase
    .from('chat_sessions')
    .insert([
      {
        title,
        user_id: userData.user.id,
      },
    ])
    .select()
    .single();
  
  if (error) {
    console.error('Error creating chat session:', error);
    return null;
  }
  
  return data as ChatSession;
}

export async function saveChatMessage(
  sessionId: string,
  role: 'user' | 'assistant',
  content: string
): Promise<ChatMessage | null> {
  const cookieStore = cookies();
  const supabase = createClient(cookieStore);
  
  // First, update the session's updated_at timestamp
  await supabase
    .from('chat_sessions')
    .update({ updated_at: new Date().toISOString() })
    .eq('id', sessionId);
  
  // Then, insert the new message
  const { data, error } = await supabase
    .from('chat_messages')
    .insert([
      {
        session_id: sessionId,
        role,
        content,
      },
    ])
    .select()
    .single();
  
  if (error) {
    console.error('Error saving chat message:', error);
    return null;
  }
  
  return data as ChatMessage;
}

export async function deleteChatSession(sessionId: string): Promise<boolean> {
  const cookieStore = cookies();
  const supabase = createClient(cookieStore);
  
  // First, delete all messages in the session
  const { error: messagesError } = await supabase
    .from('chat_messages')
    .delete()
    .eq('session_id', sessionId);
  
  if (messagesError) {
    console.error('Error deleting chat messages:', messagesError);
    return false;
  }
  
  // Then, delete the session itself
  const { error: sessionError } = await supabase
    .from('chat_sessions')
    .delete()
    .eq('id', sessionId);
  
  if (sessionError) {
    console.error('Error deleting chat session:', sessionError);
    return false;
  }
  
  return true;
}



================================================
File: lib/types/chat-history.ts
================================================
export interface ChatSession {
  id: string;
  title: string;
  created_at: string;
  updated_at: string;
  user_id: string;
  messages: ChatMessage[];
}

export interface ChatMessage {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  created_at: string;
  session_id: string;
}

export type TimeFilter = '7' | '14' | '30' | 'all';




================================================
File: scripts/run-all-tests.sh
================================================
#!/bin/bash

# Run all Jest tests
echo "Running Jest tests..."
pnpm test

# If Jest tests pass, run the model tests
if [ $? -eq 0 ]; then
  echo "Jest tests passed. Running model tests..."
  node scripts/test-models.js
else
  echo "Jest tests failed. Skipping model tests."
  exit 1
fi



================================================
File: scripts/test-models.js
================================================
#!/usr/bin/env node

/**
 * This script tests all available models in both the main app and the Tavily AI search page.
 * It sends a request to each model asking "What LLM are you?" to verify that the model
 * is correctly identified and functioning.
 * 
 * Usage:
 * node scripts/test-models.js
 */

const { execSync } = require('child_process');
const fs = require('fs');
const path = require('path');
const readline = require('readline');

// Load environment variables
require('dotenv').config({ path: path.resolve(process.cwd(), '.env.local') });

// Get models from the models.ts file
const modelsPath = path.resolve(process.cwd(), 'lib/models.ts');
const modelsContent = fs.readFileSync(modelsPath, 'utf8');

// Extract model IDs using regex
const modelIdRegex = /id:\s*['"]([^'"]+)['"]/g;
const modelIds = [];
let match;
while ((match = modelIdRegex.exec(modelsContent)) !== null) {
  modelIds.push(match[1]);
}

// Function to test a model with the main chat API
async function testMainChatModel(modelId) {
  try {
    console.log(`Testing main chat API with model: ${modelId}`);
    
    const response = await fetch('http://localhost:3000/api/chat', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        messages: [{ role: 'user', content: 'What LLM are you?' }],
        model: modelId,
        reasoning: true,
      }),
    });
    
    if (!response.ok) {
      console.error(`Error testing model ${modelId}: ${response.statusText}`);
      return false;
    }
    
    // For streaming responses, we need to read the chunks
    const reader = response.body.getReader();
    let result = '';
    
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      
      // Convert the chunk to a string
      const chunk = new TextDecoder().decode(value);
      result += chunk;
    }
    
    console.log(`✅ Model ${modelId} responded successfully`);
    return true;
  } catch (error) {
    console.error(`Error testing model ${modelId}:`, error);
    return false;
  }
}

// Function to test a model with the Tavily chat API
async function testTavilyChatModel(modelId) {
  try {
    console.log(`Testing Tavily chat API with model: ${modelId}`);
    
    const response = await fetch('http://localhost:3000/api/tavily-chat', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        messages: [{ role: 'user', content: 'What LLM are you?' }],
        selectedModelId: modelId,
        isReasoningEnabled: true,
        searchQuery: 'What is the latest news about artificial intelligence?',
      }),
    });
    
    if (!response.ok) {
      console.error(`Error testing model ${modelId} with Tavily: ${response.statusText}`);
      return false;
    }
    
    // For streaming responses, we need to read the chunks
    const reader = response.body.getReader();
    let result = '';
    
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      
      // Convert the chunk to a string
      const chunk = new TextDecoder().decode(value);
      result += chunk;
    }
    
    console.log(`✅ Model ${modelId} with Tavily search responded successfully`);
    return true;
  } catch (error) {
    console.error(`Error testing model ${modelId} with Tavily:`, error);
    return false;
  }
}

// Main function to run the tests
async function runTests() {
  console.log('Starting model tests...');
  console.log(`Found ${modelIds.length} models to test: ${modelIds.join(', ')}`);
  
  // Start the Next.js development server
  console.log('Starting Next.js development server...');
  const serverProcess = require('child_process').spawn('pnpm', ['dev'], {
    stdio: 'inherit',
    shell: true,
  });
  
  // Give the server some time to start
  console.log('Waiting for server to start...');
  await new Promise(resolve => setTimeout(resolve, 10000));
  
  // Test each model with the main chat API
  console.log('\n=== Testing Main Chat API ===');
  const mainResults = [];
  for (const modelId of modelIds) {
    const success = await testMainChatModel(modelId);
    mainResults.push({ modelId, success });
  }
  
  // Test each model with the Tavily chat API
  console.log('\n=== Testing Tavily Chat API ===');
  const tavilyResults = [];
  for (const modelId of modelIds) {
    const success = await testTavilyChatModel(modelId);
    tavilyResults.push({ modelId, success });
  }
  
  // Print summary
  console.log('\n=== Test Results Summary ===');
  console.log('Main Chat API:');
  for (const { modelId, success } of mainResults) {
    console.log(`${success ? '✅' : '❌'} ${modelId}`);
  }
  
  console.log('\nTavily Chat API:');
  for (const { modelId, success } of tavilyResults) {
    console.log(`${success ? '✅' : '❌'} ${modelId}`);
  }
  
  // Kill the server process
  serverProcess.kill();
  console.log('Tests completed.');
}

// Run the tests
runTests().catch(error => {
  console.error('Error running tests:', error);
  process.exit(1);
});



================================================
File: supabase/README.md
================================================
# Supabase Integration for GKennedy AI Search

This directory contains the necessary files and instructions for integrating Supabase with the GKennedy AI Search application.

## Setup Instructions

### 1. Environment Variables

Make sure the following environment variables are set in your `.env.local` file:

```
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key
```

### 2. Database Schema

The `schema.sql` file in this directory contains the SQL statements needed to set up the database schema for the chat history feature. You can run these statements in the Supabase SQL editor to create the necessary tables and policies.

### 3. Authentication

Supabase authentication is handled automatically through the middleware. The application uses cookie-based authentication to maintain user sessions.

The authentication flow is as follows:
1. Users sign in through your existing authentication system
2. The middleware in `utils/supabase/middleware.ts` refreshes the session on each request
3. Server components use the server-side Supabase client from `utils/supabase/server.ts`
4. Client components use the browser-side Supabase client from `utils/supabase/client.ts`

### 4. Chat History Feature

The chat history feature allows users to:

- View their chat history in the sidebar
- Filter chat sessions by time (7 days, 14 days, 30 days, or all)
- Copy chat sessions to the clipboard
- Delete chat sessions
- Restore previous chat sessions

## Features Implemented

### Chat History

- Chat sessions are stored in the `chat_sessions` table
- Chat messages are stored in the `chat_messages` table
- Users can view, create, and delete their chat history
- Chat history is filtered by time (7 days, 14 days, 30 days, or all)
- Chat sessions can be exported to clipboard

## File Structure

- `schema.sql` - SQL schema for the Supabase database
- `../utils/supabase/server.ts` - Server-side Supabase client
- `../utils/supabase/client.ts` - Client-side Supabase client
- `../utils/supabase/middleware.ts` - Middleware for session handling
- `../lib/types/chat-history.ts` - TypeScript interfaces for chat history
- `../lib/supabase/chat-history.ts` - Functions for interacting with chat history
- `../components/chat-history/chat-history-dropdown.tsx` - UI component for chat history

## Row Level Security (RLS) Policies

The database is configured with Row Level Security to ensure that users can only access their own data. The policies are defined in the `schema.sql` file and include:

- Users can only select their own chat sessions
- Users can only insert chat sessions with their own user ID
- Users can only update their own chat sessions
- Users can only delete their own chat sessions
- Similar policies apply to chat messages

## Usage

The chat history feature is accessible from the sidebar. Users can:

1. View their chat history
2. Filter chat history by time
3. Delete chat sessions
4. Copy chat sessions to clipboard
5. Restore previous chat sessions

## Security

Row Level Security (RLS) policies are implemented to ensure that users can only access their own chat sessions and messages. The policies are defined in the `schema.sql` file.

## Troubleshooting

If you encounter issues with the Supabase integration:

1. Check that your environment variables are correctly set
2. Ensure the SQL schema has been properly executed in your Supabase instance
3. Verify that your Supabase project has Row Level Security enabled
4. Check the browser console for any error messages
5. Make sure your authentication system is properly configured

For more information, refer to the [Supabase documentation](https://supabase.com/docs).



================================================
File: supabase/schema.sql
================================================
-- Create tables for chat history
CREATE TABLE IF NOT EXISTS chat_sessions (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  title TEXT NOT NULL DEFAULT 'Untitled Chat',
  user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS chat_messages (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  session_id UUID REFERENCES chat_sessions(id) ON DELETE CASCADE,
  role TEXT NOT NULL CHECK (role IN ('user', 'assistant')),
  content TEXT NOT NULL,
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Create indexes for better performance
CREATE INDEX IF NOT EXISTS idx_chat_sessions_user_id ON chat_sessions(user_id);
CREATE INDEX IF NOT EXISTS idx_chat_sessions_updated_at ON chat_sessions(updated_at);
CREATE INDEX IF NOT EXISTS idx_chat_messages_session_id ON chat_messages(session_id);
CREATE INDEX IF NOT EXISTS idx_chat_messages_created_at ON chat_messages(created_at);

-- Create RLS policies for chat_sessions
ALTER TABLE chat_sessions ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view their own chat sessions"
  ON chat_sessions FOR SELECT
  USING (auth.uid() = user_id);

CREATE POLICY "Users can insert their own chat sessions"
  ON chat_sessions FOR INSERT
  WITH CHECK (auth.uid() = user_id);

CREATE POLICY "Users can update their own chat sessions"
  ON chat_sessions FOR UPDATE
  USING (auth.uid() = user_id);

CREATE POLICY "Users can delete their own chat sessions"
  ON chat_sessions FOR DELETE
  USING (auth.uid() = user_id);

-- Create RLS policies for chat_messages
ALTER TABLE chat_messages ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view messages in their own chat sessions"
  ON chat_messages FOR SELECT
  USING (
    EXISTS (
      SELECT 1 FROM chat_sessions
      WHERE chat_sessions.id = chat_messages.session_id
      AND chat_sessions.user_id = auth.uid()
    )
  );

CREATE POLICY "Users can insert messages in their own chat sessions"
  ON chat_messages FOR INSERT
  WITH CHECK (
    EXISTS (
      SELECT 1 FROM chat_sessions
      WHERE chat_sessions.id = chat_messages.session_id
      AND chat_sessions.user_id = auth.uid()
    )
  );

CREATE POLICY "Users can update messages in their own chat sessions"
  ON chat_messages FOR UPDATE
  USING (
    EXISTS (
      SELECT 1 FROM chat_sessions
      WHERE chat_sessions.id = chat_messages.session_id
      AND chat_sessions.user_id = auth.uid()
    )
  );

CREATE POLICY "Users can delete messages in their own chat sessions"
  ON chat_messages FOR DELETE
  USING (
    EXISTS (
      SELECT 1 FROM chat_sessions
      WHERE chat_sessions.id = chat_messages.session_id
      AND chat_sessions.user_id = auth.uid()
    )
  );

-- Create function to update the updated_at timestamp
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
  NEW.updated_at = NOW();
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Create trigger to update the updated_at timestamp
CREATE TRIGGER update_chat_sessions_updated_at
BEFORE UPDATE ON chat_sessions
FOR EACH ROW
EXECUTE FUNCTION update_updated_at_column();



================================================
File: tests/multimodal.test.tsx
================================================
import React from 'react';
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import '@testing-library/jest-dom';

// Mock the Chat component instead of importing it
const mockAppend = jest.fn();
const mockStop = jest.fn();

// Mock component that simulates the Chat component's behavior
const MockChat = () => {
  const [files, setFiles] = React.useState<File[]>([]);
  const [previews, setPreviews] = React.useState<string[]>([]);

  const handleFileChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    if (e.target.files && e.target.files.length > 0) {
      const selectedFiles = Array.from(e.target.files);
      setFiles(selectedFiles);
      
      // Create previews for the files
      const newPreviews = selectedFiles.map(file => URL.createObjectURL(file));
      setPreviews(newPreviews);
    }
  };

  const handleRemoveFile = (index: number) => {
    const newFiles = [...files];
    newFiles.splice(index, 1);
    setFiles(newFiles);

    const newPreviews = [...previews];
    URL.revokeObjectURL(newPreviews[index]);
    newPreviews.splice(index, 1);
    setPreviews(newPreviews);
  };

  const handleSendMessage = () => {
    mockAppend("Test message", { attachments: files.map(file => ({ file })) });
  };

  return (
    <div>
      <div className="chat-container">
        <div className="message-list">
          {/* Message list would go here */}
        </div>
        <div className="chat-input">
          <input 
            type="text" 
            placeholder="Type a message..." 
            data-testid="chat-input"
          />
          <label htmlFor="file-upload" className="file-upload-label" data-testid="file-upload-label">
            <span>Upload</span>
          </label>
          <input
            id="file-upload"
            type="file"
            accept="image/png,image/jpeg,image/gif,image/webp,application/pdf"
            onChange={handleFileChange}
            style={{ display: 'none' }}
            data-testid="file-upload"
            multiple
          />
          <button onClick={handleSendMessage} data-testid="send-button">
            Send
          </button>
        </div>
        {files.length > 0 && (
          <div className="file-previews" data-testid="file-previews">
            {previews.map((preview, index) => (
              <div key={index} className="file-preview">
                <img 
                  src={preview} 
                  alt={`Preview ${index}`} 
                  data-testid={`file-preview-${index}`}
                />
                <button 
                  onClick={() => handleRemoveFile(index)}
                  data-testid={`remove-file-${index}`}
                >
                  Remove
                </button>
              </div>
            ))}
          </div>
        )}
      </div>
    </div>
  );
};

describe('Multimodal Feature', () => {
  beforeEach(() => {
    // Reset mocks before each test
    mockAppend.mockClear();
    mockStop.mockClear();
  });

  test('should render file upload input', async () => {
    render(<MockChat />);
    
    // Check if file upload input is rendered
    const fileUploadLabel = screen.getByTestId('file-upload-label');
    expect(fileUploadLabel).toBeInTheDocument();
  });

  test('should allow uploading and displaying file previews', async () => {
    render(<MockChat />);
    
    // Create a mock file
    const file = new File(['dummy content'], 'test-image.png', { type: 'image/png' });
    
    // Get the file input and simulate a file upload
    const fileInput = screen.getByTestId('file-upload');
    
    // Mock the URL.createObjectURL function
    const originalCreateObjectURL = URL.createObjectURL;
    URL.createObjectURL = jest.fn(() => 'mock-url');
    
    // Simulate file upload
    fireEvent.change(fileInput, { target: { files: [file] } });
    
    // Check if preview is displayed
    await waitFor(() => {
      const filePreview = screen.getByTestId('file-previews');
      expect(filePreview).toBeInTheDocument();
    });
    
    // Restore the original function
    URL.createObjectURL = originalCreateObjectURL;
  });

  test('should allow removing uploaded files', async () => {
    render(<MockChat />);
    
    // Create a mock file
    const file = new File(['dummy content'], 'test-image.png', { type: 'image/png' });
    
    // Get the file input and simulate a file upload
    const fileInput = screen.getByTestId('file-upload');
    
    // Mock URL functions
    const originalCreateObjectURL = URL.createObjectURL;
    const originalRevokeObjectURL = URL.revokeObjectURL;
    URL.createObjectURL = jest.fn(() => 'mock-url');
    URL.revokeObjectURL = jest.fn();
    
    // Simulate file upload
    fireEvent.change(fileInput, { target: { files: [file] } });
    
    // Check if preview is displayed
    await waitFor(() => {
      const filePreview = screen.getByTestId('file-previews');
      expect(filePreview).toBeInTheDocument();
    });
    
    // Remove the file
    const removeButton = screen.getByTestId('remove-file-0');
    fireEvent.click(removeButton);
    
    // Check if preview is removed
    await waitFor(() => {
      expect(screen.queryByTestId('file-previews')).not.toBeInTheDocument();
    });
    
    // Restore the original functions
    URL.createObjectURL = originalCreateObjectURL;
    URL.revokeObjectURL = originalRevokeObjectURL;
  });

  test('should send message with attachments', async () => {
    render(<MockChat />);
    
    // Create a mock file
    const file = new File(['dummy content'], 'test-image.png', { type: 'image/png' });
    
    // Get the file input and simulate a file upload
    const fileInput = screen.getByTestId('file-upload');
    
    // Mock URL functions
    const originalCreateObjectURL = URL.createObjectURL;
    URL.createObjectURL = jest.fn(() => 'mock-url');
    
    // Simulate file upload
    fireEvent.change(fileInput, { target: { files: [file] } });
    
    // Send the message
    const sendButton = screen.getByTestId('send-button');
    fireEvent.click(sendButton);
    
    // Check if append was called with the file
    expect(mockAppend).toHaveBeenCalledWith("Test message", { attachments: [{ file }] });
    
    // Restore the original function
    URL.createObjectURL = originalCreateObjectURL;
  });
});



================================================
File: tests/tavily-chat.test.tsx
================================================
import React from 'react';
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import '@testing-library/jest-dom';

// Mock the TavilyChat component instead of importing it
const mockAppend = jest.fn();
const mockStop = jest.fn();
const mockSetMessages = jest.fn();
const mockSetIsSearchEnabled = jest.fn();

// Mock the models
jest.mock('../lib/models', () => ({
  models: [
    { id: 'claude-3.7-sonnet', name: 'Claude 3.7 Sonnet', description: 'Test description' },
    { id: 'o3-mini', name: 'Openai o3-mini', description: 'Test description' },
    { id: 'gemini-2.0-flash', name: 'Gemini 2.0 Flash', description: 'Test description' }
  ]
}));

// Define message type
interface Message {
  role: string;
  content: string;
}

// Mock component that simulates the TavilyChat component's behavior
const MockTavilyChat = () => {
  const [isSearchEnabled, setIsSearchEnabled] = React.useState(false);
  const [selectedModel, setSelectedModel] = React.useState('claude-3.7-sonnet');
  const [inputValue, setInputValue] = React.useState('');
  const [messages, setMessages] = React.useState<Message[]>([]);
  
  const handleSearchToggle = () => {
    setIsSearchEnabled(!isSearchEnabled);
    mockSetIsSearchEnabled(!isSearchEnabled);
  };
  
  const handleModelChange = (e: React.ChangeEvent<HTMLSelectElement>) => {
    setSelectedModel(e.target.value);
  };
  
  const handleInputChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    setInputValue(e.target.value);
  };
  
  const handleSendMessage = () => {
    if (inputValue.trim()) {
      const newMessage: Message = { role: 'user', content: inputValue };
      setMessages([...messages, newMessage]);
      mockSetMessages([...messages, newMessage]);
      mockAppend(inputValue, { 
        data: { 
          selectedModelId: selectedModel,
          isReasoningEnabled: true,
          searchQuery: isSearchEnabled ? inputValue : ''
        } 
      });
      setInputValue('');
    }
  };
  
  return (
    <div>
      <div className="chat-header">
        <div className="model-selector">
          <select 
            value={selectedModel} 
            onChange={handleModelChange}
            data-testid="model-selector"
          >
            <option value="claude-3.7-sonnet">Claude 3.7 Sonnet</option>
            <option value="o3-mini">OpenAI o3-mini</option>
            <option value="gemini-2.0-flash">Gemini 2.0 Flash</option>
          </select>
        </div>
        <div className="search-toggle">
          <label>
            <input 
              type="checkbox" 
              checked={isSearchEnabled} 
              onChange={handleSearchToggle}
              data-testid="search-toggle"
            />
            Web Search
          </label>
        </div>
      </div>
      <div className="chat-container">
        <div className="message-list" data-testid="message-list">
          {messages.map((message, index) => (
            <div key={index} className={`message ${message.role}`} data-testid={`message-${index}`}>
              {message.content}
            </div>
          ))}
        </div>
        <div className="chat-input">
          <input 
            type="text" 
            value={inputValue}
            onChange={handleInputChange}
            placeholder="Type a message..." 
            data-testid="chat-input"
          />
          <button 
            onClick={handleSendMessage} 
            data-testid="send-button"
          >
            Send
          </button>
        </div>
      </div>
    </div>
  );
};

describe('TavilyChat Component', () => {
  beforeEach(() => {
    // Reset mocks before each test
    mockAppend.mockClear();
    mockStop.mockClear();
    mockSetMessages.mockClear();
    mockSetIsSearchEnabled.mockClear();
  });

  test('should render chat interface with model selector and search toggle', () => {
    render(<MockTavilyChat />);
    
    // Check if model selector is rendered
    const modelSelector = screen.getByTestId('model-selector');
    expect(modelSelector).toBeInTheDocument();
    
    // Check if search toggle is rendered
    const searchToggle = screen.getByTestId('search-toggle');
    expect(searchToggle).toBeInTheDocument();
    expect(searchToggle).not.toBeChecked();
    
    // Check if chat input is rendered
    const chatInput = screen.getByTestId('chat-input');
    expect(chatInput).toBeInTheDocument();
  });

  test('should toggle search functionality', () => {
    render(<MockTavilyChat />);
    
    // Get the search toggle
    const searchToggle = screen.getByTestId('search-toggle');
    
    // Toggle search on
    fireEvent.click(searchToggle);
    expect(mockSetIsSearchEnabled).toHaveBeenCalledWith(true);
    
    // Toggle search off
    fireEvent.click(searchToggle);
    expect(mockSetIsSearchEnabled).toHaveBeenCalledWith(false);
  });

  test('should change selected model', () => {
    render(<MockTavilyChat />);
    
    // Get the model selector
    const modelSelector = screen.getByTestId('model-selector');
    
    // Change model to OpenAI
    fireEvent.change(modelSelector, { target: { value: 'o3-mini' } });
    expect(modelSelector).toHaveValue('o3-mini');
    
    // Change model to Gemini
    fireEvent.change(modelSelector, { target: { value: 'gemini-2.0-flash' } });
    expect(modelSelector).toHaveValue('gemini-2.0-flash');
  });

  test('should send message with search query when search is enabled', () => {
    render(<MockTavilyChat />);
    
    // Get the search toggle and enable it
    const searchToggle = screen.getByTestId('search-toggle');
    fireEvent.click(searchToggle);
    
    // Get the chat input and send button
    const chatInput = screen.getByTestId('chat-input');
    const sendButton = screen.getByTestId('send-button');
    
    // Type a message
    fireEvent.change(chatInput, { target: { value: 'What LLM are you?' } });
    
    // Send the message
    fireEvent.click(sendButton);
    
    // Check if append was called with the correct parameters
    expect(mockAppend).toHaveBeenCalledWith('What LLM are you?', {
      data: {
        selectedModelId: 'claude-3.7-sonnet',
        isReasoningEnabled: true,
        searchQuery: 'What LLM are you?'
      }
    });
    
    // Check if the input was cleared
    expect(chatInput).toHaveValue('');
  });

  test('should send message without search query when search is disabled', () => {
    render(<MockTavilyChat />);
    
    // Get the chat input and send button
    const chatInput = screen.getByTestId('chat-input');
    const sendButton = screen.getByTestId('send-button');
    
    // Type a message
    fireEvent.change(chatInput, { target: { value: 'What LLM are you?' } });
    
    // Send the message
    fireEvent.click(sendButton);
    
    // Check if append was called with the correct parameters
    expect(mockAppend).toHaveBeenCalledWith('What LLM are you?', {
      data: {
        selectedModelId: 'claude-3.7-sonnet',
        isReasoningEnabled: true,
        searchQuery: ''
      }
    });
    
    // Check if the input was cleared
    expect(chatInput).toHaveValue('');
  });

  test('should not send empty messages', () => {
    render(<MockTavilyChat />);
    
    // Get the send button
    const sendButton = screen.getByTestId('send-button');
    
    // Try to send an empty message
    fireEvent.click(sendButton);
    
    // Check that append was not called
    expect(mockAppend).not.toHaveBeenCalled();
  });
});



================================================
File: tools/README.md
================================================
# Tavily Search Tool

This directory contains the Tavily Search Tool implementation for the GKennedy AI application.

## Setup

1. Sign up for a Tavily API key at [Tavily's website](https://tavily.com/)
2. Add your Tavily API key to your `.env.local` file:

```
TAVILY_API_KEY=your-tavily-api-key
```

## Usage

The Tavily Search Tool is integrated into the application at `/tavily-ai-search`. This page provides all the same functionality as the main chat interface but with the added ability to search the web for information related to user queries.

To use the Tavily Search:
1. Navigate to http://localhost:3000/tavily-ai-search
2. Toggle the "Web Search" option
3. Enter your query
4. The AI will use Tavily to search the web and incorporate the results into its response



================================================
File: tools/tavily-search.ts
================================================
// tools/tavily-search.ts
import { tavily } from "@tavily/core";
import { tavilyLogger } from "@/utils/tavily-logger";

// Initialize Tavily client
export const tavilyClient = (apiKey?: string) => {
  const key = apiKey || process.env.TAVILY_API_KEY;
  if (!key) {
    throw new Error("No Tavily API key provided");
  }
  
  return tavily({ apiKey: key });
};

export interface TavilySearchParams {
  query: string;
  searchDepth?: "basic" | "advanced";
  maxResults?: number;
  includeAnswer?: boolean;
  includeRawContent?: boolean;
  includeDomains?: string[];
  excludeDomains?: string[];
  modelId?: string; 
}

export async function searchTavily(params: TavilySearchParams) {
  const client = tavilyClient();
  
  try {
    const response = await client.search(
      params.query,
      {
        searchDepth: params.searchDepth || "basic",
        maxResults: params.maxResults || 5,
        includeAnswer: params.includeAnswer || false,
        includeRawContent: params.includeRawContent || false,
        includeDomains: params.includeDomains,
        excludeDomains: params.excludeDomains,
      }
    );
    
    tavilyLogger.logSearchResults(params.query, response, params.modelId);
    tavilyLogger.appendToConsolidatedLog(params.query, response, params.modelId);
    
    return response;
  } catch (error) {
    console.error("Tavily search error:", error);
    throw error;
  }
}



================================================
File: utils/ai-agents-logger.ts
================================================
// utils/ai-agents-logger.ts
import fs from 'fs';
import path from 'path';
import type { ModelConfig } from '@/lib/ai-agents/types';

/**
 * Logger utility for saving AI Agents processing results
 */
export class AIAgentsLogger {
  private outputDir: string;
  
  constructor(outputDir: string = 'ai_agents_output') {
    this.outputDir = path.resolve(process.cwd(), outputDir);
    this.ensureDirectoryExists();
  }
  
  private ensureDirectoryExists(): void {
    if (!fs.existsSync(this.outputDir)) {
      fs.mkdirSync(this.outputDir, { recursive: true });
    }
  }
  
  /**
   * Log AI Agents processing results
   */
  async logProcessing(params: {
    timestamp: string;
    query: string;
    primaryModel: ModelConfig;
    secondaryModel: ModelConfig;
    primaryResults: any;
    secondaryResults: any;
  }) {
    const { timestamp, query, primaryModel, secondaryModel, primaryResults, secondaryResults } = params;
    
    // Create filename with timestamp and both model names
    const filename = `ai-agents-${timestamp}-${primaryModel.model}-${secondaryModel.model}.json`;
    const filePath = path.join(this.outputDir, filename);
    
    // Create log entry
    const logEntry = {
      timestamp,
      query,
      primary: {
        model: primaryModel,
        results: primaryResults
      },
      secondary: {
        model: secondaryModel,
        results: secondaryResults
      }
    };
    
    // Write to file
    await fs.promises.writeFile(
      filePath,
      JSON.stringify(logEntry, null, 2)
    );
    
    // Append to log file
    const logLine = JSON.stringify({
      timestamp,
      query,
      primaryModel: primaryModel.model,
      secondaryModel: secondaryModel.model
    });
    
    await fs.promises.appendFile(
      path.join(this.outputDir, 'ai-agents-log.jsonl'),
      logLine + '\n'
    );
  }
}

// Export a singleton instance
export const aiAgentsLogger = new AIAgentsLogger();



================================================
File: utils/tavily-logger.ts
================================================
// utils/tavily-logger.ts
import fs from 'fs';
import path from 'path';

/**
 * Logger utility for saving Tavily search results to files
 */
export class TavilyLogger {
  private outputDir: string;
  
  constructor(outputDir: string = 'tavily_output') {
    // Resolve the output directory path
    this.outputDir = path.resolve(process.cwd(), outputDir);
    
    // Ensure the output directory exists
    this.ensureDirectoryExists();
  }
  
  /**
   * Ensure the output directory exists
   */
  private ensureDirectoryExists(): void {
    if (!fs.existsSync(this.outputDir)) {
      fs.mkdirSync(this.outputDir, { recursive: true });
    }
  }
  
  /**
   * Log Tavily search results to a file
   * @param query The search query
   * @param results The search results
   * @param model The model used for the search (if applicable)
   */
  public logSearchResults(query: string, results: any, model?: string): void {
    try {
      // Create a timestamp for the filename
      const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
      const modelInfo = model ? `-${model.replace(/[^a-zA-Z0-9-]/g, '-')}` : '';
      const filename = `tavily-search-${timestamp}${modelInfo}.json`;
      const filePath = path.join(this.outputDir, filename);
      
      // Create the log data
      const logData = {
        timestamp: new Date().toISOString(),
        model,
        query,
        results
      };
      
      // Write the log data to a file
      fs.writeFileSync(filePath, JSON.stringify(logData, null, 2));
      
      console.log(`Tavily search results logged to ${filePath}`);
    } catch (error) {
      console.error('Error logging Tavily search results:', error);
    }
  }
  
  /**
   * Append Tavily search results to a consolidated log file
   * @param query The search query
   * @param results The search results
   * @param model The model used for the search (if applicable)
   */
  public appendToConsolidatedLog(query: string, results: any, model?: string): void {
    try {
      const logFilePath = path.join(this.outputDir, 'tavily-search-log.jsonl');
      
      // Create the log entry
      const logEntry = {
        timestamp: new Date().toISOString(),
        model,
        query,
        results
      };
      
      // Append the log entry to the file
      fs.appendFileSync(
        logFilePath, 
        JSON.stringify(logEntry) + '\n'
      );
    } catch (error) {
      console.error('Error appending to consolidated Tavily log:', error);
    }
  }
}

// Export a singleton instance with the default output directory
export const tavilyLogger = new TavilyLogger();



================================================
File: utils/supabase/client.ts
================================================
import { createBrowserClient } from "@supabase/ssr";

export const createClient = () => {
  return createBrowserClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
  );
};



================================================
File: utils/supabase/middleware.ts
================================================
import { createServerClient, type CookieOptions } from "@supabase/ssr";
import { NextResponse, type NextRequest } from "next/server";

export const createClient = (request: NextRequest) => {
  // Create an unmodified response
  let response = NextResponse.next({
    request: {
      headers: request.headers,
    },
  });

  const supabase = createServerClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
    {
      cookies: {
        get(name: string) {
          return request.cookies.get(name)?.value;
        },
        set(name: string, value: string, options: CookieOptions) {
          // If the cookie is updated, update the cookies for the request and response
          request.cookies.set({
            name,
            value,
            ...options,
          });
          response = NextResponse.next({
            request: {
              headers: request.headers,
            },
          });
          response.cookies.set({
            name,
            value,
            ...options,
          });
        },
        remove(name: string, options: CookieOptions) {
          // If the cookie is removed, update the cookies for the request and response
          request.cookies.delete(name);
          response = NextResponse.next({
            request: {
              headers: request.headers,
            },
          });
          response.cookies.delete(name);
        },
      },
    }
  );

  return { supabase, response };
};



================================================
File: utils/supabase/server-client.ts
================================================
import { createServerClient } from "@supabase/ssr";
import { cookies } from "next/headers";

export const createClient = async () => {
  const cookieStore = await cookies(); // Await the cookies() call
  
  return createServerClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
    {
      cookies: {
        get(name: string) {
          return cookieStore.get(name)?.value;
        },
        set(name: string, value: string, options: Record<string, any>) {
          cookieStore.set(name, value, options);
        },
        remove(name: string, options: Record<string, any>) {
          cookieStore.set(name, "", { ...options, maxAge: 0 });
        },
      },
    }
  );
};



================================================
File: utils/supabase/server.ts
================================================
import { createServerClient } from "@supabase/ssr";
import { cookies, RequestCookies } from "next/headers"; // Import RequestCookies

export const createClient = (cookieStore: RequestCookies) => {
  return createServerClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
    {
      cookies: {
        get(name: string) {
          return cookieStore.get(name)?.value;
        },
        set(name: string, value: string, options: Record<string, any>) {
          cookieStore.set(name, value, options);
        },
        remove(name: string, options: Record<string, any>) {
          cookieStore.set(name, "", { ...options, maxAge: 0 });
        },
      },
    }
  );
};

// Example of how to call createClient
export const initializeClient = async () => {
  const cookieStore = await cookies(); // Await the cookies() call
  return createClient(cookieStore); // Pass the resolved cookieStore
};


